{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOTxfz7MI0luSjhOE8n7LRH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/TRANSFORMER_REASONING_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q\n",
        "!pip install transformers -q\n",
        "!pip install torch -q"
      ],
      "metadata": {
        "id": "ANmkAB3Q6Fnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehV4SxTO5G7G",
        "outputId": "7265b87b-7f5b-4b5e-8578-8df93840c278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 234/234 [00:23<00:00,  9.94it/s, loss=9.87]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Training Loss: 10.5158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 Evaluation: 100%|██████████| 42/42 [00:01<00:00, 32.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Evaluation Loss: 9.8002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 234/234 [00:22<00:00, 10.31it/s, loss=8.79]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Training Loss: 9.2945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 Evaluation: 100%|██████████| 42/42 [00:01<00:00, 32.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Evaluation Loss: 8.7138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 234/234 [00:22<00:00, 10.39it/s, loss=7.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Training Loss: 8.0429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 Evaluation: 100%|██████████| 42/42 [00:01<00:00, 32.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Evaluation Loss: 7.5185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 234/234 [00:22<00:00, 10.42it/s, loss=6.94]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Training Loss: 7.1583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 Evaluation: 100%|██████████| 42/42 [00:01<00:00, 32.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Evaluation Loss: 7.0628\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 234/234 [00:22<00:00, 10.31it/s, loss=6.65]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Training Loss: 6.7686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 Evaluation: 100%|██████████| 42/42 [00:01<00:00, 32.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Evaluation Loss: 6.8315\n",
            "\n",
            "--- Example Inference ---\n",
            "Question: A new program had 60 downloads in the first month. The number of downloads in the second month was three times as many as the downloads in the first month, but then reduced by 30% in the third month. How many downloads did the program have total over the three months?\n",
            "Actual Answer: The number of downloads of the program in the second month increased to 3*60 = <<3*60=180>>180\n",
            "In the first two months, the total number of downloads of the program was 180+60 = <<180+60=240>>240\n",
            "In the third month, the number of downloads of the program reduced by 30/100*180 = <<30/100*180=54>>54\n",
            "There were 180-54 = <<180-54=126>>126 downloads in the third month.\n",
            "In the three months, the total number of downloads of the program was 126+240 = <<126+240=366>>366\n",
            "#### 366\n",
            "Predicted Answer: the total of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the number of the\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 5e-5\n",
        "NUM_EPOCHS = 5\n",
        "D_MODEL = 256  # Reduced for faster training on a smaller scale\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 3  # Reduced for faster training\n",
        "D_FF = 512\n",
        "DROPOUT = 0.1\n",
        "MAX_LEN = 128  # Maximum sequence length\n",
        "WARMUP_STEPS = 1000\n",
        "GRADIENT_CLIPPING = 1.0\n",
        "\n",
        "# --- Load the GSM8k Dataset ---\n",
        "gsm8k_dataset = load_dataset(\"gsm8k\", \"main\")\n",
        "train_dataset = gsm8k_dataset['train']\n",
        "test_dataset = gsm8k_dataset['test']\n",
        "\n",
        "# --- Vocabulary Creation ---\n",
        "def build_vocabulary(examples):\n",
        "    tokenizer = set()\n",
        "    for example in examples:\n",
        "        text = example['question'] + \" \" + example['answer']\n",
        "        tokenizer.update(text.lower().split())\n",
        "    return sorted(list(tokenizer))\n",
        "\n",
        "vocabulary = build_vocabulary(train_dataset)\n",
        "vocab_size = len(vocabulary)\n",
        "word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "\n",
        "# Add special tokens\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "START_TOKEN = \"<start>\"\n",
        "END_TOKEN = \"<end>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "PAD_INDEX = 0\n",
        "START_INDEX = vocab_size\n",
        "END_INDEX = vocab_size + 1\n",
        "UNK_INDEX = vocab_size + 2\n",
        "\n",
        "word_to_index[PAD_TOKEN] = PAD_INDEX\n",
        "word_to_index[START_TOKEN] = START_INDEX\n",
        "word_to_index[END_TOKEN] = END_INDEX\n",
        "word_to_index[UNK_TOKEN] = UNK_INDEX\n",
        "\n",
        "index_to_word[PAD_INDEX] = PAD_TOKEN\n",
        "index_to_word[START_INDEX] = START_TOKEN\n",
        "index_to_word[END_INDEX] = END_TOKEN\n",
        "index_to_word[UNK_INDEX] = UNK_TOKEN\n",
        "\n",
        "updated_vocab_size = len(word_to_index)\n",
        "\n",
        "# --- Data Processing Function ---\n",
        "def process_example(example, max_len, word_to_index):\n",
        "    question = example['question'].lower().split()\n",
        "    answer = example['answer'].lower().split()\n",
        "\n",
        "    question_tokens = [word_to_index.get(word, UNK_INDEX) for word in question]\n",
        "    answer_tokens = [word_to_index.get(word, UNK_INDEX) for word in answer]\n",
        "\n",
        "    src_tokens = [START_INDEX] + question_tokens + [END_INDEX]\n",
        "    tgt_tokens = [START_INDEX] + answer_tokens + [END_INDEX]\n",
        "\n",
        "    src_tokens = src_tokens[:max_len]\n",
        "    tgt_tokens = tgt_tokens[:max_len]\n",
        "\n",
        "    src_padding = [PAD_INDEX] * (max_len - len(src_tokens))\n",
        "    tgt_padding = [PAD_INDEX] * (max_len - len(tgt_tokens))\n",
        "\n",
        "    src_tensor = torch.tensor(src_tokens + src_padding)\n",
        "    tgt_input_tensor = torch.tensor([START_INDEX] + answer_tokens[:max_len-1] + tgt_padding[:1]) # Input to decoder\n",
        "    tgt_output_tensor = torch.tensor(answer_tokens[:max_len-1] + [END_INDEX] + tgt_padding[:1]) # Target for decoder\n",
        "\n",
        "    return src_tensor, tgt_input_tensor, tgt_output_tensor\n",
        "\n",
        "# --- Custom Dataset Class ---\n",
        "class MathDataset(Dataset):\n",
        "    def __init__(self, dataset, max_len, word_to_index):\n",
        "        self.dataset = dataset\n",
        "        self.max_len = max_len\n",
        "        self.word_to_index = word_to_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.dataset[idx]\n",
        "        return process_example(example, self.max_len, self.word_to_index)\n",
        "\n",
        "# --- Create DataLoaders ---\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_tensors, tgt_in_tensors, tgt_out_tensors = zip(*batch)\n",
        "    # pad the sequences within the batch\n",
        "    src_tensors = pad_sequence(src_tensors, batch_first=True, padding_value=PAD_INDEX)\n",
        "    tgt_in_tensors = pad_sequence(tgt_in_tensors, batch_first=True, padding_value=PAD_INDEX)\n",
        "    tgt_out_tensors = pad_sequence(tgt_out_tensors, batch_first=True, padding_value=PAD_INDEX)\n",
        "    return src_tensors, tgt_in_tensors, tgt_out_tensors\n",
        "\n",
        "train_dataloader = DataLoader(MathDataset(train_dataset, MAX_LEN, word_to_index), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "test_dataloader = DataLoader(MathDataset(test_dataset, MAX_LEN, word_to_index), batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "# --- Transformer Model Definition ---\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output, attn_probs\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        return x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, num_heads, seq_len, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q_ = self.split_heads(self.W_q(Q))\n",
        "        K_ = self.split_heads(self.W_k(K))\n",
        "        V_ = self.split_heads(self.W_v(V))\n",
        "\n",
        "        output, attn_probs = self.scaled_dot_product_attention(Q_, K_, V_, mask)\n",
        "        output = self.combine_heads(output)\n",
        "        output = self.W_o(output)\n",
        "        return output, attn_probs\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output, _ = self.mha(x, x, x, mask)\n",
        "        norm1_output = self.norm1(x + self.dropout(attn_output))\n",
        "        ffn_output = self.ffn(norm1_output)\n",
        "        output = self.norm2(norm1_output + self.dropout(ffn_output))\n",
        "        return output\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.masked_mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.enc_dec_mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        masked_attn_output, _ = self.masked_mha(x, x, x, tgt_mask)\n",
        "        norm1_output = self.norm1(x + self.dropout(masked_attn_output))\n",
        "        enc_dec_attn_output, _ = self.enc_dec_mha(norm1_output, enc_output, enc_output, src_mask)\n",
        "        norm2_output = self.norm2(norm1_output + self.dropout(enc_dec_attn_output))\n",
        "        ffn_output = self.ffn(norm2_output)\n",
        "        output = self.norm3(norm2_output + self.dropout(ffn_output))\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(1), :].transpose(0, 1)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "                                     for _ in range(num_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, mask):\n",
        "        embedded = self.dropout(self.pos_encoding(self.embedding(src)))\n",
        "        for layer in self.layers:\n",
        "            embedded = layer(embedded, mask)\n",
        "        return embedded\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "                                     for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, enc_output, src_mask, tgt_mask):\n",
        "        embedded = self.dropout(self.pos_encoding(self.embedding(tgt)))\n",
        "        for layer in self.layers:\n",
        "            embedded = layer(embedded, enc_output, src_mask, tgt_mask)\n",
        "        output = self.fc(embedded)\n",
        "        return output\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)\n",
        "        self.decoder = Decoder(tgt_vocab_size, d_model, num_layers, num_heads, d_ff, dropout, max_len)\n",
        "\n",
        "    def make_src_mask(self, src):\n",
        "        return (src != PAD_INDEX).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "    def make_tgt_mask(self, tgt):\n",
        "        tgt_len = tgt.size(1)\n",
        "        attn_shape = (1, tgt_len, tgt_len)\n",
        "        subsequent_mask = torch.tril(torch.ones(attn_shape, device=tgt.device)).type(torch.uint8) # Create subsequent_mask on the same device as tgt\n",
        "        padding_mask = (tgt != PAD_INDEX).unsqueeze(1).unsqueeze(2)\n",
        "        return subsequent_mask & padding_mask.bool()\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        tgt_mask = self.make_tgt_mask(tgt)\n",
        "        enc_output = self.encoder(src, src_mask)\n",
        "        output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
        "        return output\n",
        "\n",
        "# --- Initialize Model, Optimizer, and Scheduler ---\n",
        "model = Transformer(updated_vocab_size, updated_vocab_size, D_MODEL, NUM_LAYERS, NUM_HEADS, D_FF, DROPOUT, MAX_LEN)\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "total_steps = len(train_dataloader) * NUM_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_INDEX)\n",
        "\n",
        "# --- Training Loop ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1}\")\n",
        "    for batch_idx, (src, tgt_in, tgt_out) in progress_bar:\n",
        "        src = src.to(device)\n",
        "        tgt_in = tgt_in.to(device)\n",
        "        tgt_out = tgt_out.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt_in)  # (batch_size, tgt_len, vocab_size)\n",
        "\n",
        "        # Reshape for loss calculation\n",
        "        output = output.view(-1, output.size(-1)) # (batch_size * tgt_len, vocab_size)\n",
        "        tgt_out = tgt_out.view(-1) # (batch_size * tgt_len)\n",
        "\n",
        "        loss = criterion(output, tgt_out)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIPPING)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1} Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # --- Evaluation Loop ---\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "\n",
        "    # Add tqdm to the evaluation loop\n",
        "    eval_progress_bar = tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc=f\"Epoch {epoch+1} Evaluation\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (src, tgt_in, tgt_out) in eval_progress_bar:\n",
        "            src = src.to(device)\n",
        "            tgt_in = tgt_in.to(device)\n",
        "            tgt_out = tgt_out.to(device)\n",
        "\n",
        "            output = model(src, tgt_in)\n",
        "            output = output.view(-1, output.size(-1))\n",
        "            tgt_out = tgt_out.view(-1)\n",
        "            loss = criterion(output, tgt_out)\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "    avg_eval_loss = eval_loss / len(test_dataloader)\n",
        "    print(f\"Epoch {epoch+1} Evaluation Loss: {avg_eval_loss:.4f}\")\n",
        "\n",
        "# --- Inference Function (Basic) ---\n",
        "def translate_sentence(model, sentence, word_to_index, index_to_word, max_len, device):\n",
        "    model.eval()\n",
        "    tokens = [word_to_index.get(word.lower(), UNK_INDEX) for word in sentence.lower().split()]\n",
        "    src_tokens = [START_INDEX] + tokens + [END_INDEX]\n",
        "    src_tokens = src_tokens[:max_len]\n",
        "    src_padding = [PAD_INDEX] * (max_len - len(src_tokens))\n",
        "    src_tensor = torch.tensor(src_tokens + src_padding).unsqueeze(0).to(device)\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "\n",
        "    memory = model.encoder(src_tensor, src_mask)\n",
        "    tgt_tokens = [START_INDEX]\n",
        "    for _ in range(max_len - 1):\n",
        "        tgt_tensor = torch.tensor(tgt_tokens).unsqueeze(0).to(device)\n",
        "        tgt_mask = model.make_tgt_mask(tgt_tensor)\n",
        "        output = model.decoder(tgt_tensor, memory, src_mask, tgt_mask)\n",
        "        pred_token = output.argmax(2)[:, -1].item()\n",
        "        if pred_token == END_INDEX:\n",
        "            break\n",
        "        tgt_tokens.append(pred_token)\n",
        "\n",
        "    translated_words = [index_to_word[token] for token in tgt_tokens if token not in [START_INDEX, END_INDEX, PAD_INDEX]]\n",
        "    return \" \".join(translated_words)\n",
        "\n",
        "# --- Example Inference ---\n",
        "if __name__ == '__main__':\n",
        "    # Example question from the dataset\n",
        "    sample_question = test_dataset[10]['question']\n",
        "    actual_answer = test_dataset[10]['answer']\n",
        "\n",
        "    translated_answer = translate_sentence(model, sample_question, word_to_index, index_to_word, MAX_LEN, device)\n",
        "\n",
        "    print(\"\\n--- Example Inference ---\")\n",
        "    print(f\"Question: {sample_question}\")\n",
        "    print(f\"Actual Answer: {actual_answer}\")\n",
        "    print(f\"Predicted Answer: {translated_answer}\")\n",
        "\n",
        "    # Note: The model is likely not well-trained with these hyperparameters and few epochs.\n",
        "    # The predicted answer will likely be poor without significant training."
      ]
    }
  ]
}