{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOVtKb1Yolqr2D962HsOE9x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/torchtune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/pytorch/torchtune"
      ],
      "metadata": {
        "id": "4EMN7yyfFm7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUU_iMNH6XCT",
        "outputId": "a5094793-a3b7-48bb-c620-ef8314c1f3b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Apr 28 04:14:52 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   44C    P8              12W /  72W |      1MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-gci3P4FGSO"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/pytorch/torchtune.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtune -q"
      ],
      "metadata": {
        "id": "qPrRqDF1IdfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tune -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1D0Y_FwSJVj",
        "outputId": "6badfb75-77a8-4f15-ec69-81f2f9176b0c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: tune [-h] {download,ls,cp,run,validate} ...\n",
            "\n",
            "Welcome to the TorchTune CLI!\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "subcommands:\n",
            "  {download,ls,cp,run,validate}\n",
            "    download            Download a model from the Hugging Face Hub.\n",
            "    ls                  List all built-in recipes and configs\n",
            "    cp                  Copy a built-in recipe or config to a local path.\n",
            "    run                 Run a recipe. For distributed recipes, this supports all torchrun\n",
            "                        arguments.\n",
            "    validate            Validate a config and ensure that it is well-formed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/torchtune/torchtune"
      ],
      "metadata": {
        "id": "LXavtnr1JkgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/torchtune/torchtune/_cli/tune.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR8FkQGPMAFX",
        "outputId": "b41583d7-43eb-42ad-dbe6-7d6364d99e66"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: tune [-h] {download,ls,cp,run,validate} ...\n",
            "\n",
            "Welcome to the TorchTune CLI!\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "subcommands:\n",
            "  {download,ls,cp,run,validate}\n",
            "    download            Download a model from the Hugging Face Hub.\n",
            "    ls                  List all built-in recipes and configs\n",
            "    cp                  Copy a built-in recipe or config to a local path.\n",
            "    run                 Run a recipe. For distributed recipes, this supports all torchrun\n",
            "                        arguments.\n",
            "    validate            Validate a config and ensure that it is well-formed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#meta-llama/Llama-2-7b-hf"
      ],
      "metadata": {
        "id": "6X3PUlxmOSGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install huggingface_hub -q\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ],
      "metadata": {
        "id": "vbsklIk4PHMz"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZYsw91hO_us",
        "outputId": "ecbcbe3a-26dd-42d5-b4fc-cf40ad7f58e4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!tune download meta-llama/Llama-2-7b-hf \\\n",
        "#--output-dir /tmp/Llama-2-7b-hf \\\n",
        "#--hf-token <HF_TOKEN> \\"
      ],
      "metadata": {
        "id": "kwtSyElRWqGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "'HF_TOKEN' is in my Enviroment"
      ],
      "metadata": {
        "id": "GB6oWyZF-EVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!which tune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f0zvfOjDqEy",
        "outputId": "d9c99450-c8f8-4e0a-b32b-e6cc8271c37c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/tune\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%mkdir -p /tmp/Llama-2-7b-hf\n",
        "#!tune download meta-llama/Llama-2-7b-hf --output-dir /tmp/Llama-2-7b-hf\n",
        "\n",
        "!tune download mistralai/Mistral-7B-v0.1 --output-dir /tmp/Mistral-7B-v0.1"
      ],
      "metadata": {
        "id": "kUl8r3cbLwQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls -ltha /tmp/Llama-2-7b-hf/*\n",
        "\n",
        "!ls -ltha /tmp/Mistral-7B-v0.1/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7yxsyvRUXL1",
        "outputId": "f2a10279-128b-44c1-806c-5b2f0b508d45"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lrwxrwxrwx 1 root root  139 Apr 28 08:15 /tmp/Mistral-7B-v0.1/pytorch_model-00001-of-00002.bin -> ../../root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/blobs/67b1ea77d83cf017d6aa2fd9aadc6ad043a0cb3233a1cf9c422916b88350991f\n",
            "lrwxrwxrwx 1 root root  139 Apr 28 08:15 /tmp/Mistral-7B-v0.1/pytorch_model-00002-of-00002.bin -> ../../root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/blobs/1feecce04754087e8e9a320847916ed57c6539ed0e5e2cd0ebdc7a816cc3773e\n",
            "-rw-r--r-- 1 root root 1.8M Apr 28 08:15 /tmp/Mistral-7B-v0.1/tokenizer.json\n",
            "-rw-r--r-- 1 root root 482K Apr 28 08:15 /tmp/Mistral-7B-v0.1/tokenizer.model\n",
            "-rw-r--r-- 1 root root  967 Apr 28 08:15 /tmp/Mistral-7B-v0.1/tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   72 Apr 28 08:15 /tmp/Mistral-7B-v0.1/special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 1.4K Apr 28 08:15 /tmp/Mistral-7B-v0.1/README.md\n",
            "-rw-r--r-- 1 root root  116 Apr 28 08:15 /tmp/Mistral-7B-v0.1/generation_config.json\n",
            "-rw-r--r-- 1 root root  25K Apr 28 08:15 /tmp/Mistral-7B-v0.1/model.safetensors.index.json\n",
            "-rw-r--r-- 1 root root  24K Apr 28 08:15 /tmp/Mistral-7B-v0.1/pytorch_model.bin.index.json\n",
            "-rw-r--r-- 1 root root  571 Apr 28 08:15 /tmp/Mistral-7B-v0.1/config.json\n",
            "-rw-r--r-- 1 root root    0 Apr 28 08:06 /tmp/Mistral-7B-v0.1/log_1714291582.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QLoRA has about 75% smaller peak GPU memory usage compared to LoRA. LoRA is about 66% faster than QLoRA in terms of tuning speed. While both methods are relatively inexpensive, LoRA is up to 40% less expensive than QLoRA. Higher max sequence length increases GPU memory consumption."
      ],
      "metadata": {
        "id": "eHOPm_Wqu_vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tune ls\n",
        "\n",
        "#RECIPE                                   CONFIG\n",
        "#full_finetune_single_device              llama2/7B_full_low_memory\n",
        "#                                         mistral/7B_full_low_memory\n",
        "#full_finetune_distributed                llama2/7B_full\n",
        "#                                         llama2/13B_full\n",
        "#                                         mistral/7B_full\n",
        "#lora_finetune_single_device              llama2/7B_lora_single_device\n",
        "#                                         llama2/7B_qlora_single_device\n",
        "#                                         mistral/7B_lora_single_device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exzSiuX2t5QX",
        "outputId": "05bd7953-39d8-4ee8-b536-4594b89ef3f7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RECIPE                                   CONFIG                                  \n",
            "full_finetune_single_device              llama2/7B_full_low_memory               \n",
            "                                         llama3/8B_full_single_device            \n",
            "                                         mistral/7B_full_low_memory              \n",
            "full_finetune_distributed                llama2/7B_full                          \n",
            "                                         llama2/13B_full                         \n",
            "                                         llama3/8B_full                          \n",
            "                                         mistral/7B_full                         \n",
            "                                         gemma/2B_full                           \n",
            "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
            "                                         llama2/7B_qlora_single_device           \n",
            "                                         llama3/8B_lora_single_device            \n",
            "                                         llama3/8B_qlora_single_device           \n",
            "                                         llama2/13B_qlora_single_device          \n",
            "                                         mistral/7B_lora_single_device           \n",
            "                                         mistral/7B_qlora_single_device          \n",
            "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
            "lora_finetune_distributed                llama2/7B_lora                          \n",
            "                                         llama2/13B_lora                         \n",
            "                                         llama2/70B_lora                         \n",
            "                                         llama3/8B_lora                          \n",
            "                                         mistral/7B_lora                         \n",
            "generate                                 generation                              \n",
            "eleuther_eval                            eleuther_evaluation                     \n",
            "quantize                                 quantization                            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ERROR WITH T4 GPU:  RuntimeError: bf16 precision was requested but not available on this hardware. Please use fp32 precision instead."
      ],
      "metadata": {
        "id": "XywjcsOj__LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/torchtune/stable/_modules/torchtune/utils/precision.html"
      ],
      "metadata": {
        "id": "VJYIEyyoVifn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/torchtune/stable/tutorials/first_finetune_tutorial.html\n",
        "\n",
        "\n",
        "https://pytorch.org/torchtune/stable/tutorials/e2e_flow.html#e2e-flow\n"
      ],
      "metadata": {
        "id": "Tu3fJH7JZsQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!tune run lora_finetune_single_device --config llama2/7B_lora_single_device\n",
        "\n",
        "!tune run lora_finetune_single_device --config mistral/7B_lora_single_device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye85PGqPF6ah",
        "outputId": "fde3f30a-8430-46b4-ce3d-d326b3db95dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils.logging:Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
            "\n",
            "batch_size: 4\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Mistral-7B-v0.1\n",
            "  checkpoint_files:\n",
            "  - pytorch_model-00001-of-00002.bin\n",
            "  - pytorch_model-00002-of-00002.bin\n",
            "  model_type: MISTRAL\n",
            "  output_dir: /tmp/Mistral-7B-v0.1\n",
            "  recipe_checkpoint: null\n",
            "compile: false\n",
            "dataset:\n",
            "  _component_: torchtune.datasets.alpaca_dataset\n",
            "  train_on_input: true\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "enable_activation_checkpointing: true\n",
            "epochs: 3\n",
            "gradient_accumulation_steps: 4\n",
            "log_every_n_steps: null\n",
            "loss:\n",
            "  _component_: torch.nn.CrossEntropyLoss\n",
            "lr_scheduler:\n",
            "  _component_: torchtune.modules.get_cosine_schedule_with_warmup\n",
            "  num_warmup_steps: 100\n",
            "max_steps_per_epoch: null\n",
            "metric_logger:\n",
            "  _component_: torchtune.utils.metric_logging.DiskLogger\n",
            "  log_dir: /tmp/Mistral-7B-v0.1\n",
            "model:\n",
            "  _component_: torchtune.models.mistral.lora_mistral_7b\n",
            "  apply_lora_to_mlp: true\n",
            "  apply_lora_to_output: true\n",
            "  lora_alpha: 16\n",
            "  lora_attn_modules:\n",
            "  - q_proj\n",
            "  - k_proj\n",
            "  - v_proj\n",
            "  lora_rank: 64\n",
            "optimizer:\n",
            "  _component_: torch.optim.AdamW\n",
            "  lr: 2.0e-05\n",
            "output_dir: /tmp/Mistral-7B-v0.1\n",
            "profiler:\n",
            "  _component_: torchtune.utils.profiler\n",
            "  enabled: false\n",
            "  output_dir: /tmp/alpaca-llama2-finetune/torchtune_perf_tracing.json\n",
            "resume_from_checkpoint: false\n",
            "seed: null\n",
            "shuffle: true\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.mistral.mistral_tokenizer\n",
            "  path: /tmp/Mistral-7B-v0.1/tokenizer.model\n",
            "\n",
            "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 2051386828. Local seed is seed + rank = 2051386828 + 0\n",
            "Writing logs to /tmp/Mistral-7B-v0.1/log_1714292225.txt\n",
            "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils.logging:Memory Stats after model init:\n",
            "{'peak_memory_active': 15.789400576, 'peak_memory_alloc': 15.789400576, 'peak_memory_reserved': 15.797846016}\n",
            "INFO:torchtune.utils.logging:Tokenizer is initialized from file.\n",
            "INFO:torchtune.utils.logging:Optimizer and loss are initialized.\n",
            "INFO:torchtune.utils.logging:Loss is initialized.\n",
            "Downloading readme: 100% 7.47k/7.47k [00:00<00:00, 21.7MB/s]\n",
            "Downloading data: 100% 24.2M/24.2M [00:00<00:00, 35.6MB/s]\n",
            "Generating train split: 100% 52002/52002 [00:00<00:00, 345506.76 examples/s]\n",
            "INFO:torchtune.utils.logging:Dataset and Sampler are initialized.\n",
            "INFO:torchtune.utils.logging:Learning rate scheduler is initialized.\n",
            "1|84|Loss: 1.8583787679672241:   1% 83/13001 [01:17<2:53:15,  1.24it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#[_checkpointer.py:473] Model checkpoint of size 9.98 GB saved to <checkpoint_dir>/hf_model_0001_0.pt\n",
        "\n",
        "#[_checkpointer.py:473] Model checkpoint of size 3.50 GB saved to <checkpoint_dir>/hf_model_0002_0.pt\n",
        "\n",
        "#[_checkpointer.py:484] Adapter checkpoint of size 0.01 GB saved to <checkpoint_dir>/adapter_0.pt"
      ],
      "metadata": {
        "id": "mWviV1LYjA5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!tune cp eleuther_evaluation ./custom_eval_config.yaml \\\n",
        "\n",
        "!tune cp eleuther_evaluation /content/custom_eval_config.yaml"
      ],
      "metadata": {
        "id": "EXFgoTYGiQWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tune run eleuther_eval --config /content/custom_eval_config.yaml\n",
        "checkpointer.checkpoint_dir=<checkpoint_dir> \\\n",
        "tokenizer.path=<checkpoint_dir>/tokenizer.model\n",
        "\n",
        "#[evaluator.py:324] Running loglikelihood requests\n",
        "#[eleuther_eval.py:195] Eval completed in 121.27 seconds.\n",
        "#[eleuther_eval.py:197] truthfulqa_mc2: {'acc,none': 0.388..."
      ],
      "metadata": {
        "id": "uFmQRzg2iTUG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}