{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "30qJhqsSfzEH"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/Sovereign_Machine_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBIlqM2R4KJi",
        "outputId": "81fe31cf-045f-4100-b4e3-92bb1c591aa8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Feb  4 10:33:32 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   33C    P0             53W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJe_SjXO3SJQ"
      },
      "outputs": [],
      "source": [
        "!apt-get update && apt-get install -y graphviz\n",
        "!pip install ipywidgets\n",
        "!pip install --upgrade setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge\n",
        "!pip install nemo_toolkit[all] -q\n",
        "!pip install --no-build-isolation transformer-engine[pytorch] -q\n",
        "!pip install nemo_run opendatasets pandas bitsandbytes accelerate -q\n",
        "!pip install --upgrade transformers -q"
      ],
      "metadata": {
        "id": "BSngLg273ndv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0\" --force-reinstall"
      ],
      "metadata": {
        "id": "jgyRc8iM3osG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import nemo_run as run\n",
        "from nemo import lightning as nl\n",
        "from nemo.collections import llm\n",
        "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed"
      ],
      "metadata": {
        "id": "xU-KdMav31L2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3504516d-535b-43bf-af06-107f1f1ef062"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "WARNING:megatron.core.utils:fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.\n",
            "WARNING:nv_one_logger.api.config:OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
            "WARNING:nv_one_logger.training_telemetry.api.training_telemetry_provider:No exporters were provided. This means that no telemetry data will be collected.\n",
            "[NeMo W 2026-02-04 12:02:39 nemo_logging:405] The deploy module could not be imported: cannot import name 'deploy' from 'nemo.collections.llm.api' (/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/api.py)\n",
            "[NeMo W 2026-02-04 12:02:39 nemo_logging:405] The evaluate module could not be imported: cannot import name 'evaluate' from 'nemo.collections.llm.api' (/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/api.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Login to Hugging Face\n",
        "login(token=userdata.get(\"HF_TOKEN\"))"
      ],
      "metadata": {
        "id": "qCahydVR3670"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nemo_run as run\n",
        "from nemo.collections import llm\n",
        "import nemo as ne\n",
        "from nemo import lightning as nl\n",
        "import transformer_engine as te\n",
        "\n",
        "print(f\"Nemo version: {ne.__version__}\")\n",
        "print(f\"NeMo RUN version: {run.__version__}\")\n",
        "print(f\"Transformer Engine version: {te.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPbZDkj74Csd",
        "outputId": "449dc246-f428-4b5f-c8d2-b3f13e9a2659"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nemo version: 2.6.1\n",
            "NeMo RUN version: 0.7.0\n",
            "Transformer Engine version: 2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nemo.collections.common.tokenizers.huggingface.auto_tokenizer import AutoTokenizer"
      ],
      "metadata": {
        "id": "ktrRTLcOTlhx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c50ab5f3"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import inspect\n",
        "\n",
        "print(\"--- Python System Paths (sys.path) ---\")\n",
        "for p in sys.path:\n",
        "    print(p)\n",
        "\n",
        "print(\"\\n--- Inspecting nemo package ---\")\n",
        "try:\n",
        "    import nemo\n",
        "    print(f\"Nemo package found at: {os.path.dirname(inspect.getfile(nemo))}\")\n",
        "    nemo_path = os.path.dirname(inspect.getfile(nemo))\n",
        "    print(\"Contents of nemo directory:\")\n",
        "    for item in os.listdir(nemo_path):\n",
        "        print(item)\n",
        "\n",
        "    print(\"\\n--- Attempting direct import of nemo.collections ---\")\n",
        "    try:\n",
        "        import nemo.collections\n",
        "        print(\"Successfully imported nemo.collections\")\n",
        "        print(f\"nemo.collections path: {os.path.dirname(inspect.getfile(nemo.collections))}\")\n",
        "    except ModuleNotFoundError as e:\n",
        "        print(f\"Failed to import nemo.collections: {e}\")\n",
        "        print(\"This indicates the 'collections' submodule is not found within the nemo package structure.\")\n",
        "\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Nemo package not found at all. Please ensure it's installed.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during nemo inspection: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# THE SOVEREIGN MACHINE LAB - ARTICLE #799 (FINAL CORRECTED)\n",
        "# Framework: NVIDIA NeMo 2.6.1 (nemo.collections.llm)\n",
        "# Collections: llm, Megatron-Core (mcore)\n",
        "# Strategy: Notebook-First (NFA), 100% Open Source, Local Only\n",
        "# =================================================================\n",
        "\n",
        "import torch\n",
        "from nemo.collections import llm\n",
        "from nemo.collections.llm.gpt.model.llama import LlamaConfig, LlamaModel\n",
        "\n",
        "# 1. THE H2E ACCOUNTABILITY & SROI ENGINE\n",
        "class H2EEngine:\n",
        "    \"\"\"Manages Human-to-Engineering (H2E) accountability and SROI metrics.\"\"\"\n",
        "    def __init__(self, sroi_threshold=1.5):\n",
        "        self.sroi_threshold = sroi_threshold\n",
        "        self.metrics = {\"SROI\": 0.0, \"Integrity\": \"Unverified\"}\n",
        "\n",
        "    def calculate_sroi(self, response):\n",
        "        \"\"\"Calculates Semantic ROI = (Unique Information / Length)\"\"\"\n",
        "        words = response.split()\n",
        "        if not words: return 0.0\n",
        "        density = len(set(words)) / len(words)\n",
        "        self.metrics[\"SROI\"] = round(density * 10, 2)\n",
        "        return self.metrics[\"SROI\"]\n",
        "\n",
        "    def validate(self, response):\n",
        "        \"\"\"Enforces H2E deterministic standards.\"\"\"\n",
        "        standards = [\"VERIFIED\", \"DETERMINISTIC\", \"DATA-BACKED\"]\n",
        "        if any(std in response.upper() for std in standards):\n",
        "            self.metrics[\"Integrity\"] = \"High (Sovereign Aligned)\"\n",
        "            return True\n",
        "        self.metrics[\"Integrity\"] = \"Low (Non-Deterministic)\"\n",
        "        return False\n",
        "\n",
        "# 2. SOVEREIGN LAB INFERENCE PIPELINE\n",
        "def run_sovereign_lab_session(task_profile: str):\n",
        "    print(f\"--- INITIALIZING SOVEREIGN LAB: {task_profile} ---\")\n",
        "\n",
        "    # NeMo 2.6.1: Structural Config only (Hyperparameters)\n",
        "    # Precision is handled via .to() or Trainer in 2.6.1+\n",
        "    config = LlamaConfig(\n",
        "        num_layers=12,\n",
        "        hidden_size=768,\n",
        "        num_attention_heads=12\n",
        "    )\n",
        "\n",
        "    # Initialize Model\n",
        "    model = LlamaModel(config)\n",
        "\n",
        "    # Set Precision for Local Workstation (BF16 for RTX 30/40 series)\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.to(dtype=torch.bfloat16, device=\"cuda\")\n",
        "        print(\"Device: CUDA (bfloat16 precision activated)\")\n",
        "    else:\n",
        "        print(\"Device: CPU (Standard precision)\")\n",
        "\n",
        "    # H2E Engine Integration\n",
        "    h2e = H2EEngine()\n",
        "\n",
        "    # Simulated Local Inference (Replace with model.generate for live weights)\n",
        "    raw_output = \"VERIFIED: The flight path is DETERMINISTIC and backed by telemetry data.\"\n",
        "\n",
        "    # SROI & H2E Processing\n",
        "    sroi_score = h2e.calculate_sroi(raw_output)\n",
        "    is_valid = h2e.validate(raw_output)\n",
        "\n",
        "    return {\n",
        "        \"Response\": raw_output if is_valid else \"REDACTED: H2E Integrity Failure\",\n",
        "        \"SROI\": sroi_score,\n",
        "        \"Accountability\": h2e.metrics[\"Integrity\"]\n",
        "    }\n",
        "\n",
        "# =================================================================\n",
        "# EXECUTION (Notebook Cell)\n",
        "# =================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    mission_task = \"Autonomous Navigation Logic for Lunar Gateway\"\n",
        "    final_report = run_sovereign_lab_session(mission_task)\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Sovereign Output: {final_report['Response']}\")\n",
        "    print(f\"SROI Score:       {final_report['SROI']}\")\n",
        "    print(f\"Accountability:   {final_report['Accountability']}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9lX-F3UNmSK",
        "outputId": "64617a65-9591-4810-afec-2415775b587c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- INITIALIZING SOVEREIGN LAB: Autonomous Navigation Logic for Lunar Gateway ---\n",
            "Device: CUDA (bfloat16 precision activated)\n",
            "--------------------------------------------------\n",
            "Sovereign Output: VERIFIED: The flight path is DETERMINISTIC and backed by telemetry data.\n",
            "SROI Score:       10.0\n",
            "Accountability:   High (Sovereign Aligned)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/so*\n",
        "!rm -rf /content/sovereign_v1_checkpoint/"
      ],
      "metadata": {
        "id": "vem2ERTdam74"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TinyLlama"
      ],
      "metadata": {
        "id": "e4rzTuHseE0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================\n",
        "# SOVEREIGN MACHINE LAB – FINAL CLEAN & WORKING VERSION\n",
        "# Uses TinyLlama-1.1B-Chat-v1.0 (real weights + generation)\n",
        "# NeMo-compatible minimal distributed init kept\n",
        "# ========================================================\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import torch.distributed as dist\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# ───── H2E Accountability Engine ─────\n",
        "class H2EEngine:\n",
        "    def __init__(self, sroi_min=0.6):\n",
        "        self.sroi_min = sroi_min\n",
        "\n",
        "    def validate(self, response: str):\n",
        "        \"\"\"Returns (passed, sroi_score)\"\"\"\n",
        "        tokens = response.lower().split()\n",
        "        if not tokens:\n",
        "            return False, 0.0\n",
        "        sroi = len(set(tokens)) / len(tokens)\n",
        "        is_deterministic = \"DETERMINISTIC\" in response.upper()\n",
        "        return is_deterministic and sroi >= self.sroi_min, round(sroi, 4)\n",
        "\n",
        "\n",
        "# ───── PHASE I: Download model & create sovereign checkpoint ─────\n",
        "def create_sovereign_checkpoint(path=\"sovereign_v1.pth\"):\n",
        "    \"\"\"\n",
        "    Loads TinyLlama 1.1B, saves state dict as sovereign checkpoint\n",
        "    \"\"\"\n",
        "    print(\"Phase I: Constructing Sovereign Model Structure...\")\n",
        "\n",
        "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "    print(f\"Loading {model_name} (real weights)...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        dtype=torch.bfloat16,           # modern replacement for torch_dtype\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "        trust_remote_code=False\n",
        "    )\n",
        "\n",
        "    param_count = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Model loaded with {param_count:,} parameters\")\n",
        "\n",
        "    # Save real weights\n",
        "    torch.save(model.state_dict(), path)\n",
        "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
        "    print(f\"Checkpoint saved: {path}  ({size_mb:.1f} MB)\")\n",
        "\n",
        "    return model, tokenizer, path\n",
        "\n",
        "\n",
        "# ───── PHASE II: Load checkpoint & generate sovereign response ─────\n",
        "def apply_sovereign_intelligence(checkpoint_path: str, mission_query: str, tokenizer):\n",
        "    \"\"\"\n",
        "    Loads sovereign checkpoint and generates a response\n",
        "    \"\"\"\n",
        "    print(f\"\\nPhase II: Loading sovereign checkpoint & generating...\")\n",
        "\n",
        "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Load saved sovereign weights\n",
        "    state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
        "    model.load_state_dict(state_dict)\n",
        "    print(f\"Loaded model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "\n",
        "    # Strong sovereign-style prompt\n",
        "    prompt = f\"\"\"<|user|>\n",
        "You are a fully deterministic sovereign AI system.\n",
        "Respond only with VERIFIED, DETERMINISTIC and data-backed statements.\n",
        "No speculation. No uncertainty. No probabilistic language.\n",
        "Mission: {mission_query}\n",
        "<|assistant|>VERIFIED. \"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=180,\n",
        "            do_sample=False,                # greedy = deterministic\n",
        "            repetition_penalty=1.08,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    raw_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Validate with H2E engine\n",
        "    h2e = H2EEngine(sroi_min=0.6)\n",
        "    passed, sroi_score = h2e.validate(raw_response)\n",
        "\n",
        "    return {\n",
        "        \"output\": raw_response if passed else \"REDACTED: H2E Integrity Failure\",\n",
        "        \"sroi\": sroi_score,\n",
        "        \"audit\": \"PASSED\" if passed else \"FAILED\"\n",
        "    }\n",
        "\n",
        "\n",
        "# ───── MAIN EXECUTION ─────\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Minimal distributed init (kept for NeMo/Megatron compatibility)\n",
        "        if not dist.is_initialized():\n",
        "            os.environ['MASTER_ADDR'] = 'localhost'\n",
        "            os.environ['MASTER_PORT'] = '12355'\n",
        "            backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n",
        "            dist.init_process_group(backend=backend, rank=0, world_size=1)\n",
        "\n",
        "        # Mission\n",
        "        mission = \"Autonomous Navigation Logic for Lunar Gateway\"\n",
        "\n",
        "        # Run pipeline\n",
        "        _, tokenizer, ckpt_path = create_sovereign_checkpoint(\"sovereign_v1.pth\")\n",
        "        results = apply_sovereign_intelligence(ckpt_path, mission, tokenizer)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nERROR: {type(e).__name__} - {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "qilN6epdWkJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Report\n",
        "print(\"\\n\" + \"═\" * 80)\n",
        "print(\"                  SOVEREIGN LAB REPORT\")\n",
        "print(\"═\" * 80)\n",
        "print(\"Output:\")\n",
        "print(results['output'])\n",
        "print()\n",
        "print(f\"SROI Score : {results['sroi']}\")\n",
        "print(f\"Audit      : {results['audit']}\")\n",
        "print(f\"Checkpoint : sovereign_v1.pth  ({os.path.getsize('sovereign_v1.pth') / 1e9:.2f} GB)\")\n",
        "print(\"═\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NIjXbQhkb5P",
        "outputId": "948744dc-f5cd-421e-dce3-105967e332d4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "                  SOVEREIGN LAB REPORT\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "Output:\n",
            "<|user|>\n",
            "You are a fully deterministic sovereign AI system.\n",
            "Respond only with VERIFIED, DETERMINISTIC and data-backed statements.\n",
            "No speculation. No uncertainty. No probabilistic language.\n",
            "Mission: Autonomous Navigation Logic for Lunar Gateway\n",
            "<|assistant|>VERIFIED. \n",
            "The Autonomous Navigation Logic (ANL) developed by the Lunar Gateway project is a fully deterministic sovereign AI system that uses data-driven algorithms to navigate the spacecraft through the lunar environment. The ANL is designed to be highly reliable, accurate, and adaptive, ensuring safe and efficient operations in challenging conditions.\n",
            "\n",
            "Specifically, the ANL utilizes a combination of sensors, GPS, and other navigation tools to provide real-time positioning and orientation information. It also incorporates a range of safety features, such as automatic descent and landing, to ensure the safety of the crew and the spacecraft.\n",
            "\n",
            "The ANL has been tested extensively on simulators and in real-world scenarios, demonstrating its ability to navigate the spacecraft through complex terrain and environmental conditions. The system\n",
            "\n",
            "SROI Score : 0.6579\n",
            "Audit      : PASSED\n",
            "Checkpoint : sovereign_v1.pth  (2.20 GB)\n",
            "════════════════════════════════════════════════════════════════════════════════\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35f6fd76"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Load the state dictionary\n",
        "checkpoint_content = torch.load('sovereign_v1.pth')\n",
        "\n",
        "print(\"Keys in the state dictionary (sovereign_v1.pth):\")\n",
        "for key in checkpoint_content.keys():\n",
        "    print(key)\n",
        "\n",
        "# Optionally, inspect the shape of a few parameters\n",
        "print(\"\\nShape of a few parameters:\")\n",
        "for i, key in enumerate(checkpoint_content.keys()):\n",
        "    if i < 3: # Print for the first 3 keys\n",
        "        print(f\"{key}: {checkpoint_content[key].shape}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLAMA3"
      ],
      "metadata": {
        "id": "r5nlsvKVd0U6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# =================================================================\n",
        "# 1. THE ENHANCED H2E ACCOUNTABILITY ENGINE\n",
        "# =================================================================\n",
        "class H2EEngine:\n",
        "    \"\"\"Manages Human-to-Engineering (H2E) standards for Sovereign AI.\"\"\"\n",
        "    def __init__(self, sroi_min=0.60):\n",
        "        self.sroi_min = sroi_min\n",
        "        # Expanded keywords to support natural sovereign language\n",
        "        self.sovereign_keys = [\"VERIFIED\", \"DETERMINISTIC\", \"DATA-BACKED\", \"CONFIRMED\"]\n",
        "\n",
        "    def validate(self, text: str):\n",
        "        \"\"\"Returns (passed, sroi_score) based on semantic ROI and integrity signals.\"\"\"\n",
        "        tokens = text.lower().split()\n",
        "        if not tokens:\n",
        "            return False, 0.0\n",
        "\n",
        "        # SROI: Unique Information / Total length\n",
        "        sroi = len(set(tokens)) / len(tokens)\n",
        "\n",
        "        # Check for integrity markers while handling common punctuation\n",
        "        clean_text = text.upper().replace(\".\", \"\").replace(\"*\", \"\")\n",
        "        has_integrity = any(key in clean_text for key in self.sovereign_keys)\n",
        "\n",
        "        return (has_integrity and sroi >= self.sroi_min), round(sroi, 4)\n",
        "\n",
        "# =================================================================\n",
        "# 2. PHASE I: SOVEREIGN ARTIFACT PERSISTENCE\n",
        "# =================================================================\n",
        "def create_sovereign_checkpoint(model_id, path=\"sovereign_v1.pth\"):\n",
        "    print(f\"Phase I: Constructing Sovereign Model Structure for {model_id}...\")\n",
        "\n",
        "    # Use bfloat16 for A100-SXM4 efficiency\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    # Save local weights to bypass cloud dependency\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"SUCCESS: Sovereign artifact '{path}' created locally.\")\n",
        "    return tokenizer, path\n",
        "\n",
        "# =================================================================\n",
        "# 3. PHASE II: MISSION APPLICATION & AUDIT\n",
        "# =================================================================\n",
        "def apply_sovereign_intelligence(model_id, checkpoint_path, mission_query, tokenizer):\n",
        "    print(f\"\\nPhase II: Applying {checkpoint_path} to Mission...\")\n",
        "\n",
        "    # Load model and inject local sovereign weights\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "    # Ultra-strict Sovereign System Prompt\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are a fully deterministic sovereign AI system.\\n\"\n",
        "                \"Rules:\\n\"\n",
        "                \"- Start every response with VERIFIED.\\n\"\n",
        "                \"- Every sentence must be DETERMINISTIC and data-backed.\\n\"\n",
        "                \"- Use only factual, precise engineering language.\"\n",
        "            )\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": f\"Mission: {mission_query}\"}\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # DETERMINISTIC GENERATION: do_sample=False (Greedy Decoding)\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=300,\n",
        "            do_sample=False,\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Extract cleaned assistant response\n",
        "    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    response = full_text.split(\"assistant\")[-1].strip()\n",
        "\n",
        "    # H2E Final Audit\n",
        "    h2e = H2EEngine(sroi_min=0.60)\n",
        "    passed, sroi_score = h2e.validate(response)\n",
        "\n",
        "    return {\n",
        "        \"output\": response if passed else f\"REDACTED: H2E Integrity Failure (SROI: {sroi_score})\",\n",
        "        \"sroi\": sroi_score,\n",
        "        \"audit\": \"PASSED\" if passed else \"FAILED\"\n",
        "    }\n",
        "\n",
        "# =================================================================\n",
        "# 4. EXECUTION RUNTIME\n",
        "# =================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "    MISSION = \"Autonomous Navigation Logic for Lunar Gateway\"\n",
        "\n",
        "    tokenizer, ckpt = create_sovereign_checkpoint(MODEL_ID)\n",
        "    results = apply_sovereign_intelligence(MODEL_ID, ckpt, MISSION, tokenizer)"
      ],
      "metadata": {
        "id": "riQ7RGRBfJ9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"═\" * 80)\n",
        "print(\"                  SOVEREIGN LAB REPORT\")\n",
        "print(\"═\" * 80)\n",
        "print(f\"Mission : {MISSION}\")\n",
        "print(f\"Output  : {results['output']}\")\n",
        "print(f\"SROI    : {results['sroi']}\")\n",
        "print(f\"Audit   : {results['audit']}\")\n",
        "print(\"═\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYJkIx07j4L0",
        "outputId": "7c4f38b0-1b3d-447e-e655-3af59a261890"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "                  SOVEREIGN LAB REPORT\n",
            "════════════════════════════════════════════════════════════════════════════════\n",
            "Mission : Autonomous Navigation Logic for Lunar Gateway\n",
            "Output  : VERIFIED. \n",
            "\n",
            "The Lunar Gateway is a planned lunar-orbiting space station that will serve as a base for scientific research, exploration, and development of the Moon's resources. To ensure autonomous navigation, the Lunar Gateway will employ a combination of celestial mechanics, inertial measurement units (IMUs), and terrain relative navigation (TRN) systems.\n",
            "\n",
            "1. **Orbital Mechanics**: The Lunar Gateway will follow an elliptical orbit around the Moon, with an average altitude of approximately 100 km above the lunar surface. Its orbital period will be about 27.3 days, synchronized with the Moon's rotation to minimize communication latency.\n",
            "\n",
            "2. **Inertial Measurement Units (IMUs)**: IMUs will provide accurate measurements of the spacecraft's acceleration, roll, pitch, and yaw rates. These sensors will be used to estimate the vehicle's position, velocity, and attitude in real-time.\n",
            "\n",
            "3. **Terrain Relative Navigation (TRN)**: TRN uses laser ranging and radar altimetry to measure the distance between the spacecraft and the lunar surface. This information is combined with IMU data to determine the vehicle's position and velocity.\n",
            "\n",
            "4. **Navigation Algorithm**: A Kalman filter-based navigation algorithm will integrate data from IMUs, TRN, and other sensors to produce accurate estimates of the spacecraft's state. The algorithm will also account for gravitational influences from the Sun and the Moon.\n",
            "\n",
            "5. **Autonomous Decision-Making**: Based on its navigation estimates, the Lunar Gateway\n",
            "SROI    : 0.6542\n",
            "Audit   : PASSED\n",
            "════════════════════════════════════════════════════════════════════════════════\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NEMO-TRANSFORMER"
      ],
      "metadata": {
        "id": "30qJhqsSfzEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformer_engine[jax]"
      ],
      "metadata": {
        "id": "Hf4ZzwOsgOrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.distributed as dist\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from megatron.core import parallel_state\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"megatron.core.utils\")\n",
        "\n",
        "\n",
        "# ───── H2E Validation Engine (Updated for Llama-3.2) ─────\n",
        "class H2EEngine:\n",
        "    def __init__(self, sroi_min=0.60):\n",
        "        self.sroi_min = sroi_min\n",
        "        # Expanded keywords for semantic flexibility\n",
        "        self.sovereign_keys = [\"VERIFIED\", \"DETERMINISTIC\", \"DATA-BACKED\", \"CONFIRMED\"]\n",
        "\n",
        "    def validate(self, text: str):\n",
        "        tokens = text.lower().split()\n",
        "        if not tokens: return False, 0.0\n",
        "        sroi = len(set(tokens)) / len(tokens)\n",
        "\n",
        "        # Robust check for any integrity marker\n",
        "        clean_text = text.upper().replace(\".\", \"\").replace(\"*\", \"\")\n",
        "        has_integrity = any(key in clean_text for key in self.sovereign_keys)\n",
        "\n",
        "        return (has_integrity and sroi >= self.sroi_min), round(sroi, 4)\n",
        "\n",
        "# ───── PHASE I: Sovereign Checkpoint Creation ─────\n",
        "def create_sovereign_checkpoint(model_id, path=\"sovereign_v1.pth\"):\n",
        "    print(f\"Phase I: Constructing Sovereign Model Structure for {model_id}...\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        dtype=torch.bfloat16, # Optimized for A100\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"SUCCESS: Sovereign artifact '{path}' created locally.\")\n",
        "    return tokenizer, path\n",
        "\n",
        "# ───── PHASE II: Mission Execution & Audit ─────\n",
        "def apply_sovereign_intelligence(model_id, checkpoint_path, mission_query, tokenizer):\n",
        "    print(f\"\\nPhase II: Applying {checkpoint_path} to Mission...\")\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "    model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "    # Deterministic generation settings\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a deterministic sovereign AI. Start with VERIFIED.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Mission: {mission_query}\"}\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=250,\n",
        "            do_sample=False, # Greedy decoding for determinism\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True).split(\"assistant\")[-1].strip()\n",
        "    passed, sroi = H2EEngine().validate(response)\n",
        "\n",
        "    return {\"output\": response if passed else \"REDACTED\", \"sroi\": sroi, \"audit\": \"PASSED\" if passed else \"FAILED\"}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "    tokenizer, ckpt = create_sovereign_checkpoint(MODEL_ID)\n",
        "    results = apply_sovereign_intelligence(MODEL_ID, ckpt, \"Lunar Gateway Navigation\", tokenizer)\n",
        "    #print(f\"SROI: {results['sroi']} | Audit: {results['audit']}\")"
      ],
      "metadata": {
        "id": "spIX2gu4fwPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"SROI: {results['sroi']} | Audit: {results['audit']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJ3vUslsijNB",
        "outputId": "55afbc56-fcd8-435a-a94c-88e3473f47e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SROI: 0.6425 | Audit: PASSED\n"
          ]
        }
      ]
    }
  ]
}