{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/Copy_of_model_load_work_transformer_demo_fp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb8HQCGUhEgE"
      },
      "source": [
        "## Model environmenmt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GM7ASLUOgp4N"
      },
      "outputs": [],
      "source": [
        "!pip install colab-env -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30HOQNnVg1D6"
      },
      "outputs": [],
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write, # ADD YOUR TOKEN HERE\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpoVJAMMt74m"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch geopy -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7cb3yWFuyoy"
      },
      "outputs": [],
      "source": [
        "!pip show transformers\n",
        "print()\n",
        "!pip show datasets\n",
        "print()\n",
        "!pip show torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsnHRSWBL5Va"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5IbgqgPUzWws"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.distance import geodesic\n",
        "from tqdm import tqdm\n",
        "from geopy.point import Point  # Import Point object\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Define airport codes and names\n",
        "airports = [\n",
        "    # North America\n",
        "    \"ATL\", \"LAX\", \"ORD\", \"DFW\", \"DEN\", \"JFK\", \"SFO\", \"LAS\", \"SEA\", \"CLT\",\n",
        "    \"MIA\", \"PHL\", \"EWR\", \"BOS\", \"PHX\", \"MCO\", \"IAH\", \"DAL\", \"FLL\", \"DTW\",\n",
        "    \"MSP\", \"IAD\", \"BWI\", \"SLC\", \"SAN\", \"MDW\", \"TPA\", \"PDX\", \"BNA\", \"STL\",\n",
        "    # Europe\n",
        "    \"LHR\", \"CDG\", \"AMS\", \"FRA\", \"MAD\", \"BCN\", \"FCO\", \"MUC\", \"DUB\", \"ZRH\",\n",
        "    # Asia\n",
        "    \"HND\", \"PVG\", \"ICN\", \"SIN\", \"KUL\", \"BKK\", \"DXB\", \"DEL\", \"HKG\", \"NRT\",\n",
        "    # Caribbean\n",
        "    \"CUN\", \"PUJ\", \"SJU\", \"MBJ\", \"NAS\",\n",
        "    # South America\n",
        "    \"GRU\", \"BOG\", \"EZE\", \"SCL\", \"LIM\",\n",
        "    # Australia & New Zealand\n",
        "    \"SYD\", \"MEL\", \"BNE\", \"PER\", \"AKL\"\n",
        "]\n",
        "\n",
        "airport_names = {\n",
        "    \"ATL\": \"Hartsfield-Jackson Atlanta International Airport\",\n",
        "    \"LAX\": \"Los Angeles International Airport\",\n",
        "    \"ORD\": \"Chicago O'Hare International Airport\",\n",
        "    \"DFW\": \"Dallas/Fort Worth International Airport\",\n",
        "    \"DEN\": \"Denver International Airport\",\n",
        "    \"JFK\": \"John F. Kennedy International Airport\",\n",
        "    \"SFO\": \"San Francisco International Airport\",\n",
        "    \"LAS\": \"Harry Reid International Airport\",\n",
        "    \"SEA\": \"Seattle-Tacoma International Airport\",\n",
        "    \"CLT\": \"Charlotte Douglas International Airport\",\n",
        "    \"MIA\": \"Miami International Airport\",\n",
        "    \"PHL\": \"Philadelphia International Airport\",\n",
        "    \"EWR\": \"Newark Liberty International Airport\",\n",
        "    \"BOS\": \"Logan International Airport\",\n",
        "    \"PHX\": \"Phoenix Sky Harbor International Airport\",\n",
        "    \"MCO\": \"Orlando International Airport\",\n",
        "    \"IAH\": \"George Bush Intercontinental Airport\",\n",
        "    \"DAL\": \"Dallas Love Field\",\n",
        "    \"FLL\": \"Fort Lauderdale-Hollywood International Airport\",\n",
        "    \"DTW\": \"Detroit Metropolitan Airport\",\n",
        "    \"MSP\": \"Minneapolis-Saint Paul International Airport\",\n",
        "    \"IAD\": \"Washington Dulles International Airport\",\n",
        "    \"BWI\": \"Baltimore/Washington International Airport\",\n",
        "    \"SLC\": \"Salt Lake City International Airport\",\n",
        "    \"SAN\": \"San Diego International Airport\",\n",
        "    \"MDW\": \"Midway International Airport\",\n",
        "    \"TPA\": \"Tampa International Airport\",\n",
        "    \"PDX\": \"Portland International Airport\",\n",
        "    \"BNA\": \"Nashville International Airport\",\n",
        "    \"STL\": \"St. Louis Lambert International Airport\",\n",
        "    # Europe\n",
        "    \"LHR\": \"London Heathrow Airport\",\n",
        "    \"CDG\": \"Charles de Gaulle Airport\",\n",
        "    \"AMS\": \"Amsterdam Airport Schiphol\",\n",
        "    \"FRA\": \"Frankfurt Airport\",\n",
        "    \"MAD\": \"Adolfo Suárez Madrid–Barajas Airport\",\n",
        "    \"BCN\": \"Barcelona–El Prat Airport\",\n",
        "    \"FCO\": \"Leonardo da Vinci–Fiumicino Airport\",\n",
        "    \"MUC\": \"Munich Airport\",\n",
        "    \"DUB\": \"Dublin Airport\",\n",
        "    \"ZRH\": \"Zurich Airport\",\n",
        "    # Asia\n",
        "    \"HND\": \"Haneda Airport\",\n",
        "    \"PVG\": \"Shanghai Pudong International Airport\",\n",
        "    \"ICN\": \"Incheon International Airport\",\n",
        "    \"SIN\": \"Singapore Changi Airport\",\n",
        "    \"KUL\": \"Kuala Lumpur International Airport\",\n",
        "    \"BKK\": \"Suvarnabhumi Airport\",\n",
        "    \"DXB\": \"Dubai International Airport\",\n",
        "    \"DEL\": \"Indira Gandhi International Airport\",\n",
        "    \"HKG\": \"Hong Kong International Airport\",\n",
        "    \"NRT\": \"Narita International Airport\",\n",
        "    # Caribbean\n",
        "    \"CUN\": \"Cancún International Airport\",\n",
        "    \"PUJ\": \"Punta Cana International Airport\",\n",
        "    \"SJU\": \"Luis Muñoz Marín International Airport\",\n",
        "    \"MBJ\": \"Sangster International Airport\",\n",
        "    \"NAS\": \"Lynden Pindling International Airport\",\n",
        "    # South America\n",
        "    \"GRU\": \"São Paulo–Guarulhos International Airport\",\n",
        "    \"BOG\": \"El Dorado International Airport\",\n",
        "    \"EZE\": \"Ministro Pistarini International Airport\",\n",
        "    \"SCL\": \"Arturo Merino Benítez International Airport\",\n",
        "    \"LIM\": \"Jorge Chávez International Airport\",\n",
        "    # Australia & New Zealand\n",
        "    \"SYD\": \"Sydney Airport\",\n",
        "    \"MEL\": \"Melbourne Airport\",\n",
        "    \"BNE\": \"Brisbane Airport\",\n",
        "    \"PER\": \"Perth Airport\",\n",
        "    \"AKL\": \"Auckland Airport\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bac8KFM-kSIJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer_name = 'gpt2'\n",
        "#tokenizer_name = 'meta-llama/Meta-Llama-3-8B'\n",
        "#tokenizer_name = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "pad_token = '<pad>'\n",
        "\n",
        "# --- Load Tokenizer and Add Pad Token ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': pad_token})\n",
        "# Ensure pad_token_id is within the vocabulary size\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "if pad_token_id >= tokenizer.vocab_size:\n",
        "    pad_token_id = tokenizer.vocab_size - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7CcHjhRYZjl"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "flight_plan_dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "flight_plan_dataset['train'][7]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flight_plan_dataset"
      ],
      "metadata": {
        "id": "ewxnzBAR6ybl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def convert_token_ids_to_coords(token_ids, tokenizer):\n",
        "    \"\"\"\n",
        "    Converts token IDs to waypoint coordinates.\n",
        "\n",
        "    Args:\n",
        "        token_ids (torch.Tensor): Token IDs representing waypoints.\n",
        "        tokenizer: The tokenizer used for the model.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Waypoint coordinates as a tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Decode token IDs to text, clipping to valid vocabulary range\n",
        "    decoded_text = tokenizer.decode(token_ids[0].clamp(0, tokenizer.vocab_size - 1))  # Assuming batch size of 1 for simplicity\n",
        "\n",
        "    # *** Print the decoded text ***\n",
        "    print(f\"Decoded Text: {decoded_text}\")\n",
        "\n",
        "    # 2. Extract waypoint coordinates from text using regular expressions\n",
        "    # Assuming the waypoints are in the format \"latitude, longitude\" (e.g., 34.0522, -118.2437)\n",
        "    # Adjust the regex to match the actual waypoint format in your data\n",
        "    # Example regex: r\"(-?\\d+\\.\\d+),\\s*(-?\\d+\\.\\d+)\"\n",
        "    waypoints_text = re.findall(r\"(-?\\d+\\.\\d+),\\s*(-?\\d+\\.\\d+)\", decoded_text)  # Updated regex\n",
        "\n",
        "    # *** Print the extracted waypoints text ***\n",
        "    #print(f\"Waypoints Text: {waypoints_text}\")\n",
        "\n",
        "    # 3. Convert waypoint text to coordinate tensors\n",
        "    coordinates = []\n",
        "    for lat, lon in waypoints_text:  # Unpack lat and lon directly\n",
        "        coordinates.append([float(lat), float(lon)])\n",
        "\n",
        "    # 4. Convert to PyTorch tensor\n",
        "    coordinates_tensor = torch.tensor(coordinates)\n",
        "\n",
        "    return coordinates_tensor"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ivHUbm8mTeq5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "source": [
        "# Cell 45: custom_loss_function\n",
        "\n",
        "import torch.nn as nn # Make sure you have imported nn\n",
        "\n",
        "def custom_loss_function(predicted_waypoints, target_waypoints, label, tokenizer):\n",
        "    \"\"\"\n",
        "    Calculates the custom loss for waypoint coordinates.\n",
        "\n",
        "    Args:\n",
        "        predicted_waypoints (torch.Tensor): Model's predicted waypoints (coordinates).\n",
        "        target_waypoints (torch.Tensor): Target waypoints (token IDs).\n",
        "        label (int): The expected number of waypoints for the flight route.\n",
        "        tokenizer: The tokenizer used for the model.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The total loss.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Convert target_waypoints (token IDs) to coordinates\n",
        "    target_coords = convert_token_ids_to_coords(target_waypoints, tokenizer)\n",
        "\n",
        "    # 2. Handle empty target_coords\n",
        "    if target_coords.nelement() == 0:  # Check if target_coords is empty\n",
        "        # You can return a default loss or handle the case as appropriate\n",
        "        #print(\"Target coordinates are empty. Returning a default loss.\")\n",
        "        return torch.tensor(0.0, requires_grad=True, device=predicted_waypoints.device)  # Ensure requires_grad=True for the default loss\n",
        "\n",
        "    # 3. Calculate the distance-based loss (e.g., Mean Squared Error)\n",
        "    # Reshape predicted_waypoints to match target_coords shape if necessary\n",
        "    predicted_waypoints = predicted_waypoints.view(-1, 2)  # Reshape to (batch_size * seq_len, 2)\n",
        "\n",
        "    # Ensure target_coords has the expected shape\n",
        "    if target_coords.dim() == 1:  # If target_coords is 1D (only one coordinate pair)\n",
        "        target_coords = target_coords.unsqueeze(0)  # Add an extra dimension\n",
        "\n",
        "    target_coords = target_coords.view(-1, 2)  # Reshape to (batch_size * seq_len, 2)\n",
        "\n",
        "    # *** Print shapes before loss calculation ***\n",
        "    print(f\"Predicted Waypoints Shape: {predicted_waypoints.shape}\")\n",
        "    print(f\"Target Coords Shape: {target_coords.shape}\")\n",
        "\n",
        "    standard_loss = nn.MSELoss()(predicted_waypoints, target_coords)\n",
        "\n",
        "    # 4. Calculate the waypoint count penalty\n",
        "    # (You might need to adjust this part based on how label is used)\n",
        "    # Assuming 'label' represents the target number of waypoints\n",
        "    count_penalty = abs(predicted_waypoints.shape[0] - label.shape[0])  # Compare number of waypoints\n",
        "\n",
        "    # *** Print individual losses ***\n",
        "    print(f\"Standard Loss: {standard_loss.item()}\")\n",
        "    print(f\"Count Penalty: {count_penalty}\")\n",
        "\n",
        "    # 5. Combine the losses\n",
        "    total_loss = standard_loss + 0.1 * count_penalty\n",
        "\n",
        "    return total_loss"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "fpRtI-XFSWQZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "batch_size = 4\n",
        "sequence_length = 128  # Adjusted based on tokenize_function\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Directly use waypoint numbers as labels\n",
        "    examples[\"labels\"] = examples[\"label\"]\n",
        "    tokenized_output = tokenizer(\n",
        "        examples[\"input\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=sequence_length\n",
        "    )\n",
        "\n",
        "    # Instead of assigning labels directly, shift them for causal LM\n",
        "    tokenized_output['labels'] = tokenized_output['input_ids'].copy()\n",
        "\n",
        "    # Replace input_ids corresponding to pad_token with -100 in labels\n",
        "    tokenized_output['labels'] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in label_list]\n",
        "        for label_list in tokenized_output['labels']\n",
        "    ]\n",
        "\n",
        "    # --- Ensure input_ids and labels are within vocabulary range before padding ---\n",
        "    tokenized_output['input_ids'] = [\n",
        "        [token if token < tokenizer.vocab_size else tokenizer.unk_token_id for token in label_list]\n",
        "        for label_list in tokenized_output['input_ids']\n",
        "    ]\n",
        "\n",
        "    # --- Replace out-of-vocabulary tokens in labels with unk_token_id ---\n",
        "    tokenized_output['labels'] = [\n",
        "        [token if token < tokenizer.vocab_size else tokenizer.unk_token_id for token in label_list]\n",
        "        for label_list in tokenized_output['labels']\n",
        "    ]\n",
        "\n",
        "    # Handle case where labels are empty (could happen due to errors in data)\n",
        "    for i, label_list in enumerate(tokenized_output['labels']):\n",
        "        if len(label_list) == 0:\n",
        "            # Replace with a single UNK token or handle as appropriate\n",
        "            tokenized_output['labels'][i] = [tokenizer.unk_token_id]\n",
        "\n",
        "    return tokenized_output\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "\n",
        "# Tokenize, format, and split dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"input\", \"label\"])\n",
        "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# --- Split the 'train' dataset within the DatasetDict ---\n",
        "train_testvalid = tokenized_datasets['train'].train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Create new DatasetDict with the splits\n",
        "tokenized_datasets = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'test': train_testvalid['test']\n",
        "})\n",
        "\n",
        "# Further split the 'test' set into validation and test\n",
        "test_valid = tokenized_datasets['test'].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "# Update the DatasetDict\n",
        "tokenized_datasets = DatasetDict({\n",
        "    'train': tokenized_datasets['train'],\n",
        "    'validation': test_valid['test'], # Renamed to 'validation'\n",
        "    'test': test_valid['train'] # Renamed to 'test'\n",
        "})\n",
        "\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "testvalid_dataset = tokenized_datasets[\"test\"]  # Assuming you still need this\n",
        "eval_dataset = tokenized_datasets[\"validation\"]\n",
        "test_dataset = tokenized_datasets[\"test\"]\n",
        "\n",
        "small_train_dataset = train_dataset.shuffle(seed=42).select(range(1600))\n",
        "small_eval_dataset = eval_dataset.shuffle(seed=42).select(range(200))\n",
        "\n",
        "# 2. Data Loading\n",
        "train_dataloader = DataLoader(small_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_dataloader = DataLoader(small_eval_dataset, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "nXpTHEPFFSKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "-in-TW_gGSRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMPUoyUWPzkN"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch geopy -q\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Configuration\n",
        "tokenizer_name = \"gpt2\"\n",
        "batch_size = 4\n",
        "sequence_length = 128  # Adjusted based on tokenize_function\n",
        "embedding_dimension = 256\n",
        "num_heads = 8\n",
        "feed_forward_dimension = 1024\n",
        "num_encoder_layers = 6\n",
        "dropout_probability = 0.3\n",
        "learning_rate = 2e-5\n",
        "num_epochs = 1\n",
        "\n",
        "# Load Tokenizer and Add Pad Token (if needed)\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
        "\n",
        "# Ensure pad_token_id is within the vocabulary size\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "if pad_token_id >= tokenizer.vocab_size:\n",
        "    pad_token_id = tokenizer.vocab_size - 1  # Clip to maximum vocabulary index\n",
        "\n",
        "\n",
        "# 1. Tokenize and Format\n",
        "def tokenize_function(examples):\n",
        "    # Directly use waypoint numbers as labels\n",
        "    examples[\"labels\"] = examples[\"label\"]\n",
        "    tokenized_output = tokenizer(\n",
        "        examples[\"input\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=sequence_length\n",
        "    )\n",
        "\n",
        "    # Instead of assigning labels directly, shift them for causal LM\n",
        "    tokenized_output['labels'] = tokenized_output['input_ids'].copy()\n",
        "\n",
        "    # Replace input_ids corresponding to pad_token with -100 in labels\n",
        "    tokenized_output['labels'] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in label_list]\n",
        "        for label_list in tokenized_output['labels']\n",
        "    ]\n",
        "\n",
        "    # --- Ensure input_ids are within vocabulary range before padding ---\n",
        "    tokenized_output['input_ids'] = [\n",
        "        [token if token < tokenizer.vocab_size else tokenizer.unk_token_id for token in label_list]\n",
        "        for label_list in tokenized_output['input_ids']\n",
        "    ]\n",
        "\n",
        "    # --- Additional Checks ---\n",
        "    # Ensure that labels are within the vocabulary range as well\n",
        "    tokenized_output['labels'] = [\n",
        "        [token if token < tokenizer.vocab_size else tokenizer.unk_token_id for token in label_list]\n",
        "        for label_list in tokenized_output['labels']\n",
        "    ]\n",
        "\n",
        "    # Handle case where labels are empty (could happen due to errors in data)\n",
        "    for i, label_list in enumerate(tokenized_output['labels']):\n",
        "        if len(label_list) == 0:\n",
        "            # Replace with a single UNK token or handle as appropriate\n",
        "            tokenized_output['labels'][i] = [tokenizer.unk_token_id]\n",
        "\n",
        "    return tokenized_output\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "\n",
        "# Tokenize, format, and split dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"input\", \"label\"])\n",
        "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# --- Split the 'train' dataset within the DatasetDict ---\n",
        "train_testvalid = tokenized_datasets['train'].train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Create new DatasetDict with the splits\n",
        "tokenized_datasets = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'test': train_testvalid['test']\n",
        "})\n",
        "\n",
        "# Further split the 'test' set into validation and test\n",
        "test_valid = tokenized_datasets['test'].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "# Update the DatasetDict\n",
        "tokenized_datasets = DatasetDict({\n",
        "    'train': tokenized_datasets['train'],\n",
        "    'validation': test_valid['test'], # Renamed to 'validation'\n",
        "    'test': test_valid['train'] # Renamed to 'test'\n",
        "})\n",
        "\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "testvalid_dataset = tokenized_datasets[\"test\"]  # Assuming you still need this\n",
        "eval_dataset = tokenized_datasets[\"validation\"]\n",
        "test_dataset = tokenized_datasets[\"test\"]\n",
        "\n",
        "small_train_dataset = train_dataset.shuffle(seed=42).select(range(1600))\n",
        "small_eval_dataset = eval_dataset.shuffle(seed=42).select(range(200))\n",
        "\n",
        "# 2. Data Loading\n",
        "train_dataloader = DataLoader(small_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_dataloader = DataLoader(small_eval_dataset, batch_size=batch_size)\n",
        "\n",
        "# 3. Model\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Embedding(block_size, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.linear_out = nn.Linear(embed_dim, vocab_size)  # Output layer for token prediction\n",
        "        self.block_size = block_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # --- Initialize weights to handle potential out-of-vocabulary tokens ---\n",
        "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data) * 0.02  # Example initialization\n",
        "\n",
        "\n",
        "        # *** Define waypoint_coords_layer (Example) ***\n",
        "        # Replace with your desired layer for converting output to waypoint coordinates\n",
        "        self.waypoint_coords_layer = nn.Linear(vocab_size, 2)  # Example: Output 2 values (lat, lon) # Change is here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Ensure input_ids are within vocabulary range\n",
        "        input_ids = input_ids.clamp(0, tokenizer.vocab_size - 1)\n",
        "\n",
        "        # Create positional embeddings - Adjusted to use batch size\n",
        "        batch_size = input_ids.shape[0]\n",
        "        positions = torch.arange(0, input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
        "        positions = positions.unsqueeze(0).repeat(batch_size, 1) # Repeat for each item in batch\n",
        "        pos_embeddings = self.pos_embedding(positions)\n",
        "\n",
        "        # Combine embeddings\n",
        "        embeddings = self.dropout(self.embedding(input_ids) + pos_embeddings)\n",
        "\n",
        "        # Create src_key_padding_mask with correct shape\n",
        "        src_key_padding_mask = (input_ids == tokenizer.pad_token_id).transpose(0, 1)  # Transpose here\n",
        "        src_key_padding_mask = src_key_padding_mask.to(input_ids.device)\n",
        "\n",
        "        #6. Pass to encoder\n",
        "\n",
        "        encoder_output  = self.encoder(embeddings, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # Output layer\n",
        "        #output = self.linear_out(encoder_output)\n",
        "        #return output\n",
        "\n",
        "        # Output layer\n",
        "        output = self.linear_out(encoder_output)\n",
        "        # --- Apply waypoint_coords_layer to the output ---\n",
        "        # Reshape output to (batch_size * sequence_length, vocab_size) # Change is here\n",
        "        output = output.view(-1, output.size(-1))\n",
        "        waypoint_coords = self.waypoint_coords_layer(output)\n",
        "\n",
        "        return waypoint_coords  # Return latitude and longitude\n",
        "\n",
        "\n",
        "model = SimpleTransformer(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embed_dim=embedding_dimension,\n",
        "    num_heads=num_heads,\n",
        "    ff_hidden_dim=feed_forward_dimension,\n",
        "    num_layers=num_encoder_layers,\n",
        "    block_size=sequence_length,\n",
        "    dropout=dropout_probability\n",
        ")\n",
        "\n",
        "# 4. Loss and Optimizer\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training and Evaluation Loop\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # --- Training ---\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1} - Training\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        waypoint_counts = batch['labels'].to(device)  # Use 'labels' for waypoint count\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "\n",
        "        # Calculate loss with custom loss function\n",
        "        # Pass the tokenizer to custom_loss_function\n",
        "        loss = custom_loss_function(outputs, labels, waypoint_counts, tokenizer)\n",
        "\n",
        "\n",
        "        # Check for NaN loss\n",
        "        if torch.isnan(loss):\n",
        "            print(\"Loss is NaN. Skipping this batch.\")\n",
        "            continue\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} - Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_dataloader, desc=f\"Epoch {epoch + 1} - Evaluation\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            waypoint_counts = batch['labels'].to(device)  # Use 'labels' for waypoint count\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "\n",
        "            # Calculate loss with custom loss function\n",
        "            # *** Pass the tokenizer to custom_loss_function ***\n",
        "            loss = custom_loss_function(outputs, labels, waypoint_counts, tokenizer)\n",
        "\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} - Average Evaluation Loss: {avg_eval_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGQToWe1cj5F"
      },
      "source": [
        "## Model Deployment To HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gBk-L1bzknrr"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade huggingface_hub -q\n",
        "from huggingface_hub import notebook_login, create_repo, upload_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8cVI94vQHFle"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login, create_repo, upload_file\n",
        "\n",
        "repo_name = \"frankmorales2020/FlightPlan_Transformer_LLM\"\n",
        "\n",
        "# --- Automatically generate README.md ---\n",
        "readme_content = f\"\"\"\n",
        "# Flight Plan Transformer\n",
        "\n",
        "This repository contains a Transformer model trained to generate flight plan waypoints based on textual input. The model was trained using the \"frankmorales2020/flight_plan_waypoints\" dataset from Hugging Face Datasets.\n",
        "\n",
        "## Model Description\n",
        "\n",
        "The model is a decoder-only Transformer architecture implemented using PyTorch and the `transformers` library. It takes textual input describing the flight (e.g., \"Calculate the waypoints from SIN to CUN. Departure: 2024-06-19, Aircraft: Airbus A320, Weather: Partly Cloudy\") and predicts a sequence of waypoints representing the flight path.\n",
        "\n",
        "## Intended Use\n",
        "\n",
        "This model can be used to:\n",
        "\n",
        "- **Generate flight plans:** Given flight details, the model can predict waypoints for the route.\n",
        "- **Flight planning assistance:** It can serve as a tool to aid in flight planning, providing suggestions for waypoints.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "- **Accuracy:** The model's accuracy is limited by the quality and coverage of the training data. It may not be perfectly accurate in all scenarios.\n",
        "- **Real-world applicability:** This model is a research prototype and should not be used for actual flight navigation without thorough validation and safety checks.\n",
        "\n",
        "## How to Use\n",
        "\n",
        "1. **Install the necessary libraries:**\n",
        "Use code with caution\n",
        "bash pip install transformers datasets torch\n",
        "\n",
        "2. **Load the model and tokenizer:**\n",
        "Use code with caution\n",
        "python from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{repo_name}\") model = AutoModelForSeq2SeqLM.from_pretrained(\"{repo_name}\")\n",
        "\n",
        "3. **Generate waypoints:**\n",
        "Use code with caution\n",
        "python def generate_flight_plan(model, tokenizer, start_text, max_new_tokens=200):\n",
        "\n",
        "# ... (Your generation code from the original script) ...\n",
        "4. **Example usage:**\n",
        "Use code with caution\n",
        "python query = \"Calculate the waypoints from SIN to CUN. Departure: 2024-06-19, Aircraft: Airbus A320, Weather: Partly Cloudy\" generated_plan = generate_flight_plan(model, tokenizer, start_text=query, max_new_tokens=256) print(generated_plan)\n",
        "\n",
        "## Training Data\n",
        "\n",
        "The model was trained on the \"frankmorales2020/flight_plan_waypoints\" dataset from Hugging Face Datasets. This dataset contains flight plans with corresponding waypoints.\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "The model's performance was evaluated on a held-out portion of the training data. (Include specific metrics or evaluation results if available)\n",
        "\n",
        "## Acknowledgements\n",
        "\n",
        "- Thanks to Hugging Face for providing the `transformers` and `datasets` libraries.\n",
        "- Thanks to frankmorales2020 for creating and sharing the flight plan dataset.\n",
        "\n",
        "## Contact\n",
        "Frank Morales, BEng, MEng, SMIEEE\n",
        "Boeing Associate Technical Fellow\n",
        "Linkedin: https://www.linkedin.com/in/frank-morales1964/\n",
        "\"\"\"\n",
        "\n",
        "# Write README.md to file\n",
        "with open(\"README.md\", \"w\") as f:\n",
        "    f.write(readme_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ljyhySbFwpas"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "token=access_token_write"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "A0WynixX3e_i"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "api.get_token_permission(token=access_token_write)\n",
        "repo_id = 'frankmorales2020/FlightPlan_Transformer_LLM'\n",
        "api.delete_repo(repo_id=repo_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McMnf9stGCch"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, HfFolder\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "# --- Push to Hugging Face Hub ---\n",
        "\n",
        "repo_name = \"frankmorales2020/FlightPlan_Transformer_LLM\"\n",
        "create_repo(repo_name, private=False, exist_ok=True)\n",
        "\n",
        "# --- Push the model directly from memory ---\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"README.md\",\n",
        "    path_in_repo=\"README.md\",\n",
        "    repo_id=repo_name,\n",
        "    repo_type=\"model\",\n",
        ")\n",
        "\n",
        "# Save the model and tokenizer to a temporary directory\n",
        "model_save_dir = \"flight_plan_transformer_model\"\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "# 1. Create a configuration dictionary for your model\n",
        "config = {\n",
        "    \"vocab_size\": tokenizer.vocab_size,\n",
        "    \"embed_dim\": embedding_dimension,  # Use your embedding dimension variable\n",
        "    \"num_heads\": num_heads,  # Use your num_heads variable\n",
        "    \"ff_hidden_dim\": feed_forward_dimension,  # Use your feed_forward_dimension variable\n",
        "    \"num_layers\": num_encoder_layers,  # Use your num_encoder_layers variable\n",
        "    #\"block_size\": block_size,  # Use your block_size variable\n",
        "    \"block_size\": sequence_length,  # Use sequence_length as block_size\n",
        "    \"dropout\": dropout_probability  # Use your dropout_probability variable\n",
        "}\n",
        "\n",
        "# 2. Save the configuration to config.json\n",
        "with open(os.path.join(model_save_dir, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "# Save the model's state_dict instead of using save_pretrained\n",
        "torch.save(model.state_dict(), os.path.join(model_save_dir, 'pytorch_model.bin'))\n",
        "tokenizer.save_pretrained(model_save_dir) # This is fine as tokenizer is from Hugging Face\n",
        "\n",
        "\n",
        "# Upload the model files, including config.json and pytorch_model.bin\n",
        "upload_file(\n",
        "    path_or_fileobj=os.path.join(model_save_dir, \"config.json\"),\n",
        "    path_in_repo=\"config.json\",\n",
        "    repo_id=repo_name,\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj=os.path.join(model_save_dir, \"pytorch_model.bin\"),\n",
        "    path_in_repo=\"pytorch_model.bin\",\n",
        "    repo_id=repo_name,\n",
        ")\n",
        "\n",
        "# Upload other files (e.g., tokenizer files)\n",
        "for filename in os.listdir(model_save_dir):\n",
        "    if filename not in [\"config.json\", \"pytorch_model.bin\"]:  # Exclude already uploaded files\n",
        "        upload_file(\n",
        "            path_or_fileobj=os.path.join(model_save_dir, filename),\n",
        "            path_in_repo=filename,\n",
        "            repo_id=repo_name,\n",
        "        )\n",
        "\n",
        "\n",
        "# Remove the temporary directory (optional)\n",
        "shutil.rmtree(model_save_dir)\n",
        "\n",
        "print(f\"Model pushed to Hugging Face Hub: {repo_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXqlKKDxcZYy"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "r8BfWTPoz5PN"
      },
      "outputs": [],
      "source": [
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tuV4e29typ0J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Embedding(block_size, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.linear_out = nn.Linear(embed_dim, vocab_size)  # Output layer for token prediction\n",
        "        self.block_size = block_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # --- Initialize weights to handle potential out-of-vocabulary tokens ---\n",
        "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data) * 0.02  # Example initialization\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Ensure input_ids are within vocabulary range\n",
        "        input_ids = input_ids.clamp(0, tokenizer.vocab_size - 1)\n",
        "\n",
        "        # Create positional embeddings - Adjusted to use batch size\n",
        "        batch_size = input_ids.shape[0]\n",
        "        positions = torch.arange(0, input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
        "        positions = positions.unsqueeze(0).repeat(batch_size, 1) # Repeat for each item in batch\n",
        "        pos_embeddings = self.pos_embedding(positions)\n",
        "\n",
        "        # Combine embeddings\n",
        "        embeddings = self.dropout(self.embedding(input_ids) + pos_embeddings)\n",
        "\n",
        "        # Create src_key_padding_mask with correct shape\n",
        "        src_key_padding_mask = (input_ids == tokenizer.pad_token_id).transpose(0, 1)  # Transpose here\n",
        "        src_key_padding_mask = src_key_padding_mask.to(input_ids.device)\n",
        "\n",
        "        #6. Pass to encoder\n",
        "\n",
        "        encoder_output  = self.encoder(embeddings, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.linear_out(encoder_output)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGw7baHNYylW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
        "import torch\n",
        "import json\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# --- Updated Model Loading ---\n",
        "pretrained_model_name = \"frankmorales2020/FlightPlan_Transformer_LLM\"\n",
        "\n",
        "# 1. Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
        "#model=AutoModelForCausalLM.from_pretrained(pretrained_model_name)\n",
        "\n",
        "#\n",
        "# 2. Load configuration using hf_hub_download\n",
        "config_path = hf_hub_download(repo_id=pretrained_model_name, filename=\"config.json\")\n",
        "with open(config_path, 'r') as f:\n",
        "    config_dict = json.load(f)\n",
        "\n",
        "# *** Add max_waypoints to the config dictionary or load it from elsewhere ***\n",
        "config_dict['max_waypoints'] = 10  # Replace with your actual max_waypoints value\n",
        "\n",
        "# 3. Instantiate your SimpleTransformer class using the loaded configuration\n",
        "model = SimpleTransformer(\n",
        "    vocab_size=config_dict['vocab_size'],\n",
        "    embed_dim=config_dict['embed_dim'],\n",
        "    num_heads=config_dict['num_heads'],\n",
        "    ff_hidden_dim=config_dict['ff_hidden_dim'],\n",
        "    num_layers=config_dict['num_layers'],\n",
        "    block_size=config_dict['block_size'],\n",
        "    dropout=config_dict['dropout'],\n",
        ")\n",
        "\n",
        "# 4. Load the model weights using hf_hub_download\n",
        "model_path = hf_hub_download(repo_id=pretrained_model_name, filename=\"pytorch_model.bin\")\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxhoIAxeY4Ky"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksojr-ek6lLz"
      },
      "outputs": [],
      "source": [
        "# --- Load and Preprocess Flight Plan Data ---\n",
        "from datasets import load_dataset, DatasetDict\n",
        "flight_plan_dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "\n",
        "# Split into train and validation sets\n",
        "flight_plan_dataset = flight_plan_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
        "flight_plan_dataset = DatasetDict({\n",
        "    'train': flight_plan_dataset['train'],\n",
        "    'validation': flight_plan_dataset['test']\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfbuUlKHxl6a"
      },
      "outputs": [],
      "source": [
        "flight_plan_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-J1dazs92UJv"
      },
      "outputs": [],
      "source": [
        "flight_data=flight_plan_dataset['validation'][100]\n",
        "print(flight_data['input'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bU3ovgeD-NTW"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.distance import geodesic\n",
        "from tqdm import tqdm\n",
        "from geopy.point import Point  # Import Point object\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Define airport codes and names\n",
        "airports = [\n",
        "    # North America\n",
        "    \"ATL\", \"LAX\", \"ORD\", \"DFW\", \"DEN\", \"JFK\", \"SFO\", \"LAS\", \"SEA\", \"CLT\",\n",
        "    \"MIA\", \"PHL\", \"EWR\", \"BOS\", \"PHX\", \"MCO\", \"IAH\", \"DAL\", \"FLL\", \"DTW\",\n",
        "    \"MSP\", \"IAD\", \"BWI\", \"SLC\", \"SAN\", \"MDW\", \"TPA\", \"PDX\", \"BNA\", \"STL\",\n",
        "    # Europe\n",
        "    \"LHR\", \"CDG\", \"AMS\", \"FRA\", \"MAD\", \"BCN\", \"FCO\", \"MUC\", \"DUB\", \"ZRH\",\n",
        "    # Asia\n",
        "    \"HND\", \"PVG\", \"ICN\", \"SIN\", \"KUL\", \"BKK\", \"DXB\", \"DEL\", \"HKG\", \"NRT\",\n",
        "    # Caribbean\n",
        "    \"CUN\", \"PUJ\", \"SJU\", \"MBJ\", \"NAS\",\n",
        "    # South America\n",
        "    \"GRU\", \"BOG\", \"EZE\", \"SCL\", \"LIM\",\n",
        "    # Australia & New Zealand\n",
        "    \"SYD\", \"MEL\", \"BNE\", \"PER\", \"AKL\"\n",
        "]\n",
        "\n",
        "airport_names = {\n",
        "    \"ATL\": \"Hartsfield-Jackson Atlanta International Airport\",\n",
        "    \"LAX\": \"Los Angeles International Airport\",\n",
        "    \"ORD\": \"Chicago O'Hare International Airport\",\n",
        "    \"DFW\": \"Dallas/Fort Worth International Airport\",\n",
        "    \"DEN\": \"Denver International Airport\",\n",
        "    \"JFK\": \"John F. Kennedy International Airport\",\n",
        "    \"SFO\": \"San Francisco International Airport\",\n",
        "    \"LAS\": \"Harry Reid International Airport\",\n",
        "    \"SEA\": \"Seattle-Tacoma International Airport\",\n",
        "    \"CLT\": \"Charlotte Douglas International Airport\",\n",
        "    \"MIA\": \"Miami International Airport\",\n",
        "    \"PHL\": \"Philadelphia International Airport\",\n",
        "    \"EWR\": \"Newark Liberty International Airport\",\n",
        "    \"BOS\": \"Logan International Airport\",\n",
        "    \"PHX\": \"Phoenix Sky Harbor International Airport\",\n",
        "    \"MCO\": \"Orlando International Airport\",\n",
        "    \"IAH\": \"George Bush Intercontinental Airport\",\n",
        "    \"DAL\": \"Dallas Love Field\",\n",
        "    \"FLL\": \"Fort Lauderdale-Hollywood International Airport\",\n",
        "    \"DTW\": \"Detroit Metropolitan Airport\",\n",
        "    \"MSP\": \"Minneapolis-Saint Paul International Airport\",\n",
        "    \"IAD\": \"Washington Dulles International Airport\",\n",
        "    \"BWI\": \"Baltimore/Washington International Airport\",\n",
        "    \"SLC\": \"Salt Lake City International Airport\",\n",
        "    \"SAN\": \"San Diego International Airport\",\n",
        "    \"MDW\": \"Midway International Airport\",\n",
        "    \"TPA\": \"Tampa International Airport\",\n",
        "    \"PDX\": \"Portland International Airport\",\n",
        "    \"BNA\": \"Nashville International Airport\",\n",
        "    \"STL\": \"St. Louis Lambert International Airport\",\n",
        "    # Europe\n",
        "    \"LHR\": \"London Heathrow Airport\",\n",
        "    \"CDG\": \"Charles de Gaulle Airport\",\n",
        "    \"AMS\": \"Amsterdam Airport Schiphol\",\n",
        "    \"FRA\": \"Frankfurt Airport\",\n",
        "    \"MAD\": \"Adolfo Suárez Madrid–Barajas Airport\",\n",
        "    \"BCN\": \"Barcelona–El Prat Airport\",\n",
        "    \"FCO\": \"Leonardo da Vinci–Fiumicino Airport\",\n",
        "    \"MUC\": \"Munich Airport\",\n",
        "    \"DUB\": \"Dublin Airport\",\n",
        "    \"ZRH\": \"Zurich Airport\",\n",
        "    # Asia\n",
        "    \"HND\": \"Haneda Airport\",\n",
        "    \"PVG\": \"Shanghai Pudong International Airport\",\n",
        "    \"ICN\": \"Incheon International Airport\",\n",
        "    \"SIN\": \"Singapore Changi Airport\",\n",
        "    \"KUL\": \"Kuala Lumpur International Airport\",\n",
        "    \"BKK\": \"Suvarnabhumi Airport\",\n",
        "    \"DXB\": \"Dubai International Airport\",\n",
        "    \"DEL\": \"Indira Gandhi International Airport\",\n",
        "    \"HKG\": \"Hong Kong International Airport\",\n",
        "    \"NRT\": \"Narita International Airport\",\n",
        "    # Caribbean\n",
        "    \"CUN\": \"Cancún International Airport\",\n",
        "    \"PUJ\": \"Punta Cana International Airport\",\n",
        "    \"SJU\": \"Luis Muñoz Marín International Airport\",\n",
        "    \"MBJ\": \"Sangster International Airport\",\n",
        "    \"NAS\": \"Lynden Pindling International Airport\",\n",
        "    # South America\n",
        "    \"GRU\": \"São Paulo–Guarulhos International Airport\",\n",
        "    \"BOG\": \"El Dorado International Airport\",\n",
        "    \"EZE\": \"Ministro Pistarini International Airport\",\n",
        "    \"SCL\": \"Arturo Merino Benítez International Airport\",\n",
        "    \"LIM\": \"Jorge Chávez International Airport\",\n",
        "    # Australia & New Zealand\n",
        "    \"SYD\": \"Sydney Airport\",\n",
        "    \"MEL\": \"Melbourne Airport\",\n",
        "    \"BNE\": \"Brisbane Airport\",\n",
        "    \"PER\": \"Perth Airport\",\n",
        "    \"AKL\": \"Auckland Airport\"\n",
        "}"
      ]
    },
    {
      "source": [
        "# Cell 62: generate_flight_plan\n",
        "\n",
        "def generate_flight_plan(model, tokenizer, query, number_of_waypoints, device='cuda'):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Start with the initial query tokens\n",
        "    start_tokens = tokenizer(query, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "\n",
        "    # *** Print start tokens for debugging ***\n",
        "    print(f\"Start Tokens: {start_tokens}\")\n",
        "    print(f\"Length of start_tokens: {len(start_tokens)}\")\n",
        "    print(f\"Length of start_tokens['input_ids']: {len(start_tokens['input_ids'])}\")\n",
        "    print(number_of_waypoints)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_waypoints = []\n",
        "        while len(generated_waypoints) < number_of_waypoints:  # Stopping condition\n",
        "\n",
        "            # Access 'input_ids' and 'attention_mask' directly from start_tokens dictionary\n",
        "            generated_waypoints = model(start_tokens['input_ids'], start_tokens['attention_mask'])\n",
        "            print(f\"Generated waypoints: {generated_waypoints}\")\n",
        "            #print(f\"Length of generated_waypoints: {len(generated_waypoints)}\")\n",
        "            #print(f\"Length of generated_waypoints[0]: {len(generated_waypoints[0])}\")\n",
        "            #print(f\"Length of generated_waypoints[0][0]: {len(generated_waypoints[0][0])}\")\n",
        "\n",
        "    # Post-processing: Limit the final list (Optional)\n",
        "    #generated_waypoints = generated_waypoints[:number_of_waypoints]\n",
        "\n",
        "    return generated_waypoints"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Q73TW5aIS7wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2hWplWeAibh"
      },
      "outputs": [],
      "source": [
        "def generate_flight_plan(model, tokenizer, query, number_of_waypoints, device='cuda'):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Start with the initial query tokens\n",
        "    # start_tokens should be a dictionary containing both 'input_ids' and 'attention_mask'\n",
        "    start_tokens = tokenizer(query, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    print(f\"Start tokens: {start_tokens}\")\n",
        "    print(f\"Length of start_tokens: {len(start_tokens)}\")\n",
        "    print(f\"Length of start_tokens['input_ids']: {len(start_tokens['input_ids'])}\")\n",
        "    print(number_of_waypoints)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_waypoints = []\n",
        "        while len(generated_waypoints) < number_of_waypoints:  # Stopping condition\n",
        "\n",
        "            # Access 'input_ids' and 'attention_mask' directly from start_tokens dictionary\n",
        "            generated_waypoints = model(start_tokens['input_ids'], start_tokens['attention_mask'])\n",
        "            #print(f\"Generated waypoints: {generated_waypoints}\")\n",
        "            #print(f\"Length of generated_waypoints: {len(generated_waypoints)}\")\n",
        "            #print(f\"Length of generated_waypoints[0]: {len(generated_waypoints[0])}\")\n",
        "            #print(f\"Length of generated_waypoints[0][0]: {len(generated_waypoints[0][0])}\")\n",
        "\n",
        "    # Post-processing: Limit the final list (Optional)\n",
        "    generated_waypoints = generated_waypoints[:number_of_waypoints]\n",
        "\n",
        "    return generated_waypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJAJ87kD4vPu"
      },
      "outputs": [],
      "source": [
        "flight_plan_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZFEO-n830eR"
      },
      "outputs": [],
      "source": [
        "for i in range(1):\n",
        "    flight_data=flight_plan_dataset['validation'][i]\n",
        "    query=flight_data['input']\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Waypints list: {flight_data['waypoints']}\")\n",
        "    number_of_waypoints=len(flight_data['waypoints'])\n",
        "    print(f\"Number of wapoints: {number_of_waypoints}\")\n",
        "    print('\\n')\n",
        "    generated_plan = generate_flight_plan(model, tokenizer, query, number_of_waypoints, device=device)\n",
        "    print(f\"\\nGenerated waypoint list: {generated_plan}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Rb8HQCGUhEgE",
        "NGQToWe1cj5F"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNTzWC0onqv4MHJEo7I39Bu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}