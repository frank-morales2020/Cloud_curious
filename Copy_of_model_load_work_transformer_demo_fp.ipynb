{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/Copy_of_model_load_work_transformer_demo_fp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model environmenmt"
      ],
      "metadata": {
        "id": "Rb8HQCGUhEgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q"
      ],
      "metadata": {
        "id": "GM7ASLUOgp4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write, # ADD YOUR TOKEN HERE\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "30HOQNnVg1D6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpoVJAMMt74m"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch geopy -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7cb3yWFuyoy"
      },
      "outputs": [],
      "source": [
        "!pip show transformers\n",
        "print()\n",
        "!pip show datasets\n",
        "print()\n",
        "!pip show torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsnHRSWBL5Va"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.distance import geodesic\n",
        "from tqdm import tqdm\n",
        "from geopy.point import Point  # Import Point object\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Define airport codes and names\n",
        "airports = [\n",
        "    # North America\n",
        "    \"ATL\", \"LAX\", \"ORD\", \"DFW\", \"DEN\", \"JFK\", \"SFO\", \"LAS\", \"SEA\", \"CLT\",\n",
        "    \"MIA\", \"PHL\", \"EWR\", \"BOS\", \"PHX\", \"MCO\", \"IAH\", \"DAL\", \"FLL\", \"DTW\",\n",
        "    \"MSP\", \"IAD\", \"BWI\", \"SLC\", \"SAN\", \"MDW\", \"TPA\", \"PDX\", \"BNA\", \"STL\",\n",
        "    # Europe\n",
        "    \"LHR\", \"CDG\", \"AMS\", \"FRA\", \"MAD\", \"BCN\", \"FCO\", \"MUC\", \"DUB\", \"ZRH\",\n",
        "    # Asia\n",
        "    \"HND\", \"PVG\", \"ICN\", \"SIN\", \"KUL\", \"BKK\", \"DXB\", \"DEL\", \"HKG\", \"NRT\",\n",
        "    # Caribbean\n",
        "    \"CUN\", \"PUJ\", \"SJU\", \"MBJ\", \"NAS\",\n",
        "    # South America\n",
        "    \"GRU\", \"BOG\", \"EZE\", \"SCL\", \"LIM\",\n",
        "    # Australia & New Zealand\n",
        "    \"SYD\", \"MEL\", \"BNE\", \"PER\", \"AKL\"\n",
        "]\n",
        "\n",
        "airport_names = {\n",
        "    \"ATL\": \"Hartsfield-Jackson Atlanta International Airport\",\n",
        "    \"LAX\": \"Los Angeles International Airport\",\n",
        "    \"ORD\": \"Chicago O'Hare International Airport\",\n",
        "    \"DFW\": \"Dallas/Fort Worth International Airport\",\n",
        "    \"DEN\": \"Denver International Airport\",\n",
        "    \"JFK\": \"John F. Kennedy International Airport\",\n",
        "    \"SFO\": \"San Francisco International Airport\",\n",
        "    \"LAS\": \"Harry Reid International Airport\",\n",
        "    \"SEA\": \"Seattle-Tacoma International Airport\",\n",
        "    \"CLT\": \"Charlotte Douglas International Airport\",\n",
        "    \"MIA\": \"Miami International Airport\",\n",
        "    \"PHL\": \"Philadelphia International Airport\",\n",
        "    \"EWR\": \"Newark Liberty International Airport\",\n",
        "    \"BOS\": \"Logan International Airport\",\n",
        "    \"PHX\": \"Phoenix Sky Harbor International Airport\",\n",
        "    \"MCO\": \"Orlando International Airport\",\n",
        "    \"IAH\": \"George Bush Intercontinental Airport\",\n",
        "    \"DAL\": \"Dallas Love Field\",\n",
        "    \"FLL\": \"Fort Lauderdale-Hollywood International Airport\",\n",
        "    \"DTW\": \"Detroit Metropolitan Airport\",\n",
        "    \"MSP\": \"Minneapolis-Saint Paul International Airport\",\n",
        "    \"IAD\": \"Washington Dulles International Airport\",\n",
        "    \"BWI\": \"Baltimore/Washington International Airport\",\n",
        "    \"SLC\": \"Salt Lake City International Airport\",\n",
        "    \"SAN\": \"San Diego International Airport\",\n",
        "    \"MDW\": \"Midway International Airport\",\n",
        "    \"TPA\": \"Tampa International Airport\",\n",
        "    \"PDX\": \"Portland International Airport\",\n",
        "    \"BNA\": \"Nashville International Airport\",\n",
        "    \"STL\": \"St. Louis Lambert International Airport\",\n",
        "    # Europe\n",
        "    \"LHR\": \"London Heathrow Airport\",\n",
        "    \"CDG\": \"Charles de Gaulle Airport\",\n",
        "    \"AMS\": \"Amsterdam Airport Schiphol\",\n",
        "    \"FRA\": \"Frankfurt Airport\",\n",
        "    \"MAD\": \"Adolfo Suárez Madrid–Barajas Airport\",\n",
        "    \"BCN\": \"Barcelona–El Prat Airport\",\n",
        "    \"FCO\": \"Leonardo da Vinci–Fiumicino Airport\",\n",
        "    \"MUC\": \"Munich Airport\",\n",
        "    \"DUB\": \"Dublin Airport\",\n",
        "    \"ZRH\": \"Zurich Airport\",\n",
        "    # Asia\n",
        "    \"HND\": \"Haneda Airport\",\n",
        "    \"PVG\": \"Shanghai Pudong International Airport\",\n",
        "    \"ICN\": \"Incheon International Airport\",\n",
        "    \"SIN\": \"Singapore Changi Airport\",\n",
        "    \"KUL\": \"Kuala Lumpur International Airport\",\n",
        "    \"BKK\": \"Suvarnabhumi Airport\",\n",
        "    \"DXB\": \"Dubai International Airport\",\n",
        "    \"DEL\": \"Indira Gandhi International Airport\",\n",
        "    \"HKG\": \"Hong Kong International Airport\",\n",
        "    \"NRT\": \"Narita International Airport\",\n",
        "    # Caribbean\n",
        "    \"CUN\": \"Cancún International Airport\",\n",
        "    \"PUJ\": \"Punta Cana International Airport\",\n",
        "    \"SJU\": \"Luis Muñoz Marín International Airport\",\n",
        "    \"MBJ\": \"Sangster International Airport\",\n",
        "    \"NAS\": \"Lynden Pindling International Airport\",\n",
        "    # South America\n",
        "    \"GRU\": \"São Paulo–Guarulhos International Airport\",\n",
        "    \"BOG\": \"El Dorado International Airport\",\n",
        "    \"EZE\": \"Ministro Pistarini International Airport\",\n",
        "    \"SCL\": \"Arturo Merino Benítez International Airport\",\n",
        "    \"LIM\": \"Jorge Chávez International Airport\",\n",
        "    # Australia & New Zealand\n",
        "    \"SYD\": \"Sydney Airport\",\n",
        "    \"MEL\": \"Melbourne Airport\",\n",
        "    \"BNE\": \"Brisbane Airport\",\n",
        "    \"PER\": \"Perth Airport\",\n",
        "    \"AKL\": \"Auckland Airport\"\n",
        "}"
      ],
      "metadata": {
        "id": "5IbgqgPUzWws"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bac8KFM-kSIJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer_name = 'gpt2'\n",
        "#tokenizer_name = 'meta-llama/Meta-Llama-3-8B'\n",
        "#tokenizer_name = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "pad_token = '<pad>'\n",
        "\n",
        "# --- Load Tokenizer and Add Pad Token ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': pad_token})\n",
        "# Ensure pad_token_id is within the vocabulary size\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "if pad_token_id >= tokenizer.vocab_size:\n",
        "    pad_token_id = tokenizer.vocab_size - 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "flight_plan_dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "flight_plan_dataset['train'][7]"
      ],
      "metadata": {
        "id": "s7CcHjhRYZjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install transformers datasets torch geopy -q\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Configuration\n",
        "tokenizer_name = \"gpt2\"\n",
        "batch_size = 4\n",
        "sequence_length = 128  # Adjusted based on tokenize_function\n",
        "embedding_dimension = 256\n",
        "num_heads = 8\n",
        "feed_forward_dimension = 1024\n",
        "num_encoder_layers = 6\n",
        "dropout_probability = 0.3\n",
        "learning_rate = 2e-5\n",
        "num_epochs = 10\n",
        "\n",
        "# Load Tokenizer and Add Pad Token (if needed)\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
        "\n",
        "# Ensure pad_token_id is within the vocabulary size\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "if pad_token_id >= tokenizer.vocab_size:\n",
        "    pad_token_id = tokenizer.vocab_size - 1  # Clip to maximum vocabulary index\n",
        "\n",
        "\n",
        "# 1. Tokenize and Format\n",
        "def tokenize_function(examples):\n",
        "    # Directly use waypoint numbers as labels\n",
        "    examples[\"labels\"] = examples[\"label\"]\n",
        "    tokenized_output = tokenizer(\n",
        "        examples[\"input\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=sequence_length\n",
        "    )\n",
        "\n",
        "    # Instead of assigning labels directly, shift them for causal LM\n",
        "    tokenized_output['labels'] = tokenized_output['input_ids'].copy()\n",
        "\n",
        "    # Replace input_ids corresponding to pad_token with -100 in labels\n",
        "    tokenized_output['labels'] = [\n",
        "        [-100 if token == tokenizer.pad_token_id else token for token in label_list]\n",
        "        for label_list in tokenized_output['labels']\n",
        "    ]\n",
        "\n",
        "    # --- Ensure input_ids are within vocabulary range before padding ---\n",
        "    tokenized_output['input_ids'] = [\n",
        "        [token if token < tokenizer.vocab_size else tokenizer.unk_token_id for token in label_list]\n",
        "        for label_list in tokenized_output['input_ids']\n",
        "    ]\n",
        "\n",
        "    # --- Additional Checks ---\n",
        "    # Ensure that labels are within the vocabulary range as well\n",
        "    tokenized_output['labels'] = [\n",
        "        [token if token < tokenizer.vocab_size else tokenizer.unk_token_id for token in label_list]\n",
        "        for label_list in tokenized_output['labels']\n",
        "    ]\n",
        "\n",
        "    # Handle case where labels are empty (could happen due to errors in data)\n",
        "    for i, label_list in enumerate(tokenized_output['labels']):\n",
        "        if len(label_list) == 0:\n",
        "            # Replace with a single UNK token or handle as appropriate\n",
        "            tokenized_output['labels'][i] = [tokenizer.unk_token_id]\n",
        "\n",
        "    return tokenized_output\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "\n",
        "# Tokenize, format, and split dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"input\", \"label\"])\n",
        "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# --- Split the 'train' dataset within the DatasetDict ---\n",
        "train_testvalid = tokenized_datasets['train'].train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "# Create new DatasetDict with the splits\n",
        "tokenized_datasets = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'test': train_testvalid['test']\n",
        "})\n",
        "\n",
        "# Further split the 'test' set into validation and test\n",
        "test_valid = tokenized_datasets['test'].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "# Update the DatasetDict\n",
        "tokenized_datasets = DatasetDict({\n",
        "    'train': tokenized_datasets['train'],\n",
        "    'validation': test_valid['test'], # Renamed to 'validation'\n",
        "    'test': test_valid['train'] # Renamed to 'test'\n",
        "})\n",
        "\n",
        "\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "testvalid_dataset = tokenized_datasets[\"test\"]  # Assuming you still need this\n",
        "eval_dataset = tokenized_datasets[\"validation\"]\n",
        "test_dataset = tokenized_datasets[\"test\"]\n",
        "\n",
        "small_train_dataset = train_dataset.shuffle(seed=42).select(range(1600))\n",
        "small_eval_dataset = eval_dataset.shuffle(seed=42).select(range(200))\n",
        "\n",
        "# 2. Data Loading\n",
        "train_dataloader = DataLoader(small_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_dataloader = DataLoader(small_eval_dataset, batch_size=batch_size)\n",
        "\n",
        "# 3. Model\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Embedding(block_size, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.linear_out = nn.Linear(embed_dim, vocab_size)  # Output layer for token prediction\n",
        "        self.block_size = block_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # --- Initialize weights to handle potential out-of-vocabulary tokens ---\n",
        "        self.embedding.weight.data = torch.randn_like(self.embedding.weight.data) * 0.02  # Example initialization\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # 1. Ensure input_ids are within vocabulary range\n",
        "        input_ids = input_ids.clamp(0, tokenizer.vocab_size - 1)\n",
        "\n",
        "        # 2. Create positional embeddings - Adjusted to use batch size\n",
        "        batch_size = input_ids.shape[0]\n",
        "        positions = torch.arange(0, input_ids.size(1), dtype=torch.long, device=input_ids.device)\n",
        "        positions = positions.unsqueeze(0).repeat(batch_size, 1) # Repeat for each item in batch\n",
        "        pos_embeddings = self.pos_embedding(positions)\n",
        "\n",
        "        # 3. Combine embeddings\n",
        "        embeddings = self.dropout(self.embedding(input_ids) + pos_embeddings)\n",
        "\n",
        "        # 4. Create attention mask\n",
        "        # attention_mask = input_ids != tokenizer.pad_token_id  # Not needed\n",
        "\n",
        "        # 5. Ensure attention_mask is boolean and has correct shape\n",
        "        #attention_mask = attention_mask.type(torch.bool)\n",
        "\n",
        "        # Create src_key_padding_mask with correct shape\n",
        "        src_key_padding_mask = (input_ids == tokenizer.pad_token_id)\n",
        "\n",
        "        # 6. Pass to encoder\n",
        "        encoder_output = self.encoder(embeddings, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        # 7. Output layer\n",
        "        output = self.linear_out(encoder_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "model = SimpleTransformer(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embed_dim=embedding_dimension,\n",
        "    num_heads=num_heads,\n",
        "    ff_hidden_dim=feed_forward_dimension,\n",
        "    num_layers=num_encoder_layers,\n",
        "    block_size=sequence_length,\n",
        "    dropout=dropout_probability\n",
        ")\n",
        "\n",
        "# 4. Loss and Optimizer\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 5. Training and Evaluation Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = loss_fn(outputs.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} - Average Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "    # Evaluation Loop\n",
        "    model.eval()\n",
        "    total_eval_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in eval_dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
        "    print(f\"Epoch {epoch + 1} - Average Evaluation Loss: {avg_eval_loss:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "LMPUoyUWPzkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Deployment To HF"
      ],
      "metadata": {
        "id": "NGQToWe1cj5F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBk-L1bzknrr"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade huggingface_hub -q\n",
        "from huggingface_hub import notebook_login, create_repo, upload_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cVI94vQHFle"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login, create_repo, upload_file\n",
        "\n",
        "repo_name = \"frankmorales2020/FlightPlan_Transformer_LLM\"\n",
        "\n",
        "# --- Automatically generate README.md ---\n",
        "readme_content = f\"\"\"\n",
        "# Flight Plan Transformer\n",
        "\n",
        "This repository contains a Transformer model trained to generate flight plan waypoints based on textual input. The model was trained using the \"frankmorales2020/flight_plan_waypoints\" dataset from Hugging Face Datasets.\n",
        "\n",
        "## Model Description\n",
        "\n",
        "The model is a decoder-only Transformer architecture implemented using PyTorch and the `transformers` library. It takes textual input describing the flight (e.g., \"Calculate the waypoints from SIN to CUN. Departure: 2024-06-19, Aircraft: Airbus A320, Weather: Partly Cloudy\") and predicts a sequence of waypoints representing the flight path.\n",
        "\n",
        "## Intended Use\n",
        "\n",
        "This model can be used to:\n",
        "\n",
        "- **Generate flight plans:** Given flight details, the model can predict waypoints for the route.\n",
        "- **Flight planning assistance:** It can serve as a tool to aid in flight planning, providing suggestions for waypoints.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "- **Accuracy:** The model's accuracy is limited by the quality and coverage of the training data. It may not be perfectly accurate in all scenarios.\n",
        "- **Real-world applicability:** This model is a research prototype and should not be used for actual flight navigation without thorough validation and safety checks.\n",
        "\n",
        "## How to Use\n",
        "\n",
        "1. **Install the necessary libraries:**\n",
        "Use code with caution\n",
        "bash pip install transformers datasets torch\n",
        "\n",
        "2. **Load the model and tokenizer:**\n",
        "Use code with caution\n",
        "python from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{repo_name}\") model = AutoModelForSeq2SeqLM.from_pretrained(\"{repo_name}\")\n",
        "\n",
        "3. **Generate waypoints:**\n",
        "Use code with caution\n",
        "python def generate_flight_plan(model, tokenizer, start_text, max_new_tokens=200):\n",
        "\n",
        "# ... (Your generation code from the original script) ...\n",
        "4. **Example usage:**\n",
        "Use code with caution\n",
        "python query = \"Calculate the waypoints from SIN to CUN. Departure: 2024-06-19, Aircraft: Airbus A320, Weather: Partly Cloudy\" generated_plan = generate_flight_plan(model, tokenizer, start_text=query, max_new_tokens=256) print(generated_plan)\n",
        "\n",
        "## Training Data\n",
        "\n",
        "The model was trained on the \"frankmorales2020/flight_plan_waypoints\" dataset from Hugging Face Datasets. This dataset contains flight plans with corresponding waypoints.\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "The model's performance was evaluated on a held-out portion of the training data. (Include specific metrics or evaluation results if available)\n",
        "\n",
        "## Acknowledgements\n",
        "\n",
        "- Thanks to Hugging Face for providing the `transformers` and `datasets` libraries.\n",
        "- Thanks to frankmorales2020 for creating and sharing the flight plan dataset.\n",
        "\n",
        "## Contact\n",
        "Frank Morales, BEng, MEng, SMIEEE\n",
        "Boeing Associate Technical Fellow\n",
        "Linkedin: https://www.linkedin.com/in/frank-morales1964/\n",
        "\"\"\"\n",
        "\n",
        "# Write README.md to file\n",
        "with open(\"README.md\", \"w\") as f:\n",
        "    f.write(readme_content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "token=access_token_write"
      ],
      "metadata": {
        "id": "ljyhySbFwpas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "api.get_token_permission(token=access_token_write)\n",
        "repo_id = 'frankmorales2020/FlightPlan_Transformer_LLM'\n",
        "api.delete_repo(repo_id=repo_id)"
      ],
      "metadata": {
        "id": "A0WynixX3e_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from huggingface_hub import HfApi, HfFolder\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- Push to Hugging Face Hub ---\n",
        "\n",
        "repo_name = \"frankmorales2020/FlightPlan_Transformer_LLM\"\n",
        "create_repo(repo_name, private=False, exist_ok=True)\n",
        "\n",
        "# --- Push the model directly from memory ---\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"README.md\",\n",
        "    path_in_repo=\"README.md\",\n",
        "    repo_id=repo_name,\n",
        "    repo_type=\"model\",\n",
        ")\n",
        "\n",
        "# Save the model and tokenizer to a temporary directory\n",
        "model_save_dir = \"flight_plan_transformer_model\"\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "# 1. Create a configuration dictionary for your model\n",
        "config = {\n",
        "    \"vocab_size\": tokenizer.vocab_size,\n",
        "    \"embed_dim\": embedding_dimension,  # Use your embedding dimension variable\n",
        "    \"num_heads\": num_heads,  # Use your num_heads variable\n",
        "    \"ff_hidden_dim\": feed_forward_dimension,  # Use your feed_forward_dimension variable\n",
        "    \"num_layers\": num_encoder_layers,  # Use your num_encoder_layers variable\n",
        "    \"block_size\": block_size,  # Use your block_size variable\n",
        "    \"dropout\": dropout_probability  # Use your dropout_probability variable\n",
        "}\n",
        "\n",
        "# 2. Save the configuration to config.json\n",
        "with open(os.path.join(model_save_dir, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "# Save the model's state_dict instead of using save_pretrained\n",
        "torch.save(model.state_dict(), os.path.join(model_save_dir, 'pytorch_model.bin'))\n",
        "tokenizer.save_pretrained(model_save_dir) # This is fine as tokenizer is from Hugging Face\n",
        "\n",
        "\n",
        "# Upload the model files, including config.json and pytorch_model.bin\n",
        "upload_file(\n",
        "    path_or_fileobj=os.path.join(model_save_dir, \"config.json\"),\n",
        "    path_in_repo=\"config.json\",\n",
        "    repo_id=repo_name,\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj=os.path.join(model_save_dir, \"pytorch_model.bin\"),\n",
        "    path_in_repo=\"pytorch_model.bin\",\n",
        "    repo_id=repo_name,\n",
        ")\n",
        "\n",
        "# Upload other files (e.g., tokenizer files)\n",
        "for filename in os.listdir(model_save_dir):\n",
        "    if filename not in [\"config.json\", \"pytorch_model.bin\"]:  # Exclude already uploaded files\n",
        "        upload_file(\n",
        "            path_or_fileobj=os.path.join(model_save_dir, filename),\n",
        "            path_in_repo=filename,\n",
        "            repo_id=repo_name,\n",
        "        )\n",
        "\n",
        "\n",
        "# Remove the temporary directory (optional)\n",
        "shutil.rmtree(model_save_dir)\n",
        "\n",
        "print(f\"Model pushed to Hugging Face Hub: {repo_name}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "McMnf9stGCch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "PXqlKKDxcZYy"
      }
    },
    {
      "source": [
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import torch\n",
        "import json\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# --- Updated Model Loading ---\n",
        "pretrained_model_name = \"frankmorales2020/FlightPlan_Transformer_LLM\"\n",
        "\n",
        "# 1. Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
        "\n",
        "# 2. Load configuration using hf_hub_download\n",
        "config_path = hf_hub_download(repo_id=pretrained_model_name, filename=\"config.json\")\n",
        "with open(config_path, 'r') as f:\n",
        "    config_dict = json.load(f)\n",
        "\n",
        "# *** Add max_waypoints to the config dictionary or load it from elsewhere ***\n",
        "config_dict['max_waypoints'] = 10  # Replace with your actual max_waypoints value\n",
        "\n",
        "# 3. Instantiate your SimpleTransformer class using the loaded configuration\n",
        "model = SimpleTransformer(\n",
        "    vocab_size=config_dict['vocab_size'],\n",
        "    embed_dim=config_dict['embed_dim'],\n",
        "    num_heads=config_dict['num_heads'],\n",
        "    ff_hidden_dim=config_dict['ff_hidden_dim'],\n",
        "    num_layers=config_dict['num_layers'],\n",
        "    block_size=config_dict['block_size'],\n",
        "    dropout=config_dict['dropout'],\n",
        "    max_waypoints=config_dict['max_waypoints'] # Pass max_waypoints here\n",
        ")\n",
        "\n",
        "# 4. Load the model weights using hf_hub_download\n",
        "model_path = hf_hub_download(repo_id=pretrained_model_name, filename=\"pytorch_model.bin\")\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mGw7baHNYylW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "JxhoIAxeY4Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load and Preprocess Flight Plan Data ---\n",
        "flight_plan_dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "\n",
        "# Split into train and validation sets\n",
        "flight_plan_dataset = flight_plan_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
        "flight_plan_dataset = DatasetDict({\n",
        "    'train': flight_plan_dataset['train'],\n",
        "    'validation': flight_plan_dataset['test']\n",
        "})"
      ],
      "metadata": {
        "id": "ksojr-ek6lLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfbuUlKHxl6a"
      },
      "outputs": [],
      "source": [
        "flight_plan_dataset['train'][1]"
      ]
    },
    {
      "source": [
        "import random\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.distance import geodesic\n",
        "from tqdm import tqdm\n",
        "from geopy.point import Point  # Import Point object\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Define airport codes and names\n",
        "airports = [\n",
        "    # North America\n",
        "    \"ATL\", \"LAX\", \"ORD\", \"DFW\", \"DEN\", \"JFK\", \"SFO\", \"LAS\", \"SEA\", \"CLT\",\n",
        "    \"MIA\", \"PHL\", \"EWR\", \"BOS\", \"PHX\", \"MCO\", \"IAH\", \"DAL\", \"FLL\", \"DTW\",\n",
        "    \"MSP\", \"IAD\", \"BWI\", \"SLC\", \"SAN\", \"MDW\", \"TPA\", \"PDX\", \"BNA\", \"STL\",\n",
        "    # Europe\n",
        "    \"LHR\", \"CDG\", \"AMS\", \"FRA\", \"MAD\", \"BCN\", \"FCO\", \"MUC\", \"DUB\", \"ZRH\",\n",
        "    # Asia\n",
        "    \"HND\", \"PVG\", \"ICN\", \"SIN\", \"KUL\", \"BKK\", \"DXB\", \"DEL\", \"HKG\", \"NRT\",\n",
        "    # Caribbean\n",
        "    \"CUN\", \"PUJ\", \"SJU\", \"MBJ\", \"NAS\",\n",
        "    # South America\n",
        "    \"GRU\", \"BOG\", \"EZE\", \"SCL\", \"LIM\",\n",
        "    # Australia & New Zealand\n",
        "    \"SYD\", \"MEL\", \"BNE\", \"PER\", \"AKL\"\n",
        "]\n",
        "\n",
        "airport_names = {\n",
        "    \"ATL\": \"Hartsfield-Jackson Atlanta International Airport\",\n",
        "    \"LAX\": \"Los Angeles International Airport\",\n",
        "    \"ORD\": \"Chicago O'Hare International Airport\",\n",
        "    \"DFW\": \"Dallas/Fort Worth International Airport\",\n",
        "    \"DEN\": \"Denver International Airport\",\n",
        "    \"JFK\": \"John F. Kennedy International Airport\",\n",
        "    \"SFO\": \"San Francisco International Airport\",\n",
        "    \"LAS\": \"Harry Reid International Airport\",\n",
        "    \"SEA\": \"Seattle-Tacoma International Airport\",\n",
        "    \"CLT\": \"Charlotte Douglas International Airport\",\n",
        "    \"MIA\": \"Miami International Airport\",\n",
        "    \"PHL\": \"Philadelphia International Airport\",\n",
        "    \"EWR\": \"Newark Liberty International Airport\",\n",
        "    \"BOS\": \"Logan International Airport\",\n",
        "    \"PHX\": \"Phoenix Sky Harbor International Airport\",\n",
        "    \"MCO\": \"Orlando International Airport\",\n",
        "    \"IAH\": \"George Bush Intercontinental Airport\",\n",
        "    \"DAL\": \"Dallas Love Field\",\n",
        "    \"FLL\": \"Fort Lauderdale-Hollywood International Airport\",\n",
        "    \"DTW\": \"Detroit Metropolitan Airport\",\n",
        "    \"MSP\": \"Minneapolis-Saint Paul International Airport\",\n",
        "    \"IAD\": \"Washington Dulles International Airport\",\n",
        "    \"BWI\": \"Baltimore/Washington International Airport\",\n",
        "    \"SLC\": \"Salt Lake City International Airport\",\n",
        "    \"SAN\": \"San Diego International Airport\",\n",
        "    \"MDW\": \"Midway International Airport\",\n",
        "    \"TPA\": \"Tampa International Airport\",\n",
        "    \"PDX\": \"Portland International Airport\",\n",
        "    \"BNA\": \"Nashville International Airport\",\n",
        "    \"STL\": \"St. Louis Lambert International Airport\",\n",
        "    # Europe\n",
        "    \"LHR\": \"London Heathrow Airport\",\n",
        "    \"CDG\": \"Charles de Gaulle Airport\",\n",
        "    \"AMS\": \"Amsterdam Airport Schiphol\",\n",
        "    \"FRA\": \"Frankfurt Airport\",\n",
        "    \"MAD\": \"Adolfo Suárez Madrid–Barajas Airport\",\n",
        "    \"BCN\": \"Barcelona–El Prat Airport\",\n",
        "    \"FCO\": \"Leonardo da Vinci–Fiumicino Airport\",\n",
        "    \"MUC\": \"Munich Airport\",\n",
        "    \"DUB\": \"Dublin Airport\",\n",
        "    \"ZRH\": \"Zurich Airport\",\n",
        "    # Asia\n",
        "    \"HND\": \"Haneda Airport\",\n",
        "    \"PVG\": \"Shanghai Pudong International Airport\",\n",
        "    \"ICN\": \"Incheon International Airport\",\n",
        "    \"SIN\": \"Singapore Changi Airport\",\n",
        "    \"KUL\": \"Kuala Lumpur International Airport\",\n",
        "    \"BKK\": \"Suvarnabhumi Airport\",\n",
        "    \"DXB\": \"Dubai International Airport\",\n",
        "    \"DEL\": \"Indira Gandhi International Airport\",\n",
        "    \"HKG\": \"Hong Kong International Airport\",\n",
        "    \"NRT\": \"Narita International Airport\",\n",
        "    # Caribbean\n",
        "    \"CUN\": \"Cancún International Airport\",\n",
        "    \"PUJ\": \"Punta Cana International Airport\",\n",
        "    \"SJU\": \"Luis Muñoz Marín International Airport\",\n",
        "    \"MBJ\": \"Sangster International Airport\",\n",
        "    \"NAS\": \"Lynden Pindling International Airport\",\n",
        "    # South America\n",
        "    \"GRU\": \"São Paulo–Guarulhos International Airport\",\n",
        "    \"BOG\": \"El Dorado International Airport\",\n",
        "    \"EZE\": \"Ministro Pistarini International Airport\",\n",
        "    \"SCL\": \"Arturo Merino Benítez International Airport\",\n",
        "    \"LIM\": \"Jorge Chávez International Airport\",\n",
        "    # Australia & New Zealand\n",
        "    \"SYD\": \"Sydney Airport\",\n",
        "    \"MEL\": \"Melbourne Airport\",\n",
        "    \"BNE\": \"Brisbane Airport\",\n",
        "    \"PER\": \"Perth Airport\",\n",
        "    \"AKL\": \"Auckland Airport\"\n",
        "}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bU3ovgeD-NTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# ... (other imports and definitions) ...\n",
        "\n",
        "def generate_flight_plan(model, tokenizer, start_text, max_new_tokens=256, device='cuda'):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    model.to(device)  # Ensure the model is on the correct device\n",
        "\n",
        "    # Tokenize input\n",
        "    start_tokens = tokenizer(start_text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # 1. Clip input_ids\n",
        "    start_tokens['input_ids'] = start_tokens['input_ids'].clip(0, tokenizer.vocab_size - 1).type(torch.int64)\n",
        "\n",
        "    # 2. Create a new attention_mask with correct shape and type\n",
        "    seq_len = start_tokens['input_ids'].shape[1]\n",
        "    start_tokens['attention_mask'] = torch.ones(1, seq_len, dtype=torch.bool, device=device) # Directly on device\n",
        "\n",
        "    # 3. Move other tensors to device\n",
        "    start_tokens['input_ids'] = start_tokens['input_ids'].to(device)\n",
        "\n",
        "    # Generate waypoints\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            # *** Get the number of waypoints from input or another source ***\n",
        "            num_waypoints = torch.tensor([5], dtype=torch.long, device=device)  # Assuming 5 waypoints here\n",
        "\n",
        "            # Now call model with num_waypoints as a Tensor\n",
        "            predicted_waypoints = model(start_tokens['input_ids'], start_tokens['attention_mask'], num_waypoints)\n",
        "\n",
        "            # Check if model generated empty or invalid waypoints\n",
        "            if predicted_waypoints.shape[0] == 0 or predicted_waypoints.shape[1] == 0 or torch.isnan(predicted_waypoints).any():\n",
        "                print(\"Warning: Model generated empty or invalid waypoints.\")\n",
        "                return []  # Return an empty list if waypoints are empty or invalid\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e):\n",
        "                print(f\"Error: {e}. Try reducing batch size or sequence length.\")\n",
        "                torch.cuda.empty_cache()  # Clear CUDA memory\n",
        "                return []  # Return an empty list to avoid hanging\n",
        "            else:\n",
        "                raise  # Re-raise the error if it's not memory-related\n",
        "\n",
        "    # Process waypoints and return as a list\n",
        "    waypoints_list = predicted_waypoints.squeeze(0).cpu().tolist()  # Move to CPU before converting to list\n",
        "\n",
        "    # --- Debugging Prints ---\n",
        "    print(\"\\n--- generate_flight_plan ---\")\n",
        "    print(\"start_text:\", start_text)\n",
        "    print(\"start_tokens['input_ids'] shape:\", start_tokens['input_ids'].shape)\n",
        "    print(\"start_tokens['attention_mask'] shape:\", start_tokens['attention_mask'].shape)\n",
        "    print(\"num_waypoints:\", num_waypoints)\n",
        "    print(\"predicted_waypoints shape:\", predicted_waypoints.shape)\n",
        "    print(\"waypoints_list:\", waypoints_list)\n",
        "\n",
        "    return waypoints_list\n",
        "\n",
        "# ... (rest of the code - same as before) ..."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "jA93ZPqN9yQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#query='Calculate the waypoints from DXB to MSP. Departure: 2024-05-03, Aircraft: Boeing 777, Weather: Overcast'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "query= 'Calculate the waypoints from LHR to AMS. Departure: 2024-06-15, Aircraft: Piper PA-28, Weather: Partly Cloudy'\n",
        "print(f\"Query: {query}\")\n",
        "generated_plan = generate_flight_plan(model, tokenizer, start_text=query, device=device) # Pass the dataset here\n",
        "print(\"\\nGenerated waypoint list:\")\n",
        "print(generated_plan)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QPkzNnVTFpqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "NGQToWe1cj5F"
      ],
      "authorship_tag": "ABX9TyNe1iV3+Yh+XfRJX41WLvNy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}