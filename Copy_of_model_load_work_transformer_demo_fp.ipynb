{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/Copy_of_model_load_work_transformer_demo_fp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model environmenmt"
      ],
      "metadata": {
        "id": "Rb8HQCGUhEgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q"
      ],
      "metadata": {
        "id": "GM7ASLUOgp4N"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write, # ADD YOUR TOKEN HERE\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "30HOQNnVg1D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LpoVJAMMt74m"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7cb3yWFuyoy"
      },
      "outputs": [],
      "source": [
        "!pip show transformers\n",
        "print()\n",
        "!pip show datasets\n",
        "print()\n",
        "!pip show torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsnHRSWBL5Va"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.distance import geodesic\n",
        "from tqdm import tqdm\n",
        "from geopy.point import Point  # Import Point object\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Define airport codes and names\n",
        "airports = [\n",
        "    # North America\n",
        "    \"ATL\", \"LAX\", \"ORD\", \"DFW\", \"DEN\", \"JFK\", \"SFO\", \"LAS\", \"SEA\", \"CLT\",\n",
        "    \"MIA\", \"PHL\", \"EWR\", \"BOS\", \"PHX\", \"MCO\", \"IAH\", \"DAL\", \"FLL\", \"DTW\",\n",
        "    \"MSP\", \"IAD\", \"BWI\", \"SLC\", \"SAN\", \"MDW\", \"TPA\", \"PDX\", \"BNA\", \"STL\",\n",
        "    # Europe\n",
        "    \"LHR\", \"CDG\", \"AMS\", \"FRA\", \"MAD\", \"BCN\", \"FCO\", \"MUC\", \"DUB\", \"ZRH\",\n",
        "    # Asia\n",
        "    \"HND\", \"PVG\", \"ICN\", \"SIN\", \"KUL\", \"BKK\", \"DXB\", \"DEL\", \"HKG\", \"NRT\",\n",
        "    # Caribbean\n",
        "    \"CUN\", \"PUJ\", \"SJU\", \"MBJ\", \"NAS\",\n",
        "    # South America\n",
        "    \"GRU\", \"BOG\", \"EZE\", \"SCL\", \"LIM\",\n",
        "    # Australia & New Zealand\n",
        "    \"SYD\", \"MEL\", \"BNE\", \"PER\", \"AKL\"\n",
        "]\n",
        "\n",
        "airport_names = {\n",
        "    \"ATL\": \"Hartsfield-Jackson Atlanta International Airport\",\n",
        "    \"LAX\": \"Los Angeles International Airport\",\n",
        "    \"ORD\": \"Chicago O'Hare International Airport\",\n",
        "    \"DFW\": \"Dallas/Fort Worth International Airport\",\n",
        "    \"DEN\": \"Denver International Airport\",\n",
        "    \"JFK\": \"John F. Kennedy International Airport\",\n",
        "    \"SFO\": \"San Francisco International Airport\",\n",
        "    \"LAS\": \"Harry Reid International Airport\",\n",
        "    \"SEA\": \"Seattle-Tacoma International Airport\",\n",
        "    \"CLT\": \"Charlotte Douglas International Airport\",\n",
        "    \"MIA\": \"Miami International Airport\",\n",
        "    \"PHL\": \"Philadelphia International Airport\",\n",
        "    \"EWR\": \"Newark Liberty International Airport\",\n",
        "    \"BOS\": \"Logan International Airport\",\n",
        "    \"PHX\": \"Phoenix Sky Harbor International Airport\",\n",
        "    \"MCO\": \"Orlando International Airport\",\n",
        "    \"IAH\": \"George Bush Intercontinental Airport\",\n",
        "    \"DAL\": \"Dallas Love Field\",\n",
        "    \"FLL\": \"Fort Lauderdale-Hollywood International Airport\",\n",
        "    \"DTW\": \"Detroit Metropolitan Airport\",\n",
        "    \"MSP\": \"Minneapolis-Saint Paul International Airport\",\n",
        "    \"IAD\": \"Washington Dulles International Airport\",\n",
        "    \"BWI\": \"Baltimore/Washington International Airport\",\n",
        "    \"SLC\": \"Salt Lake City International Airport\",\n",
        "    \"SAN\": \"San Diego International Airport\",\n",
        "    \"MDW\": \"Midway International Airport\",\n",
        "    \"TPA\": \"Tampa International Airport\",\n",
        "    \"PDX\": \"Portland International Airport\",\n",
        "    \"BNA\": \"Nashville International Airport\",\n",
        "    \"STL\": \"St. Louis Lambert International Airport\",\n",
        "    # Europe\n",
        "    \"LHR\": \"London Heathrow Airport\",\n",
        "    \"CDG\": \"Charles de Gaulle Airport\",\n",
        "    \"AMS\": \"Amsterdam Airport Schiphol\",\n",
        "    \"FRA\": \"Frankfurt Airport\",\n",
        "    \"MAD\": \"Adolfo Suárez Madrid–Barajas Airport\",\n",
        "    \"BCN\": \"Barcelona–El Prat Airport\",\n",
        "    \"FCO\": \"Leonardo da Vinci–Fiumicino Airport\",\n",
        "    \"MUC\": \"Munich Airport\",\n",
        "    \"DUB\": \"Dublin Airport\",\n",
        "    \"ZRH\": \"Zurich Airport\",\n",
        "    # Asia\n",
        "    \"HND\": \"Haneda Airport\",\n",
        "    \"PVG\": \"Shanghai Pudong International Airport\",\n",
        "    \"ICN\": \"Incheon International Airport\",\n",
        "    \"SIN\": \"Singapore Changi Airport\",\n",
        "    \"KUL\": \"Kuala Lumpur International Airport\",\n",
        "    \"BKK\": \"Suvarnabhumi Airport\",\n",
        "    \"DXB\": \"Dubai International Airport\",\n",
        "    \"DEL\": \"Indira Gandhi International Airport\",\n",
        "    \"HKG\": \"Hong Kong International Airport\",\n",
        "    \"NRT\": \"Narita International Airport\",\n",
        "    # Caribbean\n",
        "    \"CUN\": \"Cancún International Airport\",\n",
        "    \"PUJ\": \"Punta Cana International Airport\",\n",
        "    \"SJU\": \"Luis Muñoz Marín International Airport\",\n",
        "    \"MBJ\": \"Sangster International Airport\",\n",
        "    \"NAS\": \"Lynden Pindling International Airport\",\n",
        "    # South America\n",
        "    \"GRU\": \"São Paulo–Guarulhos International Airport\",\n",
        "    \"BOG\": \"El Dorado International Airport\",\n",
        "    \"EZE\": \"Ministro Pistarini International Airport\",\n",
        "    \"SCL\": \"Arturo Merino Benítez International Airport\",\n",
        "    \"LIM\": \"Jorge Chávez International Airport\",\n",
        "    # Australia & New Zealand\n",
        "    \"SYD\": \"Sydney Airport\",\n",
        "    \"MEL\": \"Melbourne Airport\",\n",
        "    \"BNE\": \"Brisbane Airport\",\n",
        "    \"PER\": \"Perth Airport\",\n",
        "    \"AKL\": \"Auckland Airport\"\n",
        "}"
      ],
      "metadata": {
        "id": "5IbgqgPUzWws"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Bac8KFM-kSIJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer_name = 'gpt2'\n",
        "#tokenizer_name = 'meta-llama/Meta-Llama-3-8B'\n",
        "#tokenizer_name = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "pad_token = '<pad>'\n",
        "\n",
        "# --- Load Tokenizer and Add Pad Token ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': pad_token})\n",
        "# Ensure pad_token_id is within the vocabulary size\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "if pad_token_id >= tokenizer.vocab_size:\n",
        "    pad_token_id = tokenizer.vocab_size - 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "flight_plan_dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "flight_plan_dataset['train'][7]"
      ],
      "metadata": {
        "id": "s7CcHjhRYZjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "\n",
        "def preprocess_flight_plan(example):\n",
        "    \"\"\"\n",
        "    Preprocesses a single flight plan example.\n",
        "    \"\"\"\n",
        "    input_text = example['input']\n",
        "\n",
        "    # Tokenize the input text\n",
        "    tokens = tokenizer(\n",
        "        input_text,\n",
        "        truncation=True,\n",
        "        max_length=sequence_length,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    # Ensure input_ids and attention_mask are tensors and have the correct shape\n",
        "    for key in ['input_ids', 'attention_mask']:\n",
        "        tokens[key] = tokens[key].squeeze(0)  # Remove extra dimension if present\n",
        "\n",
        "    # Extract relevant data\n",
        "    tokens['num_waypoints'] = example['label']\n",
        "\n",
        "    # Convert waypoints to tensor here\n",
        "    tokens['waypoints'] = torch.tensor(example['waypoints'][1:-1], dtype=torch.float32)  # Intermediate waypoints\n",
        "\n",
        "    # Handle out-of-vocabulary tokens (clip input_ids)\n",
        "    tokens['input_ids'] = tokens['input_ids'].clip(0, tokenizer.vocab_size - 1)\n",
        "\n",
        "    return tokens"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gH6EmYPP9dNR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "source": [
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collates a batch of flight plan examples.\n",
        "    \"\"\"\n",
        "    # Find the maximum number of waypoints in the batch\n",
        "    max_waypoints = max(len(item['waypoints']) for item in batch)\n",
        "\n",
        "    # Pad waypoints with [0.0, 0.0] to the maximum length\n",
        "    padded_waypoints = [\n",
        "        torch.cat([item['waypoints'], torch.zeros((max_waypoints - len(item['waypoints']), 2), dtype=torch.float32)])\n",
        "        for item in batch\n",
        "    ]\n",
        "\n",
        "    # Stack the data (using torch.stack directly)\n",
        "    collated_batch = {\n",
        "        'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
        "        'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
        "        'waypoints': torch.stack(padded_waypoints),\n",
        "        'num_waypoints': torch.tensor([item['num_waypoints'] for item in batch]),\n",
        "    }\n",
        "    return collated_batch"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "peGaonZh2oVy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "source": [
        "!pip install transformers datasets torch geopy -q\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from geopy.geocoders import Nominatim\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "# Set environment variables for CUDA\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "batch_size = 4  # Reduced batch size\n",
        "sequence_length = 256\n",
        "embedding_dimension = 256\n",
        "num_heads = 8\n",
        "feed_forward_dimension = 1024\n",
        "num_encoder_layers = 6\n",
        "dropout_probability = 0.3  # Increased dropout\n",
        "learning_rate = 2e-5  # Reduced learning rate\n",
        "num_epochs = 1  # Increased epochs\n",
        "tokenizer_name = 'gpt2'\n",
        "pad_token = '<pad>'\n",
        "waypoint_count_penalty_weight = 10.0  # Increased penalty weight\n",
        "max_waypoints = 10  # Maximum number of waypoints for length embedding\n",
        "\n",
        "\n",
        "\n",
        "# --- Load Tokenizer and Add Pad Token ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': pad_token})\n",
        "pad_token_id = tokenizer.pad_token_id\n",
        "if pad_token_id >= tokenizer.vocab_size:\n",
        "    pad_token_id = tokenizer.vocab_size - 1\n",
        "\n",
        "# *** Add airport codes to tokenizer vocabulary ***\n",
        "for airport_code in airports:\n",
        "    if airport_code not in tokenizer.vocab:  # Check if already in vocab\n",
        "        tokenizer.add_tokens([airport_code])\n",
        "\n",
        "# --- Load and Preprocess Flight Plan Data ---\n",
        "flight_plan_dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "\n",
        "# Split into train and validation sets\n",
        "flight_plan_dataset = flight_plan_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
        "flight_plan_dataset = DatasetDict({\n",
        "    'train': flight_plan_dataset['train'],\n",
        "    'validation': flight_plan_dataset['test']\n",
        "})\n",
        "\n",
        "# --- Define the Transformer Model (SimpleTransformer with length embedding) ---\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers, block_size, dropout, max_waypoints):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Embedding(block_size, embed_dim)\n",
        "        self.length_embedding = nn.Embedding(max_waypoints + 1, embed_dim)  # Length embedding\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "        self.linear_out = nn.Linear(embed_dim, 2)  # Output layer for latitude and longitude\n",
        "        self.block_size = block_size\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, num_waypoints):\n",
        "        positions = torch.arange(0, input_ids.size(1), dtype=torch.long, device=input_ids.device).unsqueeze(0)\n",
        "        pos_embeddings = self.pos_embedding(positions)\n",
        "\n",
        "        # Get length embedding\n",
        "        #num_waypoints = num_waypoints.clip(0, self.length_embedding.num_embeddings - 1)  # Clamp num_waypoints  Removed for Training\n",
        "        length_embed = self.length_embedding(num_waypoints)\n",
        "        # Expand length embedding to match input sequence length\n",
        "        length_embed = length_embed.unsqueeze(1).repeat(1, input_ids.shape[1], 1)\n",
        "\n",
        "        # Combine embeddings\n",
        "        embeddings = self.dropout(self.embedding(input_ids) + pos_embeddings + length_embed)\n",
        "\n",
        "        encoder_output = self.encoder(embeddings, src_key_padding_mask=(attention_mask == 0))\n",
        "\n",
        "        # Reshape output for waypoint coordinates\n",
        "        output = self.linear_out(encoder_output)\n",
        "        return output\n",
        "\n",
        "# --- Haversine Distance Function ---\n",
        "import torch\n",
        "\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"\n",
        "    Calculate the Haversine distance between two points.\n",
        "    \"\"\"\n",
        "    # Convert to radians\n",
        "    lat1, lon1, lat2, lon2 = map(torch.deg2rad, [lat1, lon1, lat2, lon2])\n",
        "\n",
        "    # Apply Haversine formula\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = torch.sin(dlat / 2)**2 + torch.cos(lat1) * torch.cos(lat2) * torch.sin(dlon / 2)**2\n",
        "    c = 2 * torch.arcsin(torch.sqrt(a))\n",
        "    r = 6371  # Radius of earth in kilometers. Use 3956 for miles\n",
        "    return c * r\n",
        "\n",
        "# --- Loss Function with Waypoint Count Penalty ---\n",
        "def custom_loss(pred, target, num_waypoints_target):\n",
        "    distance_loss = torch.mean(haversine_distance(\n",
        "            pred[:, :num_waypoints_target.max(), 0],\n",
        "            pred[:, :num_waypoints_target.max(), 1],\n",
        "            target[:, :num_waypoints_target.max(), 0],\n",
        "            target[:, :num_waypoints_target.max(), 1],\n",
        "        ))\n",
        "\n",
        "    # Penalty for exceeding the target number of waypoints\n",
        "    extra_waypoints = torch.clamp(pred.shape[1] - num_waypoints_target.max(), min=0)\n",
        "\n",
        "    # *** Modified Penalty Calculation (Squared Difference and Increased Weight) ***\n",
        "    waypoint_count_penalty_weight = 50.0  # Increased from 10.0\n",
        "    waypoint_count_penalty = waypoint_count_penalty_weight * (extra_waypoints.float() ** 2)\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = distance_loss + waypoint_count_penalty\n",
        "    return total_loss\n",
        "# --- Instantiate the model ---\n",
        "vocab_size = tokenizer.vocab_size\n",
        "block_size = sequence_length\n",
        "model = SimpleTransformer(vocab_size, embedding_dimension, num_heads,\n",
        "                          feed_forward_dimension, num_encoder_layers,\n",
        "                          block_size, dropout_probability, max_waypoints)\n",
        "\n",
        "\n",
        "# Update the embedding layer of the model to accommodate the new tokens\n",
        "model.embedding = nn.Embedding(tokenizer.vocab_size, embedding_dimension)\n",
        "\n",
        "\n",
        "# --- Move model to CUDA device ---\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# --- Create DataLoaders ---\n",
        "processed_dataset = flight_plan_dataset.map(preprocess_flight_plan, batched=False)  # Process individually\n",
        "train_loader = DataLoader(processed_dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "validation_loader = DataLoader(processed_dataset['validation'], batch_size=batch_size, shuffle=False,\n",
        "                              collate_fn=custom_collate_fn)\n",
        "\n",
        "# --- Optimizer ---\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# --- Training Loop ---\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        # 1. Data Preparation\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        num_waypoints_target = batch['num_waypoints'].to(device)\n",
        "\n",
        "        # Clamp num_waypoints_target and ensure correct data type and device:\n",
        "        num_waypoints_target = num_waypoints_target.clip(0, model.length_embedding.num_embeddings - 1)\n",
        "        num_waypoints_target = num_waypoints_target.type(torch.LongTensor).to(device)\n",
        "\n",
        "        # Extract intermediate waypoints as target\n",
        "        target_waypoints = batch['waypoints'].to(device) # Get the padded waypoints here\n",
        "\n",
        "        # 2. Model Prediction\n",
        "        optimizer.zero_grad()\n",
        "        predicted_waypoints = model(input_ids, attention_mask, num_waypoints_target)\n",
        "\n",
        "        # 3. Loss Calculation (with penalty for exceeding waypoint count)\n",
        "        loss = criterion(predicted_waypoints, target_waypoints, num_waypoints_target)\n",
        "\n",
        "        # 4. Optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# --- Training Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs), desc=\"Training/Epochs\"):\n",
        "        train_loss = train(model, train_loader, optimizer, custom_loss, device)\n",
        "        print('\\n')\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dCIeE9VJ51Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Deployment To HF"
      ],
      "metadata": {
        "id": "NGQToWe1cj5F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBk-L1bzknrr"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade huggingface_hub -q\n",
        "from huggingface_hub import notebook_login, create_repo, upload_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cVI94vQHFle"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login, create_repo, upload_file\n",
        "\n",
        "repo_name = \"frankmorales2020/FlightPlan_Transformer_LLM\"\n",
        "\n",
        "# --- Automatically generate README.md ---\n",
        "readme_content = f\"\"\"\n",
        "# Flight Plan Transformer\n",
        "\n",
        "This repository contains a Transformer model trained to generate flight plan waypoints based on textual input. The model was trained using the \"frankmorales2020/flight_plan_waypoints\" dataset from Hugging Face Datasets.\n",
        "\n",
        "## Model Description\n",
        "\n",
        "The model is a decoder-only Transformer architecture implemented using PyTorch and the `transformers` library. It takes textual input describing the flight (e.g., \"Calculate the waypoints from SIN to CUN. Departure: 2024-06-19, Aircraft: Airbus A320, Weather: Partly Cloudy\") and predicts a sequence of waypoints representing the flight path.\n",
        "\n",
        "## Intended Use\n",
        "\n",
        "This model can be used to:\n",
        "\n",
        "- **Generate flight plans:** Given flight details, the model can predict waypoints for the route.\n",
        "- **Flight planning assistance:** It can serve as a tool to aid in flight planning, providing suggestions for waypoints.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "- **Accuracy:** The model's accuracy is limited by the quality and coverage of the training data. It may not be perfectly accurate in all scenarios.\n",
        "- **Real-world applicability:** This model is a research prototype and should not be used for actual flight navigation without thorough validation and safety checks.\n",
        "\n",
        "## How to Use\n",
        "\n",
        "1. **Install the necessary libraries:**\n",
        "Use code with caution\n",
        "bash pip install transformers datasets torch\n",
        "\n",
        "2. **Load the model and tokenizer:**\n",
        "Use code with caution\n",
        "python from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{repo_name}\") model = AutoModelForSeq2SeqLM.from_pretrained(\"{repo_name}\")\n",
        "\n",
        "3. **Generate waypoints:**\n",
        "Use code with caution\n",
        "python def generate_flight_plan(model, tokenizer, start_text, max_new_tokens=200):\n",
        "\n",
        "# ... (Your generation code from the original script) ...\n",
        "4. **Example usage:**\n",
        "Use code with caution\n",
        "python query = \"Calculate the waypoints from SIN to CUN. Departure: 2024-06-19, Aircraft: Airbus A320, Weather: Partly Cloudy\" generated_plan = generate_flight_plan(model, tokenizer, start_text=query, max_new_tokens=256) print(generated_plan)\n",
        "\n",
        "## Training Data\n",
        "\n",
        "The model was trained on the \"frankmorales2020/flight_plan_waypoints\" dataset from Hugging Face Datasets. This dataset contains flight plans with corresponding waypoints.\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "The model's performance was evaluated on a held-out portion of the training data. (Include specific metrics or evaluation results if available)\n",
        "\n",
        "## Acknowledgements\n",
        "\n",
        "- Thanks to Hugging Face for providing the `transformers` and `datasets` libraries.\n",
        "- Thanks to frankmorales2020 for creating and sharing the flight plan dataset.\n",
        "\n",
        "## Contact\n",
        "Frank Morales, BEng, MEng, SMIEEE\n",
        "Boeing Associate Technical Fellow\n",
        "Linkedin: https://www.linkedin.com/in/frank-morales1964/\n",
        "\"\"\"\n",
        "\n",
        "# Write README.md to file\n",
        "with open(\"README.md\", \"w\") as f:\n",
        "    f.write(readme_content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "token=access_token_write"
      ],
      "metadata": {
        "id": "ljyhySbFwpas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "api.get_token_permission(token=access_token_write)\n",
        "repo_id = 'frankmorales2020/FlightPlan_Transformer_LLM'\n",
        "api.delete_repo(repo_id=repo_id)"
      ],
      "metadata": {
        "id": "A0WynixX3e_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from huggingface_hub import HfApi, HfFolder\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- Push to Hugging Face Hub ---\n",
        "\n",
        "repo_name = \"frankmorales2020/FlightPlan_Transformer_LLM\"\n",
        "create_repo(repo_name, private=False, exist_ok=True)\n",
        "\n",
        "# --- Push the model directly from memory ---\n",
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"README.md\",\n",
        "    path_in_repo=\"README.md\",\n",
        "    repo_id=repo_name,\n",
        "    repo_type=\"model\",\n",
        ")\n",
        "\n",
        "# Save the model and tokenizer to a temporary directory\n",
        "model_save_dir = \"flight_plan_transformer_model\"\n",
        "os.makedirs(model_save_dir, exist_ok=True)\n",
        "\n",
        "# 1. Create a configuration dictionary for your model\n",
        "config = {\n",
        "    \"vocab_size\": tokenizer.vocab_size,\n",
        "    \"embed_dim\": embedding_dimension,  # Use your embedding dimension variable\n",
        "    \"num_heads\": num_heads,  # Use your num_heads variable\n",
        "    \"ff_hidden_dim\": feed_forward_dimension,  # Use your feed_forward_dimension variable\n",
        "    \"num_layers\": num_encoder_layers,  # Use your num_encoder_layers variable\n",
        "    \"block_size\": block_size,  # Use your block_size variable\n",
        "    \"dropout\": dropout_probability  # Use your dropout_probability variable\n",
        "}\n",
        "\n",
        "# 2. Save the configuration to config.json\n",
        "with open(os.path.join(model_save_dir, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f)\n",
        "\n",
        "# Save the model's state_dict instead of using save_pretrained\n",
        "torch.save(model.state_dict(), os.path.join(model_save_dir, 'pytorch_model.bin'))\n",
        "tokenizer.save_pretrained(model_save_dir) # This is fine as tokenizer is from Hugging Face\n",
        "\n",
        "\n",
        "# Upload the model files, including config.json and pytorch_model.bin\n",
        "upload_file(\n",
        "    path_or_fileobj=os.path.join(model_save_dir, \"config.json\"),\n",
        "    path_in_repo=\"config.json\",\n",
        "    repo_id=repo_name,\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj=os.path.join(model_save_dir, \"pytorch_model.bin\"),\n",
        "    path_in_repo=\"pytorch_model.bin\",\n",
        "    repo_id=repo_name,\n",
        ")\n",
        "\n",
        "# Upload other files (e.g., tokenizer files)\n",
        "for filename in os.listdir(model_save_dir):\n",
        "    if filename not in [\"config.json\", \"pytorch_model.bin\"]:  # Exclude already uploaded files\n",
        "        upload_file(\n",
        "            path_or_fileobj=os.path.join(model_save_dir, filename),\n",
        "            path_in_repo=filename,\n",
        "            repo_id=repo_name,\n",
        "        )\n",
        "\n",
        "\n",
        "# Remove the temporary directory (optional)\n",
        "shutil.rmtree(model_save_dir)\n",
        "\n",
        "print(f\"Model pushed to Hugging Face Hub: {repo_name}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "McMnf9stGCch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Evaluation"
      ],
      "metadata": {
        "id": "PXqlKKDxcZYy"
      }
    },
    {
      "source": [
        "from transformers import AutoTokenizer, AutoConfig\n",
        "import torch\n",
        "import json\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# --- Updated Model Loading ---\n",
        "pretrained_model_name = \"frankmorales2020/FlightPlan_Transformer_LLM\"\n",
        "\n",
        "# 1. Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
        "\n",
        "# 2. Load configuration using hf_hub_download\n",
        "config_path = hf_hub_download(repo_id=pretrained_model_name, filename=\"config.json\")\n",
        "with open(config_path, 'r') as f:\n",
        "    config_dict = json.load(f)\n",
        "\n",
        "# *** Add max_waypoints to the config dictionary or load it from elsewhere ***\n",
        "config_dict['max_waypoints'] = 10  # Replace with your actual max_waypoints value\n",
        "\n",
        "# 3. Instantiate your SimpleTransformer class using the loaded configuration\n",
        "model = SimpleTransformer(\n",
        "    vocab_size=config_dict['vocab_size'],\n",
        "    embed_dim=config_dict['embed_dim'],\n",
        "    num_heads=config_dict['num_heads'],\n",
        "    ff_hidden_dim=config_dict['ff_hidden_dim'],\n",
        "    num_layers=config_dict['num_layers'],\n",
        "    block_size=config_dict['block_size'],\n",
        "    dropout=config_dict['dropout'],\n",
        "    max_waypoints=config_dict['max_waypoints'] # Pass max_waypoints here\n",
        ")\n",
        "\n",
        "# 4. Load the model weights using hf_hub_download\n",
        "model_path = hf_hub_download(repo_id=pretrained_model_name, filename=\"pytorch_model.bin\")\n",
        "state_dict = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "mGw7baHNYylW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "JxhoIAxeY4Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load and Preprocess Flight Plan Data ---\n",
        "flight_plan_dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")\n",
        "\n",
        "# Split into train and validation sets\n",
        "flight_plan_dataset = flight_plan_dataset['train'].train_test_split(test_size=0.2, seed=42)\n",
        "flight_plan_dataset = DatasetDict({\n",
        "    'train': flight_plan_dataset['train'],\n",
        "    'validation': flight_plan_dataset['test']\n",
        "})"
      ],
      "metadata": {
        "id": "ksojr-ek6lLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfbuUlKHxl6a"
      },
      "outputs": [],
      "source": [
        "flight_plan_dataset['train'][1]"
      ]
    },
    {
      "source": [
        "import random\n",
        "import math\n",
        "from datasets import Dataset\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.distance import geodesic\n",
        "from tqdm import tqdm\n",
        "from geopy.point import Point  # Import Point object\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Define airport codes and names\n",
        "airports = [\n",
        "    # North America\n",
        "    \"ATL\", \"LAX\", \"ORD\", \"DFW\", \"DEN\", \"JFK\", \"SFO\", \"LAS\", \"SEA\", \"CLT\",\n",
        "    \"MIA\", \"PHL\", \"EWR\", \"BOS\", \"PHX\", \"MCO\", \"IAH\", \"DAL\", \"FLL\", \"DTW\",\n",
        "    \"MSP\", \"IAD\", \"BWI\", \"SLC\", \"SAN\", \"MDW\", \"TPA\", \"PDX\", \"BNA\", \"STL\",\n",
        "    # Europe\n",
        "    \"LHR\", \"CDG\", \"AMS\", \"FRA\", \"MAD\", \"BCN\", \"FCO\", \"MUC\", \"DUB\", \"ZRH\",\n",
        "    # Asia\n",
        "    \"HND\", \"PVG\", \"ICN\", \"SIN\", \"KUL\", \"BKK\", \"DXB\", \"DEL\", \"HKG\", \"NRT\",\n",
        "    # Caribbean\n",
        "    \"CUN\", \"PUJ\", \"SJU\", \"MBJ\", \"NAS\",\n",
        "    # South America\n",
        "    \"GRU\", \"BOG\", \"EZE\", \"SCL\", \"LIM\",\n",
        "    # Australia & New Zealand\n",
        "    \"SYD\", \"MEL\", \"BNE\", \"PER\", \"AKL\"\n",
        "]\n",
        "\n",
        "airport_names = {\n",
        "    \"ATL\": \"Hartsfield-Jackson Atlanta International Airport\",\n",
        "    \"LAX\": \"Los Angeles International Airport\",\n",
        "    \"ORD\": \"Chicago O'Hare International Airport\",\n",
        "    \"DFW\": \"Dallas/Fort Worth International Airport\",\n",
        "    \"DEN\": \"Denver International Airport\",\n",
        "    \"JFK\": \"John F. Kennedy International Airport\",\n",
        "    \"SFO\": \"San Francisco International Airport\",\n",
        "    \"LAS\": \"Harry Reid International Airport\",\n",
        "    \"SEA\": \"Seattle-Tacoma International Airport\",\n",
        "    \"CLT\": \"Charlotte Douglas International Airport\",\n",
        "    \"MIA\": \"Miami International Airport\",\n",
        "    \"PHL\": \"Philadelphia International Airport\",\n",
        "    \"EWR\": \"Newark Liberty International Airport\",\n",
        "    \"BOS\": \"Logan International Airport\",\n",
        "    \"PHX\": \"Phoenix Sky Harbor International Airport\",\n",
        "    \"MCO\": \"Orlando International Airport\",\n",
        "    \"IAH\": \"George Bush Intercontinental Airport\",\n",
        "    \"DAL\": \"Dallas Love Field\",\n",
        "    \"FLL\": \"Fort Lauderdale-Hollywood International Airport\",\n",
        "    \"DTW\": \"Detroit Metropolitan Airport\",\n",
        "    \"MSP\": \"Minneapolis-Saint Paul International Airport\",\n",
        "    \"IAD\": \"Washington Dulles International Airport\",\n",
        "    \"BWI\": \"Baltimore/Washington International Airport\",\n",
        "    \"SLC\": \"Salt Lake City International Airport\",\n",
        "    \"SAN\": \"San Diego International Airport\",\n",
        "    \"MDW\": \"Midway International Airport\",\n",
        "    \"TPA\": \"Tampa International Airport\",\n",
        "    \"PDX\": \"Portland International Airport\",\n",
        "    \"BNA\": \"Nashville International Airport\",\n",
        "    \"STL\": \"St. Louis Lambert International Airport\",\n",
        "    # Europe\n",
        "    \"LHR\": \"London Heathrow Airport\",\n",
        "    \"CDG\": \"Charles de Gaulle Airport\",\n",
        "    \"AMS\": \"Amsterdam Airport Schiphol\",\n",
        "    \"FRA\": \"Frankfurt Airport\",\n",
        "    \"MAD\": \"Adolfo Suárez Madrid–Barajas Airport\",\n",
        "    \"BCN\": \"Barcelona–El Prat Airport\",\n",
        "    \"FCO\": \"Leonardo da Vinci–Fiumicino Airport\",\n",
        "    \"MUC\": \"Munich Airport\",\n",
        "    \"DUB\": \"Dublin Airport\",\n",
        "    \"ZRH\": \"Zurich Airport\",\n",
        "    # Asia\n",
        "    \"HND\": \"Haneda Airport\",\n",
        "    \"PVG\": \"Shanghai Pudong International Airport\",\n",
        "    \"ICN\": \"Incheon International Airport\",\n",
        "    \"SIN\": \"Singapore Changi Airport\",\n",
        "    \"KUL\": \"Kuala Lumpur International Airport\",\n",
        "    \"BKK\": \"Suvarnabhumi Airport\",\n",
        "    \"DXB\": \"Dubai International Airport\",\n",
        "    \"DEL\": \"Indira Gandhi International Airport\",\n",
        "    \"HKG\": \"Hong Kong International Airport\",\n",
        "    \"NRT\": \"Narita International Airport\",\n",
        "    # Caribbean\n",
        "    \"CUN\": \"Cancún International Airport\",\n",
        "    \"PUJ\": \"Punta Cana International Airport\",\n",
        "    \"SJU\": \"Luis Muñoz Marín International Airport\",\n",
        "    \"MBJ\": \"Sangster International Airport\",\n",
        "    \"NAS\": \"Lynden Pindling International Airport\",\n",
        "    # South America\n",
        "    \"GRU\": \"São Paulo–Guarulhos International Airport\",\n",
        "    \"BOG\": \"El Dorado International Airport\",\n",
        "    \"EZE\": \"Ministro Pistarini International Airport\",\n",
        "    \"SCL\": \"Arturo Merino Benítez International Airport\",\n",
        "    \"LIM\": \"Jorge Chávez International Airport\",\n",
        "    # Australia & New Zealand\n",
        "    \"SYD\": \"Sydney Airport\",\n",
        "    \"MEL\": \"Melbourne Airport\",\n",
        "    \"BNE\": \"Brisbane Airport\",\n",
        "    \"PER\": \"Perth Airport\",\n",
        "    \"AKL\": \"Auckland Airport\"\n",
        "}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "bU3ovgeD-NTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def generate_flight_plan(model, tokenizer, start_text, dataset, device='cuda'):\n",
        "    \"\"\"\n",
        "    Generates a flight plan (list of waypoint names) based on the given input text.\n",
        "\n",
        "    Args:\n",
        "        model: The trained Transformer model.\n",
        "        tokenizer: The tokenizer used for the model.\n",
        "        start_text: The input text describing the flight plan request.\n",
        "        dataset: The dataset used for training (to get num_waypoints).\n",
        "        device: The device to run the model on (default: 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        A list of waypoint names representing the generated flight plan.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    start_tokens = tokenizer(start_text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "\n",
        "    # Get num_waypoints from dataset based on partial match if exact match not found\n",
        "    num_waypoints = None\n",
        "    for example in dataset:\n",
        "        if start_text.strip() in example['input'].strip():\n",
        "            num_waypoints = example['label']\n",
        "            break\n",
        "\n",
        "    if num_waypoints is None:\n",
        "        num_waypoints = 5  # Default value if not found in dataset\n",
        "        print(\"Warning: Input text not found in dataset. Using default num_waypoints.\")\n",
        "\n",
        "    # Create a tensor for num_waypoints and move it to the device\n",
        "    num_waypoints_tensor = torch.tensor([num_waypoints], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Call the model with input_ids, attention_mask, and num_waypoints_tensor\n",
        "        # Before calling the model, ensure all input_ids are within the vocabulary range\n",
        "        start_tokens['input_ids'] = start_tokens['input_ids'].clip(0, tokenizer.vocab_size - 1)  # Clip to valid range\n",
        "\n",
        "        predicted_waypoints = model(start_tokens['input_ids'], start_tokens['attention_mask'], num_waypoints_tensor)\n",
        "\n",
        "    # Extract predicted airport codes directly, handling potential invalid codes\n",
        "    predicted_codes = []\n",
        "    for idx in predicted_waypoints[0].argmax(dim=-1):\n",
        "        predicted_token = tokenizer.convert_ids_to_tokens(idx.item())\n",
        "\n",
        "        # Check if the predicted token is a valid airport code, ignoring case\n",
        "        if predicted_token.upper() in airports:\n",
        "            predicted_codes.append(predicted_token.upper()) # Append in uppercase\n",
        "        else:\n",
        "            # Handle invalid airport codes (e.g., print a warning or use a default)\n",
        "            print(f\"Warning: Predicted token '{predicted_token}' is not a valid airport code. Skipping.\")\n",
        "            # You could append a default airport code here if needed\n",
        "\n",
        "    # Limit to num_waypoints\n",
        "    predicted_codes = predicted_codes[:num_waypoints]\n",
        "\n",
        "    # Resolve airport codes to names\n",
        "    waypoint_names = [airport_names.get(code, code) for code in predicted_codes]  # Use airport_names if available, otherwise use code\n",
        "\n",
        "    # Extract origin and destination from start_text using regex\n",
        "    match = re.search(r\"from\\s+([A-Z]{3})\\s+to\\s+([A-Z]{3})\", start_text, re.IGNORECASE)\n",
        "    if match:\n",
        "        origin, destination = match.group(1), match.group(2)\n",
        "        # Add origin and destination airport codes if not already present\n",
        "        if waypoint_names and waypoint_names[0] != origin:\n",
        "            waypoint_names.insert(0, origin)\n",
        "        if waypoint_names and waypoint_names[-1] != destination:\n",
        "            waypoint_names.append(destination)\n",
        "    else:\n",
        "        print(\"Warning: Could not extract origin and destination from input text.\")\n",
        "\n",
        "    return waypoint_names"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "lsKDmqdb1WUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#query='Calculate the waypoints from DXB to MSP. Departure: 2024-05-03, Aircraft: Boeing 777, Weather: Overcast'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "query= 'Calculate the waypoints from LHR to AMS. Departure: 2024-06-15, Aircraft: Piper PA-28, Weather: Partly Cloudy'\n",
        "print(f\"Query: {query}\")\n",
        "generated_plan = generate_flight_plan(model, tokenizer, start_text=query, dataset=flight_plan_dataset['train'], device=device) # Pass the dataset here\n",
        "print(\"\\nGenerated waypoint list:\")\n",
        "print(generated_plan)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QPkzNnVTFpqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "NGQToWe1cj5F",
        "PXqlKKDxcZYy"
      ],
      "authorship_tag": "ABX9TyMEFh+qifwYPL5hmd5FrmG4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}