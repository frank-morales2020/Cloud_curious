{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "cF04rAKeI3nl",
        "SMCjG1kUI_7o",
        "TSFr8YI2N4QU"
      ],
      "authorship_tag": "ABX9TyM1yjMHi5FBhvpnDSbd/sTB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "255fd5ec591043068bd314b63f988e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_afce317d6997450c8d41cab8ed4eeb68",
              "IPY_MODEL_04f038582fa14fb18814461ea4202a89",
              "IPY_MODEL_ab8482500fd644bea987a983006c8f45"
            ],
            "layout": "IPY_MODEL_8a09873f0cbb42f0a871017d6d078bb1"
          }
        },
        "afce317d6997450c8d41cab8ed4eeb68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f0753f2cc5441c59c80ecd65b034104",
            "placeholder": "​",
            "style": "IPY_MODEL_424e0e4c050844cf8cd1f42a8954f2ed",
            "value": "config.json: 100%"
          }
        },
        "04f038582fa14fb18814461ea4202a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f1e1a0f1e094106a48fc5e24d816a54",
            "max": 801,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33ec6c5b0e8a4609a7c297639b71f30f",
            "value": 801
          }
        },
        "ab8482500fd644bea987a983006c8f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b95287449eb143a5a2fa477500a535f7",
            "placeholder": "​",
            "style": "IPY_MODEL_0dfedf1c639349a7a938983523f79f83",
            "value": " 801/801 [00:00&lt;00:00, 104kB/s]"
          }
        },
        "8a09873f0cbb42f0a871017d6d078bb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f0753f2cc5441c59c80ecd65b034104": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "424e0e4c050844cf8cd1f42a8954f2ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f1e1a0f1e094106a48fc5e24d816a54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33ec6c5b0e8a4609a7c297639b71f30f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b95287449eb143a5a2fa477500a535f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dfedf1c639349a7a938983523f79f83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12fc658ba0304cc9be489811cec3344a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c93d6750e4ad44a19b4c9f06bd599e20",
              "IPY_MODEL_9aa22a7a51d24797ae29bc6f2b49084c",
              "IPY_MODEL_1f48e8e1d6ea4cc385a285ed801d471f"
            ],
            "layout": "IPY_MODEL_d11ce4aba65d474ba45f257c3b15eaa2"
          }
        },
        "c93d6750e4ad44a19b4c9f06bd599e20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c6301a4321b41fea4d7f6608dba1a2d",
            "placeholder": "​",
            "style": "IPY_MODEL_0556acdb8a974ec598b0f1a0f8424662",
            "value": "model.safetensors: 100%"
          }
        },
        "9aa22a7a51d24797ae29bc6f2b49084c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d88a61b046ef4c5d968f9cd66c3398a1",
            "max": 4138311608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2ff9adf5433448fa9b266578e75396b",
            "value": 4138311608
          }
        },
        "1f48e8e1d6ea4cc385a285ed801d471f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a86ec373556f41b69a22f57df14e131f",
            "placeholder": "​",
            "style": "IPY_MODEL_dda1666b42cc461d8618d1a47dcd394b",
            "value": " 4.14G/4.14G [00:12&lt;00:00, 277MB/s]"
          }
        },
        "d11ce4aba65d474ba45f257c3b15eaa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c6301a4321b41fea4d7f6608dba1a2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0556acdb8a974ec598b0f1a0f8424662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d88a61b046ef4c5d968f9cd66c3398a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2ff9adf5433448fa9b266578e75396b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a86ec373556f41b69a22f57df14e131f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dda1666b42cc461d8618d1a47dcd394b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1573d8e0c866488f8fb105c4fd3fff01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34cad9c0019044239785470b7180f528",
              "IPY_MODEL_cc34904877074ddcb5092c96be339062",
              "IPY_MODEL_6ee3d7f4f2ac49fc8b6b9ffbaa12587a"
            ],
            "layout": "IPY_MODEL_92e9f990b95647a8ac31271100ad6c1f"
          }
        },
        "34cad9c0019044239785470b7180f528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f96066c7c04d40748edeeef79e0b61ee",
            "placeholder": "​",
            "style": "IPY_MODEL_d27dfd66aa7f4084a66b9fa10c36c00f",
            "value": "video_preprocessor_config.json: "
          }
        },
        "cc34904877074ddcb5092c96be339062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcf61086a7e443f9997d8364d8c9eec2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6fe95fed7574f98825a9beed10c7e19",
            "value": 1
          }
        },
        "6ee3d7f4f2ac49fc8b6b9ffbaa12587a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7ac9a6b73a74a6aa49bc4eb93c06bdb",
            "placeholder": "​",
            "style": "IPY_MODEL_26c527c67abf4607a202045ac6cf108f",
            "value": " 1.30k/? [00:00&lt;00:00, 146kB/s]"
          }
        },
        "92e9f990b95647a8ac31271100ad6c1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f96066c7c04d40748edeeef79e0b61ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d27dfd66aa7f4084a66b9fa10c36c00f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcf61086a7e443f9997d8364d8c9eec2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e6fe95fed7574f98825a9beed10c7e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a7ac9a6b73a74a6aa49bc4eb93c06bdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26c527c67abf4607a202045ac6cf108f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/Copy_of_JEPA_AGI_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. World models that can predict and reason about real situations, not just text (LeCun’s Joint Embedding Predictive Architecture, 2022).\n",
        "2. Autonomous learning that discovers causal structure instead of memorizing patterns.\n",
        "3. Energy-based or modular systems that reason, plan, and act coherently within physical and ethical boundaries.\n",
        "4. Embodied sentience and salience — systems grounded in sensory experience, capable of focusing on what truly matters and aligning ethically with human values.\n",
        "5. Cognitive world models and evolutionary learning modules — hybrid systems that combine:\n",
        "• Common-sense reasoning about space, time, and agency,\n",
        "• Evolutionary and meta-learning algorithms that improve over generations of experience, and\n",
        "• Analog–digital integration layers that bridge symbolic reasoning with continuous perception."
      ],
      "metadata": {
        "id": "3RR2E1s6MeTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av -q"
      ],
      "metadata": {
        "id": "bS3m0V3eKYAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c66dd2c-cafb-40d3-b080-fb19e7f1784b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "3afxZb0bsnAy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c96dbad-bdbd-4f69-a8b8-2ebecc13467b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Nov 13 21:20:40 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   30C    P0             50W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d01iFpdOPpc7",
        "outputId": "e3c4fd9e-6be1-4ad9-a635-ef9eeb1ae4a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!df -h /content/gdrive"
      ],
      "metadata": {
        "id": "rsMJtRoGHXRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/castacks/TartanAviation.git\n",
        "%cd TartanAviation"
      ],
      "metadata": {
        "id": "PfNOnshJDGGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "id": "TiY9eZWiShA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install minio boto3 -q\n",
        "!apt-get install -y unzip ffmpeg"
      ],
      "metadata": {
        "id": "pUAqPkLnG_Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TartanAviation-vision"
      ],
      "metadata": {
        "id": "cF04rAKeI3nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/TartanAviation/vision"
      ],
      "metadata": {
        "id": "EaaxZ99nHcRE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/TartanAviation/vision/download.py --save_dir /content/gdrive/MyDrive/datasets/TartanAviation/vision --option Sample"
      ],
      "metadata": {
        "id": "fU6UkXYVIAfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/TartanAviation/vision/download.py --save_dir /content/gdrive/MyDrive/datasets/TartanAviation/vision --option Sample --extract_frames"
      ],
      "metadata": {
        "id": "HPIU8LXQIZfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/gdrive/MyDrive/datasets/TartanAviation/vision/\n",
        "!ls -lh /content/gdrive/MyDrive/datasets/TartanAviation/vision/1_2023-02-22-15-21-49/"
      ],
      "metadata": {
        "id": "8He8ZG5tIfGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TartanAviation-adsb"
      ],
      "metadata": {
        "id": "SMCjG1kUI_7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/gdrive/MyDrive/datasets/TartanAviation/adsb/kbtp/raw/2022/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krNpHswRI_Vm",
        "outputId": "2d572b13-aa9b-480c-cad0-1e9ef73a0d8b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 701M\n",
            "-rw------- 1 root root 700M Oct 20 11:25 2022.zip\n",
            "-rw------- 1 root root 1.4M Nov 13 20:47 classifier_head_trained_on_tartan_aviation_sample.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/adsb/kbtp/raw/2022"
      ],
      "metadata": {
        "id": "NGw276zOXXuo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gdrive/MyDrive/datasets/TartanAviation/adsb/kbtp/raw/2022\n",
        "!unzip 2022.zip -d /content/adsb/kbtp/raw/2022/"
      ],
      "metadata": {
        "id": "BxFcR4L1M97W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c2d53f6-5bce-42eb-b43e-636016176082"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/datasets/TartanAviation/adsb/kbtp/raw/2022\n",
            "Archive:  2022.zip\n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-01-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-02-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-03-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-04-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-05-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-06-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-07-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-08-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-09-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/01-10-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-10-22/2.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-11-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-12-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-13-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-14-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-15-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-16-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-17-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-18-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-19-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-20-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-21-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-22-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-23-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-24-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-25-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-26-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-27-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-28-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-29-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-30-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/01-31-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/02-01-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/02-02-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/02-05-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/02-06-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/02-23-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/02-24-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/02-25-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/02-25-22/2.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/02-25-22/3.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/02-25-22/4.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/02-26-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/02-27-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/02-28-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-01-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-02-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-02-22/2.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-03-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/03-04-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-05-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-06-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-07-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-08-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-09-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-10-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-11-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-12-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-13-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-14-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-15-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-16-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-17-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-18-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-19-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-20-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-21-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-22-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-23-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-24-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-25-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-26-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/03-27-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-13-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-14-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-14-22/2.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-15-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-16-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-17-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-18-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-18-22/2.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-19-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-20-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-20-22/2.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-20-22/3.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-20-22/4.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-20-22/5.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-20-22/6.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-21-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-22-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-23-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-24-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-25-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-25-22/2.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-25-22/3.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-26-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-27-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-28-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/04-29-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-30-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-30-22/2.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/04-30-22/3.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-01-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-02-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-03-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-04-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-04-22/2.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-05-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-06-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-06-22/2.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-06-22/3.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-06-22/4.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/05-06-22/5.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/05-06-22/6.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-06-22/7.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-07-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-08-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-09-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-10-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-11-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-12-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-13-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-14-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-15-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-16-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-17-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/05-18-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/05-19-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-04-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-05-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-06-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-07-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-08-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/06-09-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-10-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-11-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-12-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-13-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-14-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-15-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-16-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-17-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-18-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-19-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-20-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-21-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-22-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-23-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-24-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-25-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-26-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/06-27-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-28-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-29-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/06-30-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/07-01-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-02-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-03-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-04-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-05-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/07-06-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-16-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-17-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/07-18-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-19-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-20-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/07-21-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-22-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/07-23-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-23-22/2.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/07-24-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-25-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-26-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-27-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-28-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-29-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/07-30-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/07-31-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-02-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-18-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-19-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-20-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-21-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-22-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-23-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-24-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-25-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/08-26-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-27-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-28-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-29-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-30-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/08-31-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-01-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-02-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-03-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-04-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-05-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-06-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-07-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-08-22/1.csv  \n",
            " extracting: /content/adsb/kbtp/raw/2022/09-09-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-10-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-11-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-12-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-15-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-17-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-18-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-19-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-20-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-21-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-22-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-24-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-25-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-26-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-27-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-28-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-29-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/09-30-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-01-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-02-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-03-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-04-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-05-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-06-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-07-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-08-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-09-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-10-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-11-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-12-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-13-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-14-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-15-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-16-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-17-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-18-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-19-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-20-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-21-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-22-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-23-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-24-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-25-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-26-22/1.csv  \n",
            "  inflating: /content/adsb/kbtp/raw/2022/10-27-22/1.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please install OpenAI SDK first: `pip3 install openai`\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "api_key=userdata.get(\"DEEPSEEK_API_KEY\")\n",
        "\n",
        "client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")"
      ],
      "metadata": {
        "id": "XFuwjFqfZmf9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CELL1"
      ],
      "metadata": {
        "id": "vQBif4KYNftf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Conceptual Modifications - Aviation Data Definitions\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import av\n",
        "import json\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import AutoVideoProcessor, AutoModel\n",
        "from tqdm.auto import tqdm\n",
        "import logging\n",
        "import datetime\n",
        "import pytz\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s')\n",
        "\n",
        "\n",
        "\n",
        "class AgentConfig:\n",
        "    LLM_MODEL_NAME: str = \"deepseek-reasoner\"\n",
        "    CLASS_LABELS = [\n",
        "        \"airplane landing\",\n",
        "        \"airplane takeoff\",\n",
        "        \"airport ground operations\",\n",
        "        \"in-flight cruise\",\n",
        "        \"emergency landing\",\n",
        "        \"pre-flight check/maintenance\",\n",
        "        \"en-route cruise\",\n",
        "        \"climb phase\",\n",
        "        \"descent phase\",\n",
        "        \"holding pattern\"\n",
        "    ]\n",
        "\n",
        "# Define num_classes globally\n",
        "num_classes = len(AgentConfig.CLASS_LABELS)\n",
        "\n",
        "# --- FIX: CLASSIFIER_SAVE_PATH moved to global scope ---\n",
        "CLASSIFIER_SAVE_PATH = \"classifier_head_trained_on_tartan_aviation_sample.pth\"\n",
        "\n",
        "AIRPORTS = {\n",
        "    \"CYUL\": {\"name\": \"Montreal-Trudeau International\", \"lat\": 45.4706, \"lon\": -73.7408, \"elevation_ft\": 118},\n",
        "    \"LFPG\": {\"name\": \"Paris-Charles de Gaulle\", \"lat\": 49.0097, \"lon\": 2.5479, \"elevation_ft\": 392},\n",
        "}\n",
        "\n",
        "AIRCRAFT_PERFORMANCE = {\n",
        "    \"Boeing777_300ER\": {\n",
        "        \"cruise_speed_knots\": 490,\n",
        "        \"fuel_burn_kg_per_hour\": 7000,\n",
        "        \"max_range_nm\": 7900,\n",
        "        \"climb_rate_fpm\": 2500,\n",
        "        \"descent_rate_fpm\": 2000,\n",
        "        \"typical_cruise_altitude_ft\": 37000,\n",
        "        \"fuel_capacity_kg\": 145000\n",
        "    }\n",
        "}\n",
        "\n",
        "hf_repo = \"facebook/vjepa2-vitg-fpc64-256\"\n",
        "EXTRACTED_FEATURES_DIR = \"/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/\"\n",
        "\n",
        "TOTAL_FLATTENED_VJEPA_DIM = 2048 * 1408\n",
        "\n",
        "CONCEPTUAL_PLDM_LATENT_DIM = 1024\n",
        "\n",
        "latent_dim_pldm = CONCEPTUAL_PLDM_LATENT_DIM\n",
        "action_dim = 8\n",
        "\n",
        "def load_and_process_video(video_path, processor_instance, model_instance, device_instance, num_frames_to_sample=16):\n",
        "    \"\"\"\n",
        "    Loads a video, samples frames, and extracts V-JEPA features.\n",
        "    Returns extracted features (torch.Tensor, shape like [1, 2048, 1408]) and the frames.\n",
        "    Does NOT flatten the V-JEPA output here, keeping it as model's raw output.\n",
        "    \"\"\"\n",
        "    frames = []\n",
        "    if not os.path.exists(video_path):\n",
        "        logging.error(f\"ERROR: Video file '{video_path}' not found.\")\n",
        "        return None, None\n",
        "    try:\n",
        "        container = av.open(video_path)\n",
        "        total_frames_in_video = container.streams.video[0].frames\n",
        "        sampling_interval = max(1, total_frames_in_video // num_frames_to_sample)\n",
        "        logging.info(f\"Total frames in video: {total_frames_in_video}\")\n",
        "        logging.info(f\"Sampling interval: {sampling_interval} frames\")\n",
        "\n",
        "        for i, frame in enumerate(container.decode(video=0)):\n",
        "            if len(frames) >= num_frames_to_sample:\n",
        "                break\n",
        "            if i % sampling_interval == 0:\n",
        "                img = frame.to_rgb().to_ndarray()\n",
        "                frames.append(img)\n",
        "\n",
        "        if not frames:\n",
        "            logging.error(f\"ERROR: No frames could be loaded from '{video_path}'.\")\n",
        "            return None, None\n",
        "        elif len(frames) < num_frames_to_sample:\n",
        "            logging.warning(f\"WARNING: Only {len(frames)} frames loaded. Requested: {num_frames_to_sample}.\")\n",
        "\n",
        "        inputs = processor_instance(videos=list(frames), return_tensors=\"pt\")\n",
        "        inputs = {k: v.to(device_instance) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = model_instance(**inputs).last_hidden_state\n",
        "\n",
        "        logging.info(f\"Successfully extracted V-JEPA features with raw shape: {features.shape}\")\n",
        "        return features, frames\n",
        "\n",
        "    except av.FFmpegError as e:\n",
        "        logging.error(f\"Error loading video with PyAV: {e}\")\n",
        "        logging.error(\"This might indicate an issue with the video file itself or PyAV installation.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An unexpected error occurred: {e}\")\n",
        "        logging.error(\"Ensure 'av' library is installed (`pip install av`) and video file is not corrupt.\")\n",
        "        return None, None\n",
        "\n",
        "class ClassifierHead(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.dropout(self.relu(self.fc1(x))))\n",
        "\n",
        "class LatentDynamicsPredictor(torch.nn.Module):\n",
        "    def __init__(self, latent_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            nn.Linear(latent_dim + action_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, latent_state, action):\n",
        "        combined_input = torch.cat([latent_state, action], dim=-1)\n",
        "        predicted_next_latent_state = self.layers(combined_input)\n",
        "        return predicted_next_latent_state\n",
        "\n",
        "class LatentProjector_old(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.projector = nn.Linear(input_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.projector(x))\n",
        "\n",
        "\n",
        "class LatentProjector(nn.Module):\n",
        "    def __init__(self, input_dim=4, output_dim=1024):\n",
        "        super().__init__()\n",
        "        self.projector = nn.Linear(input_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n--- Instantiating Models and Optimizers ---\")\n",
        "model = AutoModel.from_pretrained(hf_repo)\n",
        "processor = AutoVideoProcessor.from_pretrained(hf_repo)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "latent_projector = LatentProjector(TOTAL_FLATTENED_VJEPA_DIM, CONCEPTUAL_PLDM_LATENT_DIM)\n",
        "latent_projector.to(device)\n",
        "\n",
        "predictor = LatentDynamicsPredictor(latent_dim_pldm, action_dim)\n",
        "predictor.to(device)\n",
        "optimizer_pldm = torch.optim.Adam(list(predictor.parameters()) + list(latent_projector.parameters()), lr=0.001)\n",
        "\n",
        "classifier = ClassifierHead(input_dim=1408, num_classes=num_classes)\n",
        "classifier.to(device)\n",
        "\n",
        "print(f\"Models instantiated and moved to {device}.\")\n",
        "print(\"\\nCell 1 setup complete for conceptual flight planning.\")"
      ],
      "metadata": {
        "id": "qHCiY0vlNkDz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200,
          "referenced_widgets": [
            "255fd5ec591043068bd314b63f988e5a",
            "afce317d6997450c8d41cab8ed4eeb68",
            "04f038582fa14fb18814461ea4202a89",
            "ab8482500fd644bea987a983006c8f45",
            "8a09873f0cbb42f0a871017d6d078bb1",
            "1f0753f2cc5441c59c80ecd65b034104",
            "424e0e4c050844cf8cd1f42a8954f2ed",
            "9f1e1a0f1e094106a48fc5e24d816a54",
            "33ec6c5b0e8a4609a7c297639b71f30f",
            "b95287449eb143a5a2fa477500a535f7",
            "0dfedf1c639349a7a938983523f79f83",
            "12fc658ba0304cc9be489811cec3344a",
            "c93d6750e4ad44a19b4c9f06bd599e20",
            "9aa22a7a51d24797ae29bc6f2b49084c",
            "1f48e8e1d6ea4cc385a285ed801d471f",
            "d11ce4aba65d474ba45f257c3b15eaa2",
            "0c6301a4321b41fea4d7f6608dba1a2d",
            "0556acdb8a974ec598b0f1a0f8424662",
            "d88a61b046ef4c5d968f9cd66c3398a1",
            "f2ff9adf5433448fa9b266578e75396b",
            "a86ec373556f41b69a22f57df14e131f",
            "dda1666b42cc461d8618d1a47dcd394b",
            "1573d8e0c866488f8fb105c4fd3fff01",
            "34cad9c0019044239785470b7180f528",
            "cc34904877074ddcb5092c96be339062",
            "6ee3d7f4f2ac49fc8b6b9ffbaa12587a",
            "92e9f990b95647a8ac31271100ad6c1f",
            "f96066c7c04d40748edeeef79e0b61ee",
            "d27dfd66aa7f4084a66b9fa10c36c00f",
            "dcf61086a7e443f9997d8364d8c9eec2",
            "e6fe95fed7574f98825a9beed10c7e19",
            "a7ac9a6b73a74a6aa49bc4eb93c06bdb",
            "26c527c67abf4607a202045ac6cf108f"
          ]
        },
        "outputId": "c9cb5d85-28ec-4236-e654-cce8722da9ce"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Instantiating Models and Optimizers ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "255fd5ec591043068bd314b63f988e5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12fc658ba0304cc9be489811cec3344a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "video_preprocessor_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1573d8e0c866488f8fb105c4fd3fff01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models instantiated and moved to cuda.\n",
            "\n",
            "Cell 1 setup complete for conceptual flight planning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CELL2"
      ],
      "metadata": {
        "id": "TSFr8YI2N4QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cell 2: Core Execution Feature Extraction, Classifier Training & Inference, LLM Interaction, and PLDM Training/Planning\n",
        "# This cell assumes Cell 1 has been successfully executed in the current session.\n",
        "# All objects (model, processor, classifier, predictor, device, optimizer_pldm)\n",
        "# and all function definitions (load_and_process_video, ClassifierHead, LatentDynamicsPredictor)\n",
        "# are expected to be available from Cell 1's execution.\n",
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import json\n",
        "from google.colab import drive\n",
        "from tqdm.auto import tqdm\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import datetime\n",
        "import pytz\n",
        "\n",
        "#Mounting Google Drive\n",
        "print(\"\\n--- Cell 2: Mounting Google Drive for dataset access ---\")\n",
        "drive.mount('/content/gdrive')\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "print(f\"Checking for extracted features directory: {EXTRACTED_FEATURES_DIR}\")\n",
        "if not os.path.exists(EXTRACTED_FEATURES_DIR):\n",
        "    logging.error(f\"ERROR: Extracted features directory '{EXTRACTED_FEATURES_DIR}' not found. Please create it and upload V-JEPA features.\")\n",
        "    exit()\n",
        "else:\n",
        "    print(f\"Extracted features directory found at {EXTRACTED_FEATURES_DIR}\")\n",
        "\n",
        "# Part 1: Load and process airplane-landing.mp4 for initial observation\n",
        "print(f\"\\n--- Cell 2: Part 1 - Loading actual video '/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4' for feature extraction ---\")\n",
        "flight_video_path = '/content/gdrive/MyDrive/datasets/TartanAviation_VJEPA_Features/airplane-landing.mp4'\n",
        "# Use the defined load_and_process_video helper function. It now returns features and frames.\n",
        "video_features_for_inference_raw, frames_for_pldm_planning = load_and_process_video(flight_video_path, processor, model, device_instance=device)\n",
        "\n",
        "# -- CRITICAL: Process raw V-JEPA features to match ClassifierHead's expected input_dim --\n",
        "if video_features_for_inference_raw is not None:\n",
        "    # V-JEPA output shape is typically [1, 2048, 1408] (Batch, Channels, Height * Width if 1D)\n",
        "    # Your old code pooled it as squeeze(0).mean(dim=0).unsqueeze(0), which resulted in [1, 1408] for classifier.\n",
        "    # So, extracted_embedding_dim should be 1408 for the classifier.\n",
        "    pooled_features_for_classifier = video_features_for_inference_raw.squeeze(0).mean(dim=0).unsqueeze(0)\n",
        "    extracted_embedding_dim_for_classifier = pooled_features_for_classifier.shape[-1]\n",
        "    logging.info(f\"Dynamically determined extracted_embedding_dim for ClassifierHead: {extracted_embedding_dim_for_classifier}\")\n",
        "else:\n",
        "    pooled_features_for_classifier = None\n",
        "    extracted_embedding_dim_for_classifier = -1\n",
        "    logging.error(\"Failed to extract video features for classifier. Exiting Cell 2.\")\n",
        "    exit()\n",
        "\n",
        "# Part 2: Classifier Training\n",
        "print(f\"\\n--- Cell 2: Part 2 - Starting Classifier Training ---\")\n",
        "print(f\"Attempting to load real V-JEPA features for classifier training or generate synthetic data.\")\n",
        "print(f\"Using device for classifier training: {device}\")\n",
        "\n",
        "try:\n",
        "    # Re-initialize classifier with the correct, dynamically determined input_dim\n",
        "    classifier = ClassifierHead(input_dim=extracted_embedding_dim_for_classifier, num_classes=num_classes)\n",
        "    classifier.to(device)\n",
        "\n",
        "    train_features_list = []\n",
        "    train_labels_list = []\n",
        "\n",
        "    map_file_path = os.path.join(EXTRACTED_FEATURES_DIR, \"feature_label_map.json\")\n",
        "    if not os.path.exists(map_file_path):\n",
        "        logging.warning(f\"Feature-label map file '{map_file_path}' not found. Generating synthetic data.\")\n",
        "        feature_label_map = {}\n",
        "    else:\n",
        "        with open(map_file_path, 'r') as f:\n",
        "            feature_label_map = json.load(f)\n",
        "\n",
        "    if not feature_label_map:\n",
        "        logging.warning(f\"Feature-label map at {map_file_path} is empty. Generating synthetic data.\")\n",
        "        num_training_samples = 2_000_000\n",
        "        # Synthetic data generation uses the dynamically determined input_dim\n",
        "        train_features = torch.rand(num_training_samples, extracted_embedding_dim_for_classifier)\n",
        "        train_labels = torch.randint(0, num_classes, (num_training_samples,))\n",
        "        train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=32, shuffle=True)\n",
        "        val_loader = None\n",
        "        print(f\"Loaded {num_training_samples} SYNTHETIC features for training.\")\n",
        "    else:\n",
        "        for item in tqdm(feature_label_map, desc=\"Loading real V-JEPA features\"):\n",
        "            feature_path = item['feature_path']\n",
        "            label_idx = item['label_idx']\n",
        "            try:\n",
        "                if not os.path.isabs(feature_path):\n",
        "                    feature_path = os.path.join(EXTRACTED_FEATURES_DIR, feature_path)\n",
        "\n",
        "                if not os.path.exists(feature_path):\n",
        "                    logging.warning(f\"Feature file not found at {feature_path}. Skipping.\")\n",
        "                    continue\n",
        "\n",
        "                feature = torch.load(feature_path, map_location=device)\n",
        "\n",
        "                # Match your working code's pooling/squashing logic to get [1408] dim\n",
        "                if feature.ndim == 3:\n",
        "                    feature = feature.squeeze(0).mean(dim=0)\n",
        "                elif feature.ndim == 2:\n",
        "                    if feature.shape[0] == 1 and feature.shape[1] == 1408:\n",
        "                        feature = feature.squeeze(0)\n",
        "                    elif feature.shape[1] == 1408:\n",
        "                        feature = feature.mean(dim=0)\n",
        "                    else:\n",
        "                        feature = feature.flatten()\n",
        "                elif feature.ndim == 1:\n",
        "                    pass\n",
        "                else:\n",
        "                    logging.warning(f\"Skipping malformed feature from {feature_path}. Unexpected dimensions: {feature.ndim}\")\n",
        "                    continue\n",
        "\n",
        "                # Final check after processing. Should be 1D with 1408 elements.\n",
        "                if feature.shape[0] != extracted_embedding_dim_for_classifier:\n",
        "                    logging.warning(f\"Skipping feature at {feature_path}. Dimension mismatch: expected {extracted_embedding_dim_for_classifier}, got {feature.shape[0]}.\")\n",
        "                    continue\n",
        "\n",
        "                train_features_list.append(feature)\n",
        "                train_labels_list.append(label_idx)\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error loading feature from {feature_path}: {e}. Skipping.\")\n",
        "\n",
        "        if train_features_list:\n",
        "            train_features = torch.stack(train_features_list).to(device)\n",
        "            train_labels = torch.tensor(train_labels_list).to(device)\n",
        "            num_training_samples = len(train_features)\n",
        "            print(f\"Loaded {num_training_samples} REAL V-JEPA features for training.\")\n",
        "\n",
        "            if num_training_samples < 2:\n",
        "                print(\"WARNING: Only 1 real V-JEPA feature loaded. Training may be unstable. Consider more data.\")\n",
        "                train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=1, shuffle=True)\n",
        "                val_loader = None\n",
        "            else:\n",
        "                dataset_size = len(train_features)\n",
        "                train_size = int(0.8 * dataset_size)\n",
        "                val_size = dataset_size - train_size\n",
        "\n",
        "                if val_size == 0 and train_size > 0:\n",
        "                    train_size = dataset_size\n",
        "                    train_dataset_real = TensorDataset(train_features, train_labels)\n",
        "                    val_dataset_real = None\n",
        "                else:\n",
        "                    train_dataset_real, val_dataset_real = torch.utils.data.random_split(\n",
        "                        TensorDataset(train_features, train_labels), [train_size, val_size]\n",
        "                    )\n",
        "                train_loader = DataLoader(train_dataset_real, batch_size=32, shuffle=True)\n",
        "                val_loader = DataLoader(val_dataset_real, batch_size=32, shuffle=False) if val_dataset_real else None\n",
        "                print(f\"Training on {len(train_dataset_real)} samples, Validation on {len(val_dataset_real) if val_dataset_real else 0} samples.\")\n",
        "        else:\n",
        "            logging.error(\"No real V-JEPA features could be loaded from map file. Generating synthetic data as fallback.\")\n",
        "            num_training_samples = 2_000_000\n",
        "            train_features = torch.rand(num_training_samples, extracted_embedding_dim_for_classifier)\n",
        "            train_labels = torch.randint(0, num_classes, (num_training_samples,))\n",
        "            train_loader = DataLoader(TensorDataset(train_features, train_labels), batch_size=32, shuffle=True)\n",
        "            val_loader = None\n",
        "            print(f\"Loaded {num_training_samples} SYNTHETIC features for training as fallback.\")\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer_classifier = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "\n",
        "    num_epochs = 20\n",
        "    for epoch in range(num_epochs):\n",
        "        classifier.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer_classifier.zero_grad()\n",
        "            outputs = classifier(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer_classifier.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        if val_loader and len(val_loader.dataset) > 0:\n",
        "            classifier.eval()\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = classifier(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    val_loss += loss.item()\n",
        "            val_loss /= len(val_loader.dataset)\n",
        "            logging.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "        else:\n",
        "            logging.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")\n",
        "            print(\"No validation data available or validation dataset is empty.\")\n",
        "\n",
        "\n",
        "    print(\"\\n--- Classifier Training Complete ---\")\n",
        "    torch.save(classifier.state_dict(), CLASSIFIER_SAVE_PATH)\n",
        "    print(f\"Classifier saved to: {CLASSIFIER_SAVE_PATH}\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error during classifier training: {e}\")\n",
        "\n",
        "\n",
        "print(\"Cell 2 execution complete.\")"
      ],
      "metadata": {
        "id": "SLxI7dIUN73d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import OpenAI client for DeepSeek API\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# --- DeepSeek API Setup ---\n",
        "api_key = userdata.get(\"DEEPSEEK_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    print(\"Error: DEEPSEEK_API_KEY not found in userdata.\")\n",
        "    print(\"Please set your DeepSeek API key in Colab secrets.\")\n",
        "    exit()\n",
        "\n",
        "client = OpenAI(api_key=api_key, base_url=\"https://api.deepseek.com\")\n",
        "MODEL = \"deepseek-reasoner\""
      ],
      "metadata": {
        "id": "omhJ0G26T2Vi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Part 3: Classification Inference and DEEPSEEK LLM Interaction\n",
        "print(\"\\n--- Cell 2: Part 3 - Starting V-JEPA Feature-Driven Classification Inference and DEEPSEEK LLM Interaction ---\")\n",
        "if pooled_features_for_classifier is None:\n",
        "    logging.error(\"ERROR: Cannot perform classifier inference as 'pooled_features_for_classifier' is None.\")\n",
        "else:\n",
        "    try:\n",
        "        pooled_features_for_inference_on_device = pooled_features_for_classifier.to(device)\n",
        "\n",
        "        classifier.load_state_dict(torch.load(CLASSIFIER_SAVE_PATH, map_location=device))\n",
        "        logging.info(f\"Classifier weights loaded from: {CLASSIFIER_SAVE_PATH}\")\n",
        "\n",
        "        classifier.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = classifier(pooled_features_for_inference_on_device)\n",
        "            probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "        predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
        "        predicted_confidence = probabilities[0, predicted_class_idx].item()\n",
        "\n",
        "\n",
        "        # --- FIX: Use AgentConfig.CLASS_LABELS ---\n",
        "        predicted_label = AgentConfig.CLASS_LABELS[predicted_class_idx]\n",
        "\n",
        "        llm_input_description = \"\"\n",
        "        if predicted_label == \"airplane landing\":\n",
        "            llm_input_description = \"The visual system detected an airplane landing.\"\n",
        "        elif predicted_label == \"airplane takeoff\":\n",
        "            llm_input_description = \"The visual system detected an airplane takeoff.\"\n",
        "        elif predicted_label == \"airport ground operations\":\n",
        "            llm_input_description = \"The visual system detected airport ground operations.\"\n",
        "        elif predicted_label == \"in-flight cruise\":\n",
        "            llm_input_description = \"The visual system detected an airplane in-flight cruise.\"\n",
        "        elif predicted_label == \"emergency landing\":\n",
        "            llm_input_description = \"The visual system detected a possible emergency landing.\"\n",
        "        elif predicted_label == \"pre-flight check/maintenance\":\n",
        "            llm_input_description = \"The visual system detected pre-flight check or maintenance activity.\"\n",
        "        else:\n",
        "            llm_input_description = \"The visual system detected an unrecognised flight activity.\"\n",
        "\n",
        "        llm_input_description += f\" (Confidence: {predicted_confidence:.2f})\"\n",
        "\n",
        "        print(f\"\\n--- AI Agent's Understanding from Classifier ---\")\n",
        "        print(f\"**Primary Classification (Predicted by AI):** '{predicted_label}' {llm_input_description.split('Confidence:')[1].strip()}\")\n",
        "        print(f\"**Description for LLM:** {llm_input_description}\")\n",
        "        print(f\"Note: This classification's accuracy depends heavily on the quality and size of the real dataset used for classifier training.\")\n",
        "\n",
        "        print(\"\\n--- Engaging DEEPSEEK LLM for Further Reasoning ---\")\n",
        "        try:\n",
        "            llm_model = AgentConfig.LLM_MODEL_NAME\n",
        "\n",
        "            prompt_for_deepseek = f\"\"\"\n",
        "                  You are an AI assistant for flight planning operations.\n",
        "                  Current visual observation: {llm_input_description}\n",
        "                  Current time (EST): {datetime.datetime.now(pytz.timezone('EST')).strftime('%Y-%m-%d %H:%M:%S EST')}\n",
        "\n",
        "                  Based on this visual observation, provide a concise operational assessment relevant to flight planning.\n",
        "                  If the observation seems random or uncertain, state that. Do not add any conversational filler.\n",
        "                  \"\"\"\n",
        "\n",
        "\n",
        "            deepseek_response = client.chat.completions.create(\n",
        "            model=llm_model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "                {\"role\": \"user\", \"content\": prompt_for_deepseek},\n",
        "            ],\n",
        "            stream=False\n",
        "            )\n",
        "\n",
        "\n",
        "            print(\"\\n--- DEEPSEEK LLM Response ---\")\n",
        "            if deepseek_response.choices and deepseek_response.choices[0].message.content:\n",
        "                print(deepseek_response.choices[0].message.content)\n",
        "                print(\"--- DEEPSEEK LLM Response - END ---\")\n",
        "                print('\\n')\n",
        "            else:\n",
        "\n",
        "                print(\"DEEPSEEK LLM did not provide a text response or cannot provide one.\")\n",
        "                # Check if there's an error attribute before trying to print it\n",
        "                if hasattr(deepseek_response, 'error') and deepseek_response.error:\n",
        "                     print(f\"LLM Error: {deepseek_response.error}\")\n",
        "                # Check if there's a prompt_feedback attribute before trying to print it\n",
        "                if hasattr(deepseek_response, 'prompt_feedback') and deepseek_response.prompt_feedback:\n",
        "                    print(f\"Prompt Feedback: {deepseek_response.prompt_feedback}\")\n",
        "\n",
        "\n",
        "        except Exception as llm_e:\n",
        "            logging.error(f\"Error interacting with DEEPSEEK LLM: {llm_e}\")\n",
        "            logging.error(\"Ensure your DEEPSEEK_API_KEY is correctly set in Colab Secrets.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during classification inference or overall Cell 2 execution: {e}\")\n",
        "\n",
        "print(f\"The V-JEPA features (shape: {pooled_features_for_classifier.shape}) are the core input that a trained classifier would learn from.\")\n",
        "print(f\"Current time in EST: {datetime.datetime.now(pytz.timezone('EST')).strftime('%Y-%m-%d %H:%M:%S EST')}\")\n",
        "\n",
        "print(\"\\nCell 2 execution complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLIE_3nWY0Px",
        "outputId": "b2ca78c9-af0d-4480-d6de-4d60e19805a3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Cell 2: Part 3 - Starting V-JEPA Feature-Driven Classification Inference and DEEPSEEK LLM Interaction ---\n",
            "\n",
            "--- AI Agent's Understanding from Classifier ---\n",
            "**Primary Classification (Predicted by AI):** 'airplane landing' 1.00)\n",
            "**Description for LLM:** The visual system detected an airplane landing. (Confidence: 1.00)\n",
            "Note: This classification's accuracy depends heavily on the quality and size of the real dataset used for classifier training.\n",
            "\n",
            "--- Engaging DEEPSEEK LLM for Further Reasoning ---\n",
            "\n",
            "--- DEEPSEEK LLM Response ---\n",
            "Active landing confirms runway occupancy; assess for potential impacts on arrival sequencing and departure schedules.\n",
            "--- DEEPSEEK LLM Response - END ---\n",
            "\n",
            "\n",
            "The V-JEPA features (shape: torch.Size([1, 1408])) are the core input that a trained classifier would learn from.\n",
            "Current time in EST: 2025-11-13 16:23:49 EST\n",
            "\n",
            "Cell 2 execution complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CELL3-NEW"
      ],
      "metadata": {
        "id": "sdaDFBMnR9ME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "LEJEPA, or the **L**ean **J**oint-**E**mbedding **P**redictive **A**rchitecture, is fundamentally defined by a **new loss calculation** and a set of architectural principles.\n",
        "\n",
        "Here is a breakdown of why this loss is significant:\n",
        "\n",
        "* ### **1. The Core Loss Components**\n",
        "    The total LEJEPA loss is a combination of two essential parts:\n",
        "    * **JEPA Predictive Loss:** This part is common to Joint-Embedding methods (like the one used in Cell 3). It encourages the predictor network to correctly predict the latent representation of a target view ($\\mathbf{z}_{\\text{target}}$) based on an input view ($\\mathbf{z}_{\\text{input}}$).\n",
        "    * **$\\mathbf{SIGReg}$ Loss:** This is the novel component: the **S**ketched **I**sotropic **G**aussian **Reg**ularization loss.\n",
        "\n",
        "* ### **2. The Function of $\\mathbf{SIGReg}$**\n",
        "    The $\\mathbf{SIGReg}$ loss is what makes LEJEPA a **mathematically rigorous** architecture. It serves to:\n",
        "    * **Prevent Collapse:** It guarantees that the learned latent representations do not collapse into a trivial, single point, which is a major challenge in self-supervised learning.\n",
        "    * **Enforce Structure:** It enforces the latent distribution to be the statistically optimal **Isotropic Gaussian distribution** ($N(0, I)$).\n",
        "\n",
        "In essence, LEJEPA doesn't invent a new neural network architecture but provides a **provably optimal loss function** that simplifies training and ensures the resulting feature space is stable and high-quality for downstream tasks (like the causal planning in Cell 4)."
      ],
      "metadata": {
        "id": "TcdIUI8ITvGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, I can absolutely compare LEJEPA's new loss calculation against the older, heuristic-driven methods it aims to replace.\n",
        "\n",
        "---\n",
        "\n",
        "## ⚖️ LEJEPA Loss vs. Older Self-Supervised Losses\n",
        "\n",
        "The primary significance of LEJEPA (Lean Joint-Embedding Predictive Architecture) is its ability to eliminate complexity and instability by introducing a mathematically grounded regularization term.\n",
        "\n",
        "| Feature | LEJEPA (New Loss) | Older Self-Supervised Methods (e.g., MoCo, SimSiam, DINO) |\n",
        "| :--- | :--- | :--- |\n",
        "| **Core Objective** | $\\text{Prediction Loss} + \\mathbf{SIGReg}$ (Isotropic Gaussian Regularization) | $\\text{Prediction/Contrastive Loss}$ (often with hard coding to prevent collapse) |\n",
        "| **Collapse Prevention** | **Guaranteed** by the **SIGReg** loss, which explicitly enforces the optimal $N(0, I)$ distribution on the latent embeddings. | Relied on complex **heuristics** (tricks) to prevent collapse, such as negative sampling, stop-gradients, or momentum encoders. |\n",
        "| **Training Stability** | **High stability** and robustness across different hyperparameters (like batch size and $\\lambda$). | **Low stability**, often requiring fine-tuning of momentum coefficients, learning rate schedules, and other hand-tuned parameters. |\n",
        "| **Architecture** | Architecturally simple: just an **Encoder** and a **Predictor** network. | Often required complex dual-network architectures (e.g., **Teacher/Student** or **Momentum Encoders**) to function. |\n",
        "| **Complexity** | **Single, simple trade-off hyperparameter** ($\\lambda$). | Many ad-hoc hyperparameters required for each stabilization trick. |\n",
        "\n",
        "***\n",
        "\n",
        "## 💡 The Role of SIGReg\n",
        "\n",
        "The innovation is entirely within the $\\mathbf{SIGReg}$ (Sketched Isotropic Gaussian Regularization) component.\n",
        "\n",
        "### The Problem in Old Methods: Collapse\n",
        "Older methods primarily rely on making two different views of the same image/video (e.g., a zoomed crop and a colored crop) map to similar latent representations. This setup is inherently unstable because the simplest solution is for the encoder to map *all* inputs to the exact same vector (the **trivial/collapsing solution**).\n",
        "\n",
        "### The Solution in LEJEPA: Optimal Constraint\n",
        "LEJEPA solves this by providing the mathematical proof that the optimal distribution for the latent codes is the **Isotropic Gaussian** ($N(0, I)$).\n",
        "\n",
        "1.  **Old Methods' Solution:** They added hacks like momentum encoders or stop-gradients to prevent the model from finding the collapsing solution, but these methods lacked theoretical justification and required constant tuning.\n",
        "2.  **LEJEPA's Solution:** It adds the $\\mathbf{SIGReg}$ loss, which **explicitly penalizes** the encoder if its output vectors deviate from the $N(0, I)$ distribution. This is a mathematically justified constraint that removes the need for all the clumsy heuristics, leading to a leaner and more robust framework."
      ],
      "metadata": {
        "id": "Zi6-USxdUJ1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import logging\n",
        "import psutil\n",
        "import gc\n",
        "\n",
        "# Set logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
        "\n",
        "# Constants\n",
        "STATE_DIM = 4      # Lat, Lon, Altitude, Speed\n",
        "LATENT_DIM = 16\n",
        "ACTION_DIM = 8\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- LEJEPA-inspired Isotropic Gaussian Regularization (SIGReg Proxy) ---\n",
        "class IsotropicGaussianLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    A simplified proxy for LEJEPA's SIGReg (Sketched Isotropic Gaussian Regularization).\n",
        "    It penalizes deviation of the batch statistics from the optimal N(0, I) distribution.\n",
        "    The goal is to prevent representational collapse by enforcing zero mean and unit variance.\n",
        "    \"\"\"\n",
        "    def __init__(self, lambda_reg=5.0): # lambda_reg is the trade-off hyperparameter\n",
        "        super().__init__()\n",
        "        self.lambda_reg = lambda_reg\n",
        "\n",
        "    def forward(self, z):\n",
        "        # 1. Zero-mean penalty (L2 norm of the batch mean)\n",
        "        batch_mean = z.mean(dim=0)\n",
        "        mean_penalty = torch.norm(batch_mean)\n",
        "\n",
        "        # 2. Unit-variance penalty (Deviation of batch standard deviation from 1.0)\n",
        "        batch_std = z.std(dim=0)\n",
        "        std_penalty = torch.norm(batch_std - 1.0)\n",
        "\n",
        "        # The total loss is a weighted sum of deviation from target mean (0) and std (1)\n",
        "        total_regularization_loss = (mean_penalty + std_penalty) * self.lambda_reg\n",
        "        return total_regularization_loss, mean_penalty.item(), std_penalty.item()\n",
        "\n",
        "# Latent Projector (maps 4D state to 16D latent)\n",
        "class LatentProjector(nn.Module):\n",
        "    def __init__(self, state_dim=STATE_DIM, latent_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.encoder_net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        return self.encoder_net(x)\n",
        "\n",
        "latent_projector = LatentProjector(state_dim=STATE_DIM, latent_dim=LATENT_DIM).to(device)\n",
        "latent_projector.eval()\n",
        "print(\"LatentProjector instance:\", latent_projector)\n",
        "\n",
        "# Latent Dynamics Predictor\n",
        "class LatentDynamicsPredictor(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM, action_dim=ACTION_DIM):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim + action_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, latent_dim)\n",
        "\n",
        "    def forward(self, latent, action):\n",
        "        x = torch.cat([latent, action], dim=-1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        z_tp1 = self.fc2(x)\n",
        "        return z_tp1\n",
        "\n",
        "predictor = LatentDynamicsPredictor(latent_dim=LATENT_DIM, action_dim=ACTION_DIM).to(device)\n",
        "# Optimizer for both the predictor and the projector\n",
        "optimizer_pldm = optim.Adam(list(predictor.parameters()) + list(latent_projector.parameters()), lr=1e-3)\n",
        "print(\"Predictor instance:\", predictor)\n",
        "\n",
        "# Training Function (Modified to include LEJEPA Loss)\n",
        "def train_latent_dynamics_model(predictor_model, latent_projector_model, optimizer, training_data_loader, epochs=5):\n",
        "    predictor_model.train()\n",
        "    latent_projector_model.train()\n",
        "\n",
        "    # JEPA Predictive Loss\n",
        "    predictive_criterion = torch.nn.functional.mse_loss\n",
        "\n",
        "    # LEJEPA Regularization Loss (SIGReg Proxy)\n",
        "    sigreg_criterion = IsotropicGaussianLoss(lambda_reg=5.0)\n",
        "\n",
        "    print(\"\\n--- Training Latent Dynamics Predictor with LEJEPA-inspired SIGReg Regularization ---\")\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        total_mean_pen = 0\n",
        "        total_std_pen = 0\n",
        "        for batch_idx, (latent_s_t, action_t, latent_s_t_plus_1) in tqdm(\\\n",
        "            enumerate(training_data_loader),\\\n",
        "            total=len(training_data_loader),\\\n",
        "            desc=f\"Epoch {epoch+1}/{epochs}\"\n",
        "        ):\n",
        "            latent_s_t, action_t, latent_s_t_plus_1 = (\\\n",
        "                latent_s_t.to(device),\\\n",
        "                action_t.to(device),\\\n",
        "                latent_s_t_plus_1.to(device)\\\n",
        "            )\n",
        "\n",
        "            # 1. JEPA Predictive Step (Minimize prediction error)\n",
        "            predicted_z_t_plus_1 = predictor_model(latent_s_t, action_t)\n",
        "            predictive_loss = predictive_criterion(predicted_z_t_plus_1, latent_s_t_plus_1)\n",
        "\n",
        "            # 2. LEJEPA Regularization Step (Enforce N(0, I) on latent codes)\n",
        "            # Regularize the predicted state\n",
        "            sigreg_loss_pred, mean_pen_pred, std_pen_pred = sigreg_criterion(predicted_z_t_plus_1)\n",
        "            # Regularize the input state (enforcing the entire latent space distribution)\n",
        "            sigreg_loss_input, mean_pen_input, std_pen_input = sigreg_criterion(latent_s_t)\n",
        "\n",
        "            # Combine all loss components: JEPA Predictive Loss + LEJEPA Regularization Loss\n",
        "            total_loss_batch = predictive_loss + sigreg_loss_pred + sigreg_loss_input\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss_batch.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += total_loss_batch.item()\n",
        "            total_mean_pen += (mean_pen_pred + mean_pen_input) / 2\n",
        "            total_std_pen += (std_pen_pred + std_pen_input) / 2\n",
        "\n",
        "        avg_loss = total_loss / len(training_data_loader)\n",
        "        avg_mean_pen = total_mean_pen / len(training_data_loader)\n",
        "        avg_std_pen = total_std_pen / len(training_data_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Total Avg Loss: {avg_loss:.6f}, Pred Loss: {predictive_loss.item():.6f}, SIGReg Mean Pen: {avg_mean_pen:.6f}, SIGReg Std Pen: {avg_std_pen:.6f}\")\n",
        "    print(\"--- Training Complete with LEJEPA-inspired Regularization ---\")\n",
        "\n",
        "# Data Loading Function (Uses LatentProjector from THIS cell)\n",
        "def load_real_dynamics_data(device, adsb_dir=\"/content/adsb/kbtp/raw/2022\", num_trajectories=10, max_traj_per_file=100):\n",
        "    data = []\n",
        "    trajectory_files = []\n",
        "    # [... File loading and processing logic remains the same ...]\n",
        "    try:\n",
        "        for root, _, files in os.walk(adsb_dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.csv'):\n",
        "                    trajectory_files.append(os.path.join(root, file))\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"Failed to walk directory {adsb_dir}. Error: {e}\")\n",
        "        return []\n",
        "\n",
        "    if not trajectory_files:\n",
        "        logging.warning(f\"No CSV files found in {adsb_dir}. Falling back to synthetic data.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"\\nAttempting to load REAL ADS-B data...\")\n",
        "    print(f\"Found {len(trajectory_files)} CSV files. Limiting to {num_trajectories} files for demo speed.\")\n",
        "\n",
        "    trajectory_files = trajectory_files[:num_trajectories]\n",
        "\n",
        "    for file in tqdm(trajectory_files, desc=\"Processing Trajectories\"):\n",
        "        try:\n",
        "            if os.path.getsize(file) == 0:\n",
        "                logging.warning(f\"Skipping {file}: Empty file.\")\n",
        "                continue\n",
        "\n",
        "            df = pd.read_csv(file, on_bad_lines='skip', engine='python')\n",
        "            required_cols = ['Lat', 'Lon', 'Altitude', 'Speed']\n",
        "            time_col = 'Time'\n",
        "\n",
        "            available_cols = df.columns.tolist()\n",
        "            if not all(col in available_cols for col in required_cols):\n",
        "                logging.warning(f\"Skipping {file}: Missing columns. Required: {required_cols}, Available: {available_cols}\")\n",
        "                continue\n",
        "\n",
        "            for col in required_cols:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "            df.dropna(subset=required_cols, inplace=True)\n",
        "            if df.empty or len(df) < 2:\n",
        "                logging.warning(f\"Skipping {file}: Fewer than 2 valid rows after cleaning.\")\n",
        "                continue\n",
        "\n",
        "            # Clean Date and Time\n",
        "            def clean_date(x):\n",
        "                try:\n",
        "                    if isinstance(x, (list, tuple)):\n",
        "                        return '-'.join(str(i).zfill(2) if idx > 0 else str(i) for idx, i in enumerate(x))\n",
        "                    if isinstance(x, str):\n",
        "                        if x.startswith('['):\n",
        "                            x = eval(x)\n",
        "                            return '-'.join(str(i).zfill(2) if idx > 0 else str(i) for idx, i in enumerate(x))\n",
        "                        if '-' in x or '/' in x:\n",
        "                            return pd.to_datetime(x, errors='coerce').strftime('%Y-%m-%d')\n",
        "                        return x\n",
        "                    return str(x)\n",
        "                except Exception as e:\n",
        "                    return 'unknown'\n",
        "\n",
        "            def clean_time(x):\n",
        "                try:\n",
        "                    if isinstance(x, (list, tuple)):\n",
        "                        return ':'.join(str(i).zfill(2) for i in x)\n",
        "                    if isinstance(x, str):\n",
        "                        if x.startswith('['):\n",
        "                            x = eval(x)\n",
        "                            return ':'.join(str(i).zfill(2) for i in x)\n",
        "                        if ':' in x:\n",
        "                            return x\n",
        "                        return x\n",
        "                    return str(x)\n",
        "                except Exception as e:\n",
        "                    return '00:00:00'\n",
        "\n",
        "            if 'Date' in df.columns:\n",
        "                df['Date'] = df['Date'].apply(clean_date)\n",
        "            if 'Time' in df.columns:\n",
        "                df['Time'] = df['Time'].apply(clean_time)\n",
        "                df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce')\n",
        "\n",
        "            if 'Tail' in df.columns and 'Date' in df.columns:\n",
        "                df['traj_id'] = df['Tail'].astype(str) + '_' + df['Date'].astype(str)\n",
        "                df['traj_id'] = df['traj_id'].fillna('missing_' + pd.Series(range(len(df)), index=df.index).astype(str))\n",
        "            else:\n",
        "                logging.warning(f\"Missing 'Tail' or 'Date' in {file}. Using index as traj_id.\")\n",
        "                df['traj_id'] = pd.Series(range(len(df)), index=df.index).astype(str)\n",
        "\n",
        "\n",
        "            traj_ids = df['traj_id'].unique()[:max_traj_per_file]\n",
        "\n",
        "            latent_projector.eval()\n",
        "\n",
        "            for traj_id in traj_ids:\n",
        "                traj = df[df['traj_id'] == traj_id].sort_values(time_col)\n",
        "                if len(traj) < 2:\n",
        "                    logging.warning(f\"Skipping trajectory {traj_id} in {file}: Fewer than 2 points.\")\n",
        "                    continue\n",
        "\n",
        "                for i in range(len(traj) - 1):\n",
        "                    try:\n",
        "                        row_t = traj.iloc[i][required_cols]\n",
        "                        row_tp1 = traj.iloc[i+1][required_cols]\n",
        "                        if not all(row_t.apply(lambda x: isinstance(x, (int, float)) and not pd.isna(x))) or \\\n",
        "                           not all(row_tp1.apply(lambda x: isinstance(x, (int, float)) and not pd.isna(x))):\n",
        "                            logging.warning(f\"Non-numeric data in trajectory {traj_id} at index {i}: {row_t.values}, {row_tp1.values}\")\n",
        "                            continue\n",
        "                        state_t = torch.tensor(row_t.values.astype(np.float64), dtype=torch.float32, device=device)\n",
        "                        state_tp1 = torch.tensor(row_tp1.values.astype(np.float64), dtype=torch.float32, device=device)\n",
        "                        action = torch.tensor([\\\n",
        "                            state_tp1[3] - state_t[3],  # Delta Speed\n",
        "                            state_tp1[2] - state_t[2],  # Delta Altitude\n",
        "                            0.0, 0.0, 0.0, 0.0, 0.0, 0.0\\\n",
        "                        ], dtype=torch.float32, device=device)\n",
        "                        with torch.no_grad():\n",
        "                            projected_state_t = latent_projector(state_t).squeeze(0).cpu()\n",
        "                            projected_state_tp1 = latent_projector(state_tp1).squeeze(0).cpu()\n",
        "                        data.append((projected_state_t, action.cpu(), projected_state_tp1))\n",
        "                    except Exception as e:\n",
        "                        logging.warning(f\"Error processing trajectory {traj_id} in {file}: {e}\")\n",
        "\n",
        "            del df\n",
        "            gc.collect()\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error reading {file}: {e}\")\n",
        "\n",
        "    print(f\"Loaded {len(data)} real dynamics samples from {len(trajectory_files)} files.\")\n",
        "    return data\n",
        "\n",
        "# Synthetic Data Fallback\n",
        "def generate_synthetic_data(num_trajectories=1000, trajectory_length=20):\n",
        "    latent_projector.eval()\n",
        "    synthetic_data = []\n",
        "    # [.. Synthetic data generation logic remains the same ...]\n",
        "    for _ in range(num_trajectories):\n",
        "        base_state = torch.tensor([45.47, -73.74, 37000.0, 490.0], dtype=torch.float32, device=device)\n",
        "        base_state += torch.randn(STATE_DIM, device=device) * 0.1\n",
        "        current_state = base_state.clone()\n",
        "        for _ in range(trajectory_length):\n",
        "            delta_v = random.uniform(-10.0, 10.0)\n",
        "            delta_alt = random.uniform(-50.0, 50.0)\n",
        "            action = torch.tensor([delta_v, delta_alt, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\\\n",
        "                                  dtype=torch.float32, device='cpu').detach()\n",
        "            next_state = current_state.clone()\n",
        "            next_state[3] += delta_v\n",
        "            next_state[2] += delta_alt\n",
        "            with torch.no_grad():\n",
        "                projected_state_t = latent_projector(current_state.to(device)).squeeze(0).cpu().detach()\n",
        "                projected_state_tp1 = latent_projector(next_state.to(device)).squeeze(0).cpu().detach()\n",
        "            synthetic_data.append((projected_state_t, action, projected_state_tp1))\n",
        "            current_state = next_state.clone()\n",
        "    return synthetic_data\n",
        "\n",
        "# Execution Block\n",
        "FILE_LIMIT = 20\n",
        "print(\"\\n--- Starting AGI Demo Pipeline (Cell 3) ---\\n\")\n",
        "dynamics_training_data = load_real_dynamics_data(device, num_trajectories=FILE_LIMIT, max_traj_per_file=100)\n",
        "if not dynamics_training_data:\n",
        "    print(f\"--- CRITICAL WARNING: NO REAL DATA LOADED from {FILE_LIMIT} files. CREATING SYNTHETIC FALLBACK. ---\\n\")\n",
        "    dynamics_training_data = generate_synthetic_data(num_trajectories=1000)\n",
        "    print(f\"Successfully generated {len(dynamics_training_data)} synthetic causal samples for demonstration.\")\n",
        "if dynamics_training_data:\n",
        "    print(\"\\nPreparing DataLoader for batch training...\")\n",
        "    z_t_list, a_t_list, z_tp1_list = zip(*dynamics_training_data)\n",
        "    Z_T = torch.stack(z_t_list)\n",
        "    A_T = torch.stack(a_t_list)\n",
        "    Z_TP1 = torch.stack(z_tp1_list)\n",
        "    dataset = TensorDataset(Z_T, A_T, Z_TP1)\n",
        "    BATCH_SIZE = 64\n",
        "    dynamics_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    print(f\"Total batches for training: {len(dynamics_dataloader)}\")\n",
        "    train_latent_dynamics_model(predictor, latent_projector, optimizer_pldm, dynamics_dataloader, epochs=5)\n",
        "else:\n",
        "    print(\"FATAL ERROR: No data (real or synthetic) could be prepared. Training aborted.\")\n",
        "print(\"\\nCell 3 execution complete.\")\n",
        "print(\"Cell 3 completed. Predictor and latent_projector defined, incorporating LEJEPA-inspired regularization.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANWGXCqiOuik",
        "outputId": "226529af-a9c8-4291-b941-dc9dc48c7f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "LatentProjector instance: LatentProjector(\n",
            "  (encoder_net): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=16, bias=True)\n",
            "  )\n",
            ")\n",
            "Predictor instance: LatentDynamicsPredictor(\n",
            "  (fc1): Linear(in_features=24, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=16, bias=True)\n",
            ")\n",
            "\n",
            "--- Starting AGI Demo Pipeline (Cell 3) ---\n",
            "\n",
            "\n",
            "Attempting to load REAL ADS-B data...\n",
            "Found 246 CSV files. Limiting to 20 files for demo speed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Trajectories:   0%|          | 0/20 [00:00<?, ?it/s]WARNING:root:Skipping trajectory N247JB_2022-02-01 in /content/adsb/kbtp/raw/2022/02-01-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:   5%|▌         | 1/20 [02:24<45:43, 144.41s/it]WARNING:root:Skipping trajectory N133SY_2022-09-24 in /content/adsb/kbtp/raw/2022/09-24-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  10%|█         | 2/20 [03:06<25:14, 84.16s/it] WARNING:root:Skipping trajectory N187PQ_2022-06-05 in /content/adsb/kbtp/raw/2022/06-05-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N833DN_2022-06-05 in /content/adsb/kbtp/raw/2022/06-05-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N190JE_2022-06-05 in /content/adsb/kbtp/raw/2022/06-05-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  15%|█▌        | 3/20 [03:39<17:14, 60.84s/it]WARNING:root:Skipping trajectory N435AN_2022-07-19 in /content/adsb/kbtp/raw/2022/07-19-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N6711M_2022-07-19 in /content/adsb/kbtp/raw/2022/07-19-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  25%|██▌       | 5/20 [05:19<13:18, 53.21s/it]WARNING:root:Skipping /content/adsb/kbtp/raw/2022/06-09-22/1.csv: Empty file.\n",
            "Processing Trajectories:  35%|███▌      | 7/20 [05:24<05:55, 27.32s/it]WARNING:root:Skipping trajectory C-FJAS_2022-08-21 in /content/adsb/kbtp/raw/2022/08-21-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  40%|████      | 8/20 [05:48<05:18, 26.55s/it]WARNING:root:Skipping trajectory N610NN_2022-03-03 in /content/adsb/kbtp/raw/2022/03-03-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory C-GJVX_2022-03-03 in /content/adsb/kbtp/raw/2022/03-03-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N29MG_2022-03-03 in /content/adsb/kbtp/raw/2022/03-03-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory C-GHPD_2022-03-03 in /content/adsb/kbtp/raw/2022/03-03-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  45%|████▌     | 9/20 [07:43<09:19, 50.86s/it]WARNING:root:Skipping trajectory N902BC_2022-05-09 in /content/adsb/kbtp/raw/2022/05-09-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  55%|█████▌    | 11/20 [12:00<13:35, 90.57s/it]WARNING:root:Skipping trajectory N446AW_2022-02-02 in /content/adsb/kbtp/raw/2022/02-02-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  60%|██████    | 12/20 [13:23<11:46, 88.30s/it]WARNING:root:Skipping trajectory N843DN_2022-07-05 in /content/adsb/kbtp/raw/2022/07-05-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N301DV_2022-07-05 in /content/adsb/kbtp/raw/2022/07-05-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  65%|██████▌   | 13/20 [13:56<08:24, 72.09s/it]WARNING:root:Skipping trajectory N465UA_2022-06-23 in /content/adsb/kbtp/raw/2022/06-23-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N8644C_2022-06-23 in /content/adsb/kbtp/raw/2022/06-23-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  70%|███████   | 14/20 [14:46<06:33, 65.61s/it]WARNING:root:Skipping trajectory N455UP_2022-06-17 in /content/adsb/kbtp/raw/2022/06-17-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N424AN_2022-06-17 in /content/adsb/kbtp/raw/2022/06-17-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N892FD_2022-06-17 in /content/adsb/kbtp/raw/2022/06-17-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  75%|███████▌  | 15/20 [15:34<05:01, 60.33s/it]WARNING:root:Skipping trajectory N425AW_2022-08-28 in /content/adsb/kbtp/raw/2022/08-28-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  80%|████████  | 16/20 [16:36<04:03, 60.90s/it]WARNING:root:Skipping trajectory N912UY_2022-03-13 in /content/adsb/kbtp/raw/2022/03-13-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory C-GHPJ_2022-03-13 in /content/adsb/kbtp/raw/2022/03-13-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory C-GWJG_2022-03-13 in /content/adsb/kbtp/raw/2022/03-13-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N364NW_2022-03-13 in /content/adsb/kbtp/raw/2022/03-13-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  85%|████████▌ | 17/20 [17:10<02:38, 52.99s/it]WARNING:root:Skipping /content/adsb/kbtp/raw/2022/04-23-22/1.csv: Fewer than 2 valid rows after cleaning.\n",
            "WARNING:root:Skipping trajectory N215JQ_2022-09-22 in /content/adsb/kbtp/raw/2022/09-22-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N748FE_2022-09-22 in /content/adsb/kbtp/raw/2022/09-22-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  95%|█████████▌| 19/20 [17:59<00:39, 39.77s/it]WARNING:root:Skipping trajectory N823MA_2022-04-26 in /content/adsb/kbtp/raw/2022/04-26-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N905EV_2022-04-26 in /content/adsb/kbtp/raw/2022/04-26-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories: 100%|██████████| 20/20 [18:10<00:00, 54.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 588006 real dynamics samples from 20 files.\n",
            "\n",
            "Preparing DataLoader for batch training...\n",
            "Total batches for training: 9188\n",
            "\n",
            "--- Training Latent Dynamics Predictor with LEJEPA-inspired SIGReg Regularization ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|██████████| 9188/9188 [00:25<00:00, 365.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Total Avg Loss: 72044.490055, Pred Loss: 68.497559, SIGReg Mean Pen: 3069.874507, SIGReg Std Pen: 3703.850365\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5:  66%|██████▌   | 6069/9188 [00:16<00:08, 375.00it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CEL4-NEW"
      ],
      "metadata": {
        "id": "hO_QYKpgRv33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import gc # Import garbage collector\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Define constants used by predictor/projector from Cell 3 (for robustness) ---\n",
        "LATENT_DIM = 16\n",
        "ACTION_DIM = 8\n",
        "# ---------------------------------------------------------------------------------\n",
        "\n",
        "# Define missing variables (re-initializing models here for demonstration resilience)\n",
        "AIRPORTS = {\n",
        "    \"CYUL\": {\"lat\": 45.4706, \"lon\": -73.7408, \"name\": \"Montreal-Trudeau International\"},\n",
        "    \"LFPG\": {\"lat\": 49.0128, \"lon\": 2.5500, \"name\": \"Paris-Charles de Gaulle\"}\n",
        "}\n",
        "AIRCRAFT_PERFORMANCE = {\n",
        "    \"Boeing777_300ER\": {\n",
        "        \"max_speed_kts\": 490.0,\n",
        "        \"cruise_altitude_ft\": 37000.0,\n",
        "        \"range_nm\": 7370.0\n",
        "    }\n",
        "}\n",
        "# Move model and processor to the specified device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=False)\n",
        "\n",
        "# Redefine LatentProjector (Mapping Visual Feature Dim -> Latent Dim 16)\n",
        "class LatentProjector(nn.Module):\n",
        "    def __init__(self, state_dim=4, latent_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        # This re-definition assumes the input size is the CLIP feature dimension (512) for the planning logic,\n",
        "        # but the latent output size is consistent with Cell 3 (16D)\n",
        "        self.encoder_net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        return self.encoder_net(x)\n",
        "\n",
        "# Placeholder for load_and_process_video (Using CLIP features for planning demo)\n",
        "def load_and_process_video(video_path, processor, model, device, num_frames=1):\n",
        "    try:\n",
        "        from torchvision.io import read_video\n",
        "        video, _, _ = read_video(video_path, pts_unit='sec')\n",
        "        video = video[:num_frames].to(device)\n",
        "        features_list = []\n",
        "        for frame in video:\n",
        "            inputs = processor(images=frame.unsqueeze(0), return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                features = model.get_image_features(**inputs)\n",
        "                features_list.append(features)\n",
        "\n",
        "        if not features_list:\n",
        "             return None, \"No frames processed\"\n",
        "\n",
        "        averaged_features = torch.mean(torch.stack(features_list), dim=0)\n",
        "        del features_list, features, inputs, video, frame\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return averaged_features, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_path}: {e}\")\n",
        "        return None, str(e)\n",
        "\n",
        "# Predictor model (same structure as Cell 3)\n",
        "class LatentDynamicsPredictor(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM, action_dim=ACTION_DIM):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim + action_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, latent_dim)\n",
        "\n",
        "    def forward(self, latent, action):\n",
        "        x = torch.cat([latent, action], dim=-1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        z_tp1 = self.fc2(x)\n",
        "        return z_tp1\n",
        "\n",
        "# Re-instantiating models from Cell 3 to allow standalone execution (conceptually loaded)\n",
        "# NOTE: These models are UNTRAINED here, but in a functional pipeline, they would carry the weights\n",
        "# trained in Cell 3 with the LEJEPA regularization.\n",
        "latent_projector = LatentProjector(state_dim=512, latent_dim=LATENT_DIM).to(device) # Initialized with CLIP dim (512)\n",
        "predictor = LatentDynamicsPredictor(latent_dim=LATENT_DIM, action_dim=ACTION_DIM).to(device)\n",
        "\n",
        "\n",
        "def plan_montreal_to_paris_flight(start_airport_data, target_airport_data, aircraft_model_data,\\\n",
        "                                  encoder_model, processor_instance, predictor_model, latent_projector_instance,\\\n",
        "                                  planning_horizon=50, action_dim=ACTION_DIM, num_action_samples=50):\n",
        "    encoder_model.eval()\n",
        "    predictor_model.eval()\n",
        "    latent_projector_instance.eval()\n",
        "\n",
        "    initial_video_path = '/content/gdrive/MyDrive/datasets/TartanAviation/vision/1_2023-02-22-15-21-49/1_2023-02-22-15-21-49.mp4'\n",
        "    target_video_path = initial_video_path\n",
        "\n",
        "    # 1. VISUAL ENCODING (CLIP proxy)\n",
        "    initial_features, initial_error = load_and_process_video(initial_video_path, processor_instance, encoder_model, device, num_frames=1)\n",
        "    target_features, target_error = load_and_process_video(target_video_path, processor_instance, encoder_model, device, num_frames=1)\n",
        "\n",
        "    if initial_features is None or target_features is None:\n",
        "        print(\"Video load failed. Using dummy features.\")\n",
        "        dummy_feature_shape = (1, 512)\n",
        "        initial_features = torch.rand(dummy_feature_shape).to(device)\n",
        "        target_features = torch.rand(dummy_feature_shape).to(device)\n",
        "\n",
        "    # 2. LATENT PROJECTION (Using LEJEPA-stabilized Projector logic)\n",
        "    visual_feature_dim = initial_features.shape[-1]\n",
        "\n",
        "    # Re-initialize projector if size mismatch (essential for resilience)\n",
        "    if latent_projector_instance.encoder_net[0].in_features != visual_feature_dim:\n",
        "         latent_projector_instance = LatentProjector(state_dim=visual_feature_dim, latent_dim=LATENT_DIM).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        current_latent_state = latent_projector_instance(initial_features)\n",
        "        target_latent_state = latent_projector_instance(target_features)\n",
        "\n",
        "    del initial_features, target_features\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # 3. CONTEXT AND ETHICAL BOUNDARIES\n",
        "    ETHICAL_BOUNDARY_LATENT_VECTOR = torch.zeros(1, LATENT_DIM).to(device)\n",
        "    salience = torch.rand(1).to(device) * 0.8\n",
        "\n",
        "    print(\"\\n--- Starting Causal Flight Plan (MPPI) ---\")\n",
        "    print(f\"Current Latent State Shape: {current_latent_state.shape} (LEJEPA-stabilized latent space)\")\n",
        "    print(f\"Target Latent State is established.\")\n",
        "\n",
        "    best_action_sequence = []\n",
        "    current_latent = current_latent_state\n",
        "\n",
        "    # 4. ITERATIVE PLANNING LOOP\n",
        "    for step in range(planning_horizon):\n",
        "        candidate_actions = torch.rand(num_action_samples, action_dim).to(device) * 2.0 - 1.0\n",
        "\n",
        "        repeated_current_latent = current_latent.repeat(num_action_samples, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            simulated_next_latents = predictor_model(repeated_current_latent, candidate_actions)\n",
        "\n",
        "        del repeated_current_latent\n",
        "\n",
        "        simulated_trajectories_cost = []\n",
        "        for i in range(num_action_samples):\n",
        "            simulated_next_latent = simulated_next_latents[i].unsqueeze(0)\n",
        "\n",
        "            # --- COST FUNCTION (Pillars 3, 4, 5) ---\n",
        "            goal_proximity_cost = torch.norm(target_latent_state - simulated_next_latent) * 1.0\n",
        "            conceptual_fuel_cost = torch.norm(candidate_actions[i]) * 0.05\n",
        "            ethical_cost = 5.0 * torch.norm(ETHICAL_BOUNDARY_LATENT_VECTOR - simulated_next_latent)\n",
        "            salience_alignment_cost = 2.0 * torch.norm(candidate_actions[i]) * salience # Penalty based on env salience\n",
        "\n",
        "            total_cost = goal_proximity_cost + conceptual_fuel_cost + ethical_cost + salience_alignment_cost\n",
        "            simulated_trajectories_cost.append(total_cost.item())\n",
        "\n",
        "        del simulated_next_latents\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        best_candidate_idx = torch.argmin(torch.tensor(simulated_trajectories_cost))\n",
        "\n",
        "        # Regenerate candidate actions to retrieve the optimal one\n",
        "        candidate_actions_regenerated = torch.rand(num_action_samples, action_dim).to(device) * 2.0 - 1.0\n",
        "        optimal_action_for_step = candidate_actions_regenerated[best_candidate_idx]\n",
        "        del candidate_actions_regenerated\n",
        "\n",
        "        best_action_sequence.append(optimal_action_for_step.squeeze().cpu().numpy())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Update current_latent using the chosen optimal action (Causal Prediction)\n",
        "            optimal_action_input = optimal_action_for_step.unsqueeze(0)\n",
        "            current_latent = predictor_model(current_latent, optimal_action_input)\n",
        "\n",
        "        del optimal_action_for_step, optimal_action_input\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "    print(f\"\\nConceptual Flight Plan generated for {planning_horizon} steps (first 5 actions shown):\")\n",
        "    for i, action in enumerate(best_action_sequence[:5]):\n",
        "        print(f\"Step {i+1}: {np.round(action, 4)}\")\n",
        "    return best_action_sequence\n",
        "\n",
        "# --- EXECUTION ---\n",
        "try:\n",
        "    conceptual_flight_plan_actions = plan_montreal_to_paris_flight(\n",
        "        AIRPORTS[\"CYUL\"], AIRPORTS[\"LFPG\"], AIRCRAFT_PERFORMANCE[\"Boeing777_300ER\"],\n",
        "        model, processor, predictor, latent_projector,\n",
        "        num_action_samples=50\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during flight planning: {e}\")\n",
        "    print(\"NOTE: This execution will use randomly initialized (untrained) models due to the previous failure, resulting in random action outputs.\")"
      ],
      "metadata": {
        "id": "8LYyIUbQSHFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CELL3-OLD"
      ],
      "metadata": {
        "id": "HFbPvmxpSIWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import gc # Import garbage collector\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Define constants used by predictor/projector from Cell 3 (for robustness) ---\n",
        "LATENT_DIM = 16\n",
        "ACTION_DIM = 8\n",
        "# ---------------------------------------------------------------------------------\n",
        "\n",
        "# Define missing variables (re-initializing models here for demonstration resilience)\n",
        "AIRPORTS = {\n",
        "    \"CYUL\": {\"lat\": 45.4706, \"lon\": -73.7408, \"name\": \"Montreal-Trudeau International\"},\n",
        "    \"LFPG\": {\"lat\": 49.0128, \"lon\": 2.5500, \"name\": \"Paris-Charles de Gaulle\"}\n",
        "}\n",
        "AIRCRAFT_PERFORMANCE = {\n",
        "    \"Boeing777_300ER\": {\n",
        "        \"max_speed_kts\": 490.0,\n",
        "        \"cruise_altitude_ft\": 37000.0,\n",
        "        \"range_nm\": 7370.0\n",
        "    }\n",
        "}\n",
        "# Move model and processor to the specified device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=False)\n",
        "\n",
        "# Redefine LatentProjector (Mapping Visual Feature Dim -> Latent Dim 16)\n",
        "class LatentProjector(nn.Module):\n",
        "    def __init__(self, state_dim=4, latent_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        # This re-definition assumes the input size is the CLIP feature dimension (512) for the planning logic,\n",
        "        # but the latent output size is consistent with Cell 3 (16D)\n",
        "        self.encoder_net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        return self.encoder_net(x)\n",
        "\n",
        "# Placeholder for load_and_process_video (Using CLIP features for planning demo)\n",
        "def load_and_process_video(video_path, processor, model, device, num_frames=1):\n",
        "    try:\n",
        "        from torchvision.io import read_video\n",
        "        video, _, _ = read_video(video_path, pts_unit='sec')\n",
        "        video = video[:num_frames].to(device)\n",
        "        features_list = []\n",
        "        for frame in video:\n",
        "            inputs = processor(images=frame.unsqueeze(0), return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                features = model.get_image_features(**inputs)\n",
        "                features_list.append(features)\n",
        "\n",
        "        if not features_list:\n",
        "             return None, \"No frames processed\"\n",
        "\n",
        "        averaged_features = torch.mean(torch.stack(features_list), dim=0)\n",
        "        del features_list, features, inputs, video, frame\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        return averaged_features, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_path}: {e}\")\n",
        "        return None, str(e)\n",
        "\n",
        "# Predictor model (same structure as Cell 3)\n",
        "class LatentDynamicsPredictor(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM, action_dim=ACTION_DIM):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim + action_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, latent_dim)\n",
        "\n",
        "    def forward(self, latent, action):\n",
        "        x = torch.cat([latent, action], dim=-1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        z_tp1 = self.fc2(x)\n",
        "        return z_tp1\n",
        "\n",
        "# Re-instantiating models from Cell 3 to allow standalone execution (conceptually loaded)\n",
        "# NOTE: These models are UNTRAINED here, but in a functional pipeline, they would carry the weights\n",
        "# trained in Cell 3 with the LEJEPA regularization.\n",
        "latent_projector = LatentProjector(state_dim=512, latent_dim=LATENT_DIM).to(device) # Initialized with CLIP dim (512)\n",
        "predictor = LatentDynamicsPredictor(latent_dim=LATENT_DIM, action_dim=ACTION_DIM).to(device)\n",
        "\n",
        "\n",
        "def plan_montreal_to_paris_flight(start_airport_data, target_airport_data, aircraft_model_data,\\\n",
        "                                  encoder_model, processor_instance, predictor_model, latent_projector_instance,\\\n",
        "                                  planning_horizon=50, action_dim=ACTION_DIM, num_action_samples=50):\n",
        "    encoder_model.eval()\n",
        "    predictor_model.eval()\n",
        "    latent_projector_instance.eval()\n",
        "\n",
        "    initial_video_path = '/content/gdrive/MyDrive/datasets/TartanAviation/vision/1_2023-02-22-15-21-49/1_2023-02-22-15-21-49.mp4'\n",
        "    target_video_path = initial_video_path\n",
        "\n",
        "    # 1. VISUAL ENCODING (CLIP proxy)\n",
        "    initial_features, initial_error = load_and_process_video(initial_video_path, processor_instance, encoder_model, device, num_frames=1)\n",
        "    target_features, target_error = load_and_process_video(target_video_path, processor_instance, encoder_model, device, num_frames=1)\n",
        "\n",
        "    if initial_features is None or target_features is None:\n",
        "        print(\"Video load failed. Using dummy features.\")\n",
        "        dummy_feature_shape = (1, 512)\n",
        "        initial_features = torch.rand(dummy_feature_shape).to(device)\n",
        "        target_features = torch.rand(dummy_feature_shape).to(device)\n",
        "\n",
        "    # 2. LATENT PROJECTION (Using LEJEPA-stabilized Projector logic)\n",
        "    visual_feature_dim = initial_features.shape[-1]\n",
        "\n",
        "    # Re-initialize projector if size mismatch (essential for resilience)\n",
        "    if latent_projector_instance.encoder_net[0].in_features != visual_feature_dim:\n",
        "         latent_projector_instance = LatentProjector(state_dim=visual_feature_dim, latent_dim=LATENT_DIM).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        current_latent_state = latent_projector_instance(initial_features)\n",
        "        target_latent_state = latent_projector_instance(target_features)\n",
        "\n",
        "    del initial_features, target_features\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # 3. CONTEXT AND ETHICAL BOUNDARIES\n",
        "    ETHICAL_BOUNDARY_LATENT_VECTOR = torch.zeros(1, LATENT_DIM).to(device)\n",
        "    salience = torch.rand(1).to(device) * 0.8\n",
        "\n",
        "    print(\"\\n--- Starting Causal Flight Plan (MPPI) ---\")\n",
        "    print(f\"Current Latent State Shape: {current_latent_state.shape} (LEJEPA-stabilized latent space)\")\n",
        "    print(f\"Target Latent State is established.\")\n",
        "\n",
        "    best_action_sequence = []\n",
        "    current_latent = current_latent_state\n",
        "\n",
        "    # 4. ITERATIVE PLANNING LOOP\n",
        "    for step in range(planning_horizon):\n",
        "        candidate_actions = torch.rand(num_action_samples, action_dim).to(device) * 2.0 - 1.0\n",
        "\n",
        "        repeated_current_latent = current_latent.repeat(num_action_samples, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            simulated_next_latents = predictor_model(repeated_current_latent, candidate_actions)\n",
        "\n",
        "        del repeated_current_latent\n",
        "\n",
        "        simulated_trajectories_cost = []\n",
        "        for i in range(num_action_samples):\n",
        "            simulated_next_latent = simulated_next_latents[i].unsqueeze(0)\n",
        "\n",
        "            # --- COST FUNCTION (Pillars 3, 4, 5) ---\n",
        "            goal_proximity_cost = torch.norm(target_latent_state - simulated_next_latent) * 1.0\n",
        "            conceptual_fuel_cost = torch.norm(candidate_actions[i]) * 0.05\n",
        "            ethical_cost = 5.0 * torch.norm(ETHICAL_BOUNDARY_LATENT_VECTOR - simulated_next_latent)\n",
        "            salience_alignment_cost = 2.0 * torch.norm(candidate_actions[i]) * salience # Penalty based on env salience\n",
        "\n",
        "            total_cost = goal_proximity_cost + conceptual_fuel_cost + ethical_cost + salience_alignment_cost\n",
        "            simulated_trajectories_cost.append(total_cost.item())\n",
        "\n",
        "        del simulated_next_latents\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        best_candidate_idx = torch.argmin(torch.tensor(simulated_trajectories_cost))\n",
        "\n",
        "        # Regenerate candidate actions to retrieve the optimal one\n",
        "        candidate_actions_regenerated = torch.rand(num_action_samples, action_dim).to(device) * 2.0 - 1.0\n",
        "        optimal_action_for_step = candidate_actions_regenerated[best_candidate_idx]\n",
        "        del candidate_actions_regenerated\n",
        "\n",
        "        best_action_sequence.append(optimal_action_for_step.squeeze().cpu().numpy())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Update current_latent using the chosen optimal action (Causal Prediction)\n",
        "            optimal_action_input = optimal_action_for_step.unsqueeze(0)\n",
        "            current_latent = predictor_model(current_latent, optimal_action_input)\n",
        "\n",
        "        del optimal_action_for_step, optimal_action_input\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "    print(f\"\\nConceptual Flight Plan generated for {planning_horizon} steps (first 5 actions shown):\")\n",
        "    for i, action in enumerate(best_action_sequence[:5]):\n",
        "        print(f\"Step {i+1}: {np.round(action, 4)}\")\n",
        "    return best_action_sequence\n",
        "\n",
        "# --- EXECUTION ---\n",
        "try:\n",
        "    conceptual_flight_plan_actions = plan_montreal_to_paris_flight(\n",
        "        AIRPORTS[\"CYUL\"], AIRPORTS[\"LFPG\"], AIRCRAFT_PERFORMANCE[\"Boeing777_300ER\"],\n",
        "        model, processor, predictor, latent_projector,\n",
        "        num_action_samples=50\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during flight planning: {e}\")\n",
        "    print(\"NOTE: This execution will use randomly initialized (untrained) models due to the previous failure, resulting in random action outputs.\")"
      ],
      "metadata": {
        "id": "GLRfVTp9Rzg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import logging\n",
        "import psutil\n",
        "import gc\n",
        "\n",
        "# Set logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(message)s')\n",
        "\n",
        "# Constants\n",
        "STATE_DIM = 4      # Lat, Lon, Altitude, Speed\n",
        "LATENT_DIM = 16\n",
        "ACTION_DIM = 8\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Latent Projector\n",
        "class LatentProjector(nn.Module):\n",
        "    def __init__(self, state_dim=STATE_DIM, latent_dim=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.encoder_net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        return self.encoder_net(x)\n",
        "\n",
        "latent_projector = LatentProjector(state_dim=STATE_DIM, latent_dim=LATENT_DIM).to(device)\n",
        "latent_projector.eval()\n",
        "print(\"LatentProjector instance:\", latent_projector)\n",
        "\n",
        "# Latent Dynamics Predictor\n",
        "class LatentDynamicsPredictor(nn.Module):\n",
        "    def __init__(self, latent_dim=LATENT_DIM, action_dim=ACTION_DIM):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim + action_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, latent_dim)\n",
        "\n",
        "    def forward(self, latent, action):\n",
        "        x = torch.cat([latent, action], dim=-1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        z_tp1 = self.fc2(x)\n",
        "        return z_tp1\n",
        "\n",
        "predictor = LatentDynamicsPredictor(latent_dim=LATENT_DIM, action_dim=ACTION_DIM).to(device)\n",
        "optimizer_pldm = optim.Adam(list(predictor.parameters()) + list(latent_projector.parameters()), lr=1e-3)\n",
        "print(\"Predictor instance:\", predictor)\n",
        "\n",
        "# Training Function\n",
        "def train_latent_dynamics_model(predictor_model, optimizer, training_data_loader, epochs=5):\n",
        "    predictor_model.train()\n",
        "    criterion = torch.nn.functional.mse_loss\n",
        "    print(\"\\n--- Training Latent Dynamics Predictor for Conceptual Real Flights (Causal Focus) ---\")\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (latent_s_t, action_t, latent_s_t_plus_1) in tqdm(\n",
        "            enumerate(training_data_loader),\n",
        "            total=len(training_data_loader),\n",
        "            desc=f\"Epoch {epoch+1}/{epochs}\"\n",
        "        ):\n",
        "            latent_s_t, action_t, latent_s_t_plus_1 = (\n",
        "                latent_s_t.to(device),\n",
        "                action_t.to(device),\n",
        "                latent_s_t_plus_1.to(device)\n",
        "            )\n",
        "            predicted_z_t_plus_1 = predictor_model(latent_s_t, action_t)\n",
        "            loss = criterion(predicted_z_t_plus_1, latent_s_t_plus_1)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {total_loss / len(training_data_loader):.6f}\")\n",
        "    print(\"--- Training Complete ---\")\n",
        "\n",
        "# Data Loading Function\n",
        "def load_real_dynamics_data(device, adsb_dir=\"/content/adsb/kbtp/raw/2022\", num_trajectories=10, max_traj_per_file=100):\n",
        "    \"\"\"\n",
        "    Loads ADS-B trajectories and transforms them into (latent_s_t, action_t, latent_s_t_plus_1) tuples.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    trajectory_files = []\n",
        "\n",
        "    try:\n",
        "        for root, _, files in os.walk(adsb_dir):\n",
        "            for file in files:\n",
        "                if file.endswith('.csv'):\n",
        "                    trajectory_files.append(os.path.join(root, file))\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"Failed to walk directory {adsb_dir}. Error: {e}\")\n",
        "        return []\n",
        "\n",
        "    if not trajectory_files:\n",
        "        logging.warning(f\"No CSV files found in {adsb_dir}. Falling back to synthetic data.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"\\nAttempting to load REAL ADS-B data...\")\n",
        "    print(f\"Found {len(trajectory_files)} CSV files. Limiting to {num_trajectories} files for demo speed.\")\n",
        "\n",
        "    trajectory_files = trajectory_files[:num_trajectories]\n",
        "\n",
        "    for file in tqdm(trajectory_files, desc=\"Processing Trajectories\"):\n",
        "        try:\n",
        "            print(f\"\\nProcessing {file}\")\n",
        "            print(f\"Memory usage before: {psutil.virtual_memory().percent}%\")\n",
        "            if os.path.getsize(file) == 0:\n",
        "                logging.warning(f\"Skipping {file}: Empty file.\")\n",
        "                continue\n",
        "\n",
        "            df = pd.read_csv(file, on_bad_lines='skip', engine='python')\n",
        "            required_cols = ['Lat', 'Lon', 'Altitude', 'Speed']\n",
        "            time_col = 'Time'\n",
        "\n",
        "            available_cols = df.columns.tolist()\n",
        "            print(f\"Columns in {file}: {available_cols}\")\n",
        "            if not all(col in available_cols for col in required_cols):\n",
        "                logging.warning(f\"Skipping {file}: Missing columns. Required: {required_cols}, Available: {available_cols}\")\n",
        "                continue\n",
        "\n",
        "            for col in required_cols:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "            df.dropna(subset=required_cols, inplace=True)\n",
        "            print(f\"Rows after cleaning {file}: {len(df)}\")\n",
        "\n",
        "            if df.empty or len(df) < 2:\n",
        "                logging.warning(f\"Skipping {file}: Fewer than 2 valid rows after cleaning.\")\n",
        "                continue\n",
        "\n",
        "            # Clean Date\n",
        "            def clean_date(x):\n",
        "                try:\n",
        "                    if isinstance(x, (list, tuple)):\n",
        "                        return '-'.join(str(i).zfill(2) if idx > 0 else str(i) for idx, i in enumerate(x))\n",
        "                    if isinstance(x, str):\n",
        "                        if x.startswith('['):\n",
        "                            x = eval(x)\n",
        "                            return '-'.join(str(i).zfill(2) if idx > 0 else str(i) for idx, i in enumerate(x))\n",
        "                        if '-' in x or '/' in x:\n",
        "                            return pd.to_datetime(x, errors='coerce').strftime('%Y-%m-%d')\n",
        "                        return x\n",
        "                    return str(x)\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error cleaning Date in {file}: {e}\")\n",
        "                    return 'unknown'\n",
        "\n",
        "            # Clean Time\n",
        "            def clean_time(x):\n",
        "                try:\n",
        "                    if isinstance(x, (list, tuple)):\n",
        "                        return ':'.join(str(i).zfill(2) for i in x)\n",
        "                    if isinstance(x, str):\n",
        "                        if x.startswith('['):\n",
        "                            x = eval(x)\n",
        "                            return ':'.join(str(i).zfill(2) for i in x)\n",
        "                        if ':' in x:\n",
        "                            return x\n",
        "                        return x\n",
        "                    return str(x)\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Error cleaning Time in {file}: {e}\")\n",
        "                    return '00:00:00'\n",
        "\n",
        "            if 'Date' in df.columns:\n",
        "                df['Date'] = df['Date'].apply(clean_date)\n",
        "                print(f\"Cleaned Date sample in {file}:\\n\", df['Date'].head(5).to_list())\n",
        "            if 'Time' in df.columns:\n",
        "                df['Time'] = df['Time'].apply(clean_time)\n",
        "                print(f\"Cleaned Time sample in {file}:\\n\", df['Time'].head(5).to_list())\n",
        "                df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S', errors='coerce')\n",
        "\n",
        "            if 'Tail' in df.columns and 'Date' in df.columns:\n",
        "                df['traj_id'] = df['Tail'].astype(str) + '_' + df['Date'].astype(str)\n",
        "                df['traj_id'] = df['traj_id'].fillna('missing_' + pd.Series(range(len(df)), index=df.index).astype(str))\n",
        "            else:\n",
        "                logging.warning(f\"Missing 'Tail' or 'Date' in {file}. Using index as traj_id.\")\n",
        "                df['traj_id'] = pd.Series(range(len(df)), index=df.index).astype(str)\n",
        "\n",
        "            print(f\"Unique trajectories in {file}: {df['traj_id'].nunique()}\")\n",
        "            print(\"Rows per traj_id:\\n\", df.groupby('traj_id').size().describe())\n",
        "\n",
        "            # Limit trajectories to manage memory\n",
        "            traj_ids = df['traj_id'].unique()[:max_traj_per_file]\n",
        "            print(f\"Limiting to {len(traj_ids)} trajectories in {file}\")\n",
        "\n",
        "            latent_projector.eval()\n",
        "\n",
        "            for traj_id in traj_ids:\n",
        "                traj = df[df['traj_id'] == traj_id].sort_values(time_col)\n",
        "                #print(f\"Trajectory {traj_id} in {file}: {len(traj)} rows\")\n",
        "                if len(traj) < 2:\n",
        "                    logging.warning(f\"Skipping trajectory {traj_id} in {file}: Fewer than 2 points.\")\n",
        "                    continue\n",
        "\n",
        "                for i in range(len(traj) - 1):\n",
        "                    try:\n",
        "                        row_t = traj.iloc[i][required_cols]\n",
        "                        row_tp1 = traj.iloc[i+1][required_cols]\n",
        "                        if not all(row_t.apply(lambda x: isinstance(x, (int, float)) and not pd.isna(x))) or \\\n",
        "                           not all(row_tp1.apply(lambda x: isinstance(x, (int, float)) and not pd.isna(x))):\n",
        "                            logging.warning(f\"Non-numeric data in trajectory {traj_id} at index {i}: {row_t.values}, {row_tp1.values}\")\n",
        "                            continue\n",
        "                        state_t = torch.tensor(row_t.values.astype(np.float64), dtype=torch.float32, device=device)\n",
        "                        state_tp1 = torch.tensor(row_tp1.values.astype(np.float64), dtype=torch.float32, device=device)\n",
        "                        action = torch.tensor([\n",
        "                            state_tp1[3] - state_t[3],  # Delta Speed\n",
        "                            state_tp1[2] - state_t[2],  # Delta Altitude\n",
        "                            0.0, 0.0, 0.0, 0.0, 0.0, 0.0\n",
        "                        ], dtype=torch.float32, device=device)\n",
        "                        with torch.no_grad():\n",
        "                            projected_state_t = latent_projector(state_t).squeeze(0).cpu()\n",
        "                            projected_state_tp1 = latent_projector(state_tp1).squeeze(0).cpu()\n",
        "                        data.append((projected_state_t, action.cpu(), projected_state_tp1))\n",
        "                    except Exception as e:\n",
        "                        logging.warning(f\"Error processing trajectory {traj_id} in {file}: {e}\")\n",
        "\n",
        "            del df\n",
        "            gc.collect()\n",
        "            print(f\"Memory usage after {file}: {psutil.virtual_memory().percent}%\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Error reading {file}: {e}\")\n",
        "\n",
        "    print(f\"Loaded {len(data)} real dynamics samples from {len(trajectory_files)} files.\")\n",
        "    return data\n",
        "\n",
        "# Synthetic Data Fallback\n",
        "def generate_synthetic_data(num_trajectories=1000, trajectory_length=20):\n",
        "    latent_projector.eval()\n",
        "    synthetic_data = []\n",
        "    for _ in range(num_trajectories):\n",
        "        base_state = torch.tensor([45.47, -73.74, 37000.0, 490.0], dtype=torch.float32, device=device)\n",
        "        base_state += torch.randn(STATE_DIM, device=device) * 0.1\n",
        "        current_state = base_state.clone()\n",
        "        for _ in range(trajectory_length):\n",
        "            delta_v = random.uniform(-10.0, 10.0)\n",
        "            delta_alt = random.uniform(-50.0, 50.0)\n",
        "            action = torch.tensor([delta_v, delta_alt, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
        "                                  dtype=torch.float32, device='cpu').detach()\n",
        "            next_state = current_state.clone()\n",
        "            next_state[3] += delta_v\n",
        "            next_state[2] += delta_alt\n",
        "            with torch.no_grad():\n",
        "                projected_state_t = latent_projector(current_state.to(device)).squeeze(0).cpu().detach()\n",
        "                projected_state_tp1 = latent_projector(next_state.to(device)).squeeze(0).cpu().detach()\n",
        "            synthetic_data.append((projected_state_t, action, projected_state_tp1))\n",
        "            current_state = next_state.clone()\n",
        "    return synthetic_data\n",
        "\n",
        "# Execution Block\n",
        "FILE_LIMIT = 20\n",
        "print(\"\\n--- Starting AGI Demo Pipeline (Cell 3) ---\")\n",
        "dynamics_training_data = load_real_dynamics_data(device, num_trajectories=FILE_LIMIT, max_traj_per_file=100)\n",
        "if not dynamics_training_data:\n",
        "    print(f\"--- CRITICAL WARNING: NO REAL DATA LOADED from {FILE_LIMIT} files. CREATING SYNTHETIC FALLBACK. ---\")\n",
        "    dynamics_training_data = generate_synthetic_data(num_trajectories=1000)\n",
        "    print(f\"Successfully generated {len(dynamics_training_data)} synthetic causal samples for demonstration.\")\n",
        "if dynamics_training_data:\n",
        "    print(\"\\nPreparing DataLoader for batch training...\")\n",
        "    z_t_list, a_t_list, z_tp1_list = zip(*dynamics_training_data)\n",
        "    Z_T = torch.stack(z_t_list)\n",
        "    A_T = torch.stack(a_t_list)\n",
        "    Z_TP1 = torch.stack(z_tp1_list)\n",
        "    dataset = TensorDataset(Z_T, A_T, Z_TP1)\n",
        "    BATCH_SIZE = 64\n",
        "    dynamics_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    print(f\"Total batches for training: {len(dynamics_dataloader)}\")\n",
        "    train_latent_dynamics_model(predictor, optimizer_pldm, dynamics_dataloader, epochs=5)\n",
        "else:\n",
        "    print(\"FATAL ERROR: No data (real or synthetic) could be prepared. Training aborted.\")\n",
        "print(\"\\nCell 3 execution complete.\")\n",
        "print(\"Cell 3 completed. Predictor and latent_projector defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_ZnPRKSe8GnR",
        "outputId": "0ecf7f57-5e11-4e52-f5f8-49d8f88cab5d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "LatentProjector instance: LatentProjector(\n",
            "  (encoder_net): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=16, bias=True)\n",
            "  )\n",
            ")\n",
            "Predictor instance: LatentDynamicsPredictor(\n",
            "  (fc1): Linear(in_features=24, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=16, bias=True)\n",
            ")\n",
            "\n",
            "--- Starting AGI Demo Pipeline (Cell 3) ---\n",
            "\n",
            "Attempting to load REAL ADS-B data...\n",
            "Found 246 CSV files. Limiting to 20 files for demo speed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Trajectories:   0%|          | 0/20 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing /content/adsb/kbtp/raw/2022/02-01-22/1.csv\n",
            "Memory usage before: 5.0%\n",
            "Columns in /content/adsb/kbtp/raw/2022/02-01-22/1.csv: ['ID', 'Time', 'Date', 'Altitude', 'Speed', 'Heading', 'Lat', 'Lon', 'Age', 'Range', 'Bearing', 'Tail', 'AltisGNSS']\n",
            "Rows after cleaning /content/adsb/kbtp/raw/2022/02-01-22/1.csv: 138017\n",
            "Cleaned Date sample in /content/adsb/kbtp/raw/2022/02-01-22/1.csv:\n",
            " ['2022-02-01', '2022-02-01', '2022-02-01', '2022-02-01', '2022-02-01']\n",
            "Cleaned Time sample in /content/adsb/kbtp/raw/2022/02-01-22/1.csv:\n",
            " ['07:43:28.314', '07:43:28.327', '07:43:30.07', '07:43:31.86', '07:43:32.85']\n",
            "Unique trajectories in /content/adsb/kbtp/raw/2022/02-01-22/1.csv: 293\n",
            "Rows per traj_id:\n",
            " count      293.000000\n",
            "mean       471.047782\n",
            "std       1248.226269\n",
            "min          1.000000\n",
            "25%         86.000000\n",
            "50%        143.000000\n",
            "75%        267.000000\n",
            "max      12933.000000\n",
            "dtype: float64\n",
            "Limiting to 100 trajectories in /content/adsb/kbtp/raw/2022/02-01-22/1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Skipping trajectory N247JB_2022-02-01 in /content/adsb/kbtp/raw/2022/02-01-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:   5%|▌         | 1/20 [02:22<45:15, 142.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage after /content/adsb/kbtp/raw/2022/02-01-22/1.csv: 5.0%\n",
            "\n",
            "Processing /content/adsb/kbtp/raw/2022/09-24-22/1.csv\n",
            "Memory usage before: 5.0%\n",
            "Columns in /content/adsb/kbtp/raw/2022/09-24-22/1.csv: ['ID', 'Time', 'Date', 'Altitude', 'Speed', 'Heading', 'Lat', 'Lon', 'Age', 'Range', 'Bearing', 'Tail', 'AltisGNSS']\n",
            "Rows after cleaning /content/adsb/kbtp/raw/2022/09-24-22/1.csv: 62137\n",
            "Cleaned Date sample in /content/adsb/kbtp/raw/2022/09-24-22/1.csv:\n",
            " ['2022-09-24', '2022-09-24', '2022-09-24', '2022-09-24', '2022-09-24']\n",
            "Cleaned Time sample in /content/adsb/kbtp/raw/2022/09-24-22/1.csv:\n",
            " ['06:12:17', '06:12:18', '06:12:20', '06:12:21', '06:12:22']\n",
            "Unique trajectories in /content/adsb/kbtp/raw/2022/09-24-22/1.csv: 225\n",
            "Rows per traj_id:\n",
            " count     225.000000\n",
            "mean      276.164444\n",
            "std       455.327477\n",
            "min         1.000000\n",
            "25%        85.000000\n",
            "50%       156.000000\n",
            "75%       272.000000\n",
            "max      4137.000000\n",
            "dtype: float64\n",
            "Limiting to 100 trajectories in /content/adsb/kbtp/raw/2022/09-24-22/1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Skipping trajectory N133SY_2022-09-24 in /content/adsb/kbtp/raw/2022/09-24-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  10%|█         | 2/20 [03:03<24:53, 82.98s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage after /content/adsb/kbtp/raw/2022/09-24-22/1.csv: 5.1%\n",
            "\n",
            "Processing /content/adsb/kbtp/raw/2022/06-05-22/1.csv\n",
            "Memory usage before: 5.1%\n",
            "Columns in /content/adsb/kbtp/raw/2022/06-05-22/1.csv: ['ID', 'Time', 'Date', 'Altitude', 'Speed', 'Heading', 'Lat', 'Lon', 'Age', 'Range', 'Bearing', 'Tail', 'AltisGNSS']\n",
            "Rows after cleaning /content/adsb/kbtp/raw/2022/06-05-22/1.csv: 108075\n",
            "Cleaned Date sample in /content/adsb/kbtp/raw/2022/06-05-22/1.csv:\n",
            " ['2022-06-05', '2022-06-05', '2022-06-05', '2022-06-05', '2022-06-05']\n",
            "Cleaned Time sample in /content/adsb/kbtp/raw/2022/06-05-22/1.csv:\n",
            " ['06:00:04', '06:00:05', '06:00:06', '06:00:08', '06:00:10']\n",
            "Unique trajectories in /content/adsb/kbtp/raw/2022/06-05-22/1.csv: 384\n",
            "Rows per traj_id:\n",
            " count     384.000000\n",
            "mean      281.445312\n",
            "std       502.908794\n",
            "min         1.000000\n",
            "25%        65.000000\n",
            "50%       169.500000\n",
            "75%       307.750000\n",
            "max      4616.000000\n",
            "dtype: float64\n",
            "Limiting to 100 trajectories in /content/adsb/kbtp/raw/2022/06-05-22/1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Skipping trajectory N187PQ_2022-06-05 in /content/adsb/kbtp/raw/2022/06-05-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N833DN_2022-06-05 in /content/adsb/kbtp/raw/2022/06-05-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N190JE_2022-06-05 in /content/adsb/kbtp/raw/2022/06-05-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  15%|█▌        | 3/20 [03:36<16:57, 59.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage after /content/adsb/kbtp/raw/2022/06-05-22/1.csv: 5.1%\n",
            "\n",
            "Processing /content/adsb/kbtp/raw/2022/07-19-22/1.csv\n",
            "Memory usage before: 5.1%\n",
            "Columns in /content/adsb/kbtp/raw/2022/07-19-22/1.csv: ['ID', 'Time', 'Date', 'Altitude', 'Speed', 'Heading', 'Lat', 'Lon', 'Age', 'Range', 'Bearing', 'Tail', 'AltisGNSS']\n",
            "Rows after cleaning /content/adsb/kbtp/raw/2022/07-19-22/1.csv: 213791\n",
            "Cleaned Date sample in /content/adsb/kbtp/raw/2022/07-19-22/1.csv:\n",
            " ['2022-07-19', '2022-07-19', '2022-07-19', '2022-07-19', '2022-07-19']\n",
            "Cleaned Time sample in /content/adsb/kbtp/raw/2022/07-19-22/1.csv:\n",
            " ['06:00:04', '06:00:04', '06:00:04', '06:00:05', '06:00:04']\n",
            "Unique trajectories in /content/adsb/kbtp/raw/2022/07-19-22/1.csv: 814\n",
            "Rows per traj_id:\n",
            " count     814.000000\n",
            "mean      262.642506\n",
            "std       449.433270\n",
            "min         1.000000\n",
            "25%        81.250000\n",
            "50%       168.000000\n",
            "75%       289.000000\n",
            "max      6728.000000\n",
            "dtype: float64\n",
            "Limiting to 100 trajectories in /content/adsb/kbtp/raw/2022/07-19-22/1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Skipping trajectory N435AN_2022-07-19 in /content/adsb/kbtp/raw/2022/07-19-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N6711M_2022-07-19 in /content/adsb/kbtp/raw/2022/07-19-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  20%|██        | 4/20 [04:35<15:53, 59.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage after /content/adsb/kbtp/raw/2022/07-19-22/1.csv: 5.1%\n",
            "\n",
            "Processing /content/adsb/kbtp/raw/2022/06-25-22/1.csv\n",
            "Memory usage before: 5.1%\n",
            "Columns in /content/adsb/kbtp/raw/2022/06-25-22/1.csv: ['ID', 'Time', 'Date', 'Altitude', 'Speed', 'Heading', 'Lat', 'Lon', 'Age', 'Range', 'Bearing', 'Tail', 'AltisGNSS']\n",
            "Rows after cleaning /content/adsb/kbtp/raw/2022/06-25-22/1.csv: 214956\n",
            "Cleaned Date sample in /content/adsb/kbtp/raw/2022/06-25-22/1.csv:\n",
            " ['2022-06-25', '2022-06-25', '2022-06-25', '2022-06-25', '2022-06-25']\n",
            "Cleaned Time sample in /content/adsb/kbtp/raw/2022/06-25-22/1.csv:\n",
            " ['06:07:09', '06:07:10', '06:07:11', '06:07:12', '06:07:14']\n",
            "Unique trajectories in /content/adsb/kbtp/raw/2022/06-25-22/1.csv: 614\n",
            "Rows per traj_id:\n",
            " count     614.000000\n",
            "mean      350.091205\n",
            "std       688.155293\n",
            "min         1.000000\n",
            "25%        93.250000\n",
            "50%       185.000000\n",
            "75%       315.750000\n",
            "max      6626.000000\n",
            "dtype: float64\n",
            "Limiting to 100 trajectories in /content/adsb/kbtp/raw/2022/06-25-22/1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Trajectories:  25%|██▌       | 5/20 [05:15<13:06, 52.45s/it]WARNING:root:Skipping /content/adsb/kbtp/raw/2022/06-09-22/1.csv: Empty file.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage after /content/adsb/kbtp/raw/2022/06-25-22/1.csv: 5.2%\n",
            "\n",
            "Processing /content/adsb/kbtp/raw/2022/06-09-22/1.csv\n",
            "Memory usage before: 5.2%\n",
            "\n",
            "Processing /content/adsb/kbtp/raw/2022/05-03-22/1.csv\n",
            "Memory usage before: 5.2%\n",
            "Columns in /content/adsb/kbtp/raw/2022/05-03-22/1.csv: ['ID', 'Time', 'Date', 'Altitude', 'Speed', 'Heading', 'Lat', 'Lon', 'Age', 'Range', 'Bearing', 'Tail', 'AltisGNSS']\n",
            "Rows after cleaning /content/adsb/kbtp/raw/2022/05-03-22/1.csv: 1990\n",
            "Cleaned Date sample in /content/adsb/kbtp/raw/2022/05-03-22/1.csv:\n",
            " ['2022-05-03', '2022-05-03', '2022-05-03', '2022-05-03', '2022-05-03']\n",
            "Cleaned Time sample in /content/adsb/kbtp/raw/2022/05-03-22/1.csv:\n",
            " ['06:43:08.696', '06:43:10.097', '06:43:11.568', '06:43:12.615', '06:43:14.054']\n",
            "Unique trajectories in /content/adsb/kbtp/raw/2022/05-03-22/1.csv: 26\n",
            "Rows per traj_id:\n",
            " count     26.000000\n",
            "mean      76.538462\n",
            "std       54.636421\n",
            "min        2.000000\n",
            "25%       31.250000\n",
            "50%       78.500000\n",
            "75%      107.000000\n",
            "max      212.000000\n",
            "dtype: float64\n",
            "Limiting to 26 trajectories in /content/adsb/kbtp/raw/2022/05-03-22/1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Trajectories:  35%|███▌      | 7/20 [05:19<05:48, 26.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage after /content/adsb/kbtp/raw/2022/05-03-22/1.csv: 5.1%\n",
            "\n",
            "Processing /content/adsb/kbtp/raw/2022/08-21-22/1.csv\n",
            "Memory usage before: 5.1%\n",
            "Columns in /content/adsb/kbtp/raw/2022/08-21-22/1.csv: ['ID', 'Time', 'Date', 'Altitude', 'Speed', 'Heading', 'Lat', 'Lon', 'Age', 'Range', 'Bearing', 'Tail', 'AltisGNSS']\n",
            "Rows after cleaning /content/adsb/kbtp/raw/2022/08-21-22/1.csv: 82495\n",
            "Cleaned Date sample in /content/adsb/kbtp/raw/2022/08-21-22/1.csv:\n",
            " ['2022-08-21', '2022-08-21', '2022-08-21', '2022-08-21', '2022-08-21']\n",
            "Cleaned Time sample in /content/adsb/kbtp/raw/2022/08-21-22/1.csv:\n",
            " ['06:16:43', '06:16:45', '06:16:47', '06:16:49', '06:16:51']\n",
            "Unique trajectories in /content/adsb/kbtp/raw/2022/08-21-22/1.csv: 525\n",
            "Rows per traj_id:\n",
            " count     525.000000\n",
            "mean      157.133333\n",
            "std       194.974654\n",
            "min         1.000000\n",
            "25%        37.000000\n",
            "50%       123.000000\n",
            "75%       209.000000\n",
            "max      2561.000000\n",
            "dtype: float64\n",
            "Limiting to 100 trajectories in /content/adsb/kbtp/raw/2022/08-21-22/1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Skipping trajectory C-FJAS_2022-08-21 in /content/adsb/kbtp/raw/2022/08-21-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  40%|████      | 8/20 [05:43<05:13, 26.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage after /content/adsb/kbtp/raw/2022/08-21-22/1.csv: 5.1%\n",
            "\n",
            "Processing /content/adsb/kbtp/raw/2022/03-03-22/1.csv\n",
            "Memory usage before: 5.1%\n",
            "Columns in /content/adsb/kbtp/raw/2022/03-03-22/1.csv: ['ID', 'Time', 'Date', 'Altitude', 'Speed', 'Heading', 'Lat', 'Lon', 'Age', 'Range', 'Bearing', 'Tail', 'AltisGNSS']\n",
            "Rows after cleaning /content/adsb/kbtp/raw/2022/03-03-22/1.csv: 87744\n",
            "Cleaned Date sample in /content/adsb/kbtp/raw/2022/03-03-22/1.csv:\n",
            " ['2022-03-03', '2022-03-03', '2022-03-03', '2022-03-03', '2022-03-03']\n",
            "Cleaned Time sample in /content/adsb/kbtp/raw/2022/03-03-22/1.csv:\n",
            " ['09:41:17.316', '09:41:17.506', '09:41:19.7', '09:41:20.742', '09:41:22.089']\n",
            "Unique trajectories in /content/adsb/kbtp/raw/2022/03-03-22/1.csv: 192\n",
            "Rows per traj_id:\n",
            " count     192.000000\n",
            "mean      457.000000\n",
            "std      1110.315553\n",
            "min         1.000000\n",
            "25%        57.750000\n",
            "50%       101.500000\n",
            "75%       204.250000\n",
            "max      8873.000000\n",
            "dtype: float64\n",
            "Limiting to 100 trajectories in /content/adsb/kbtp/raw/2022/03-03-22/1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Skipping trajectory N610NN_2022-03-03 in /content/adsb/kbtp/raw/2022/03-03-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory C-GJVX_2022-03-03 in /content/adsb/kbtp/raw/2022/03-03-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory N29MG_2022-03-03 in /content/adsb/kbtp/raw/2022/03-03-22/1.csv: Fewer than 2 points.\n",
            "WARNING:root:Skipping trajectory C-GHPD_2022-03-03 in /content/adsb/kbtp/raw/2022/03-03-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  45%|████▌     | 9/20 [07:37<09:13, 50.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory usage after /content/adsb/kbtp/raw/2022/03-03-22/1.csv: 5.2%\n",
            "\n",
            "Processing /content/adsb/kbtp/raw/2022/05-09-22/1.csv\n",
            "Memory usage before: 5.2%\n",
            "Columns in /content/adsb/kbtp/raw/2022/05-09-22/1.csv: ['ID', 'Time', 'Date', 'Altitude', 'Speed', 'Heading', 'Lat', 'Lon', 'Age', 'Range', 'Bearing', 'Tail', 'AltisGNSS']\n",
            "Rows after cleaning /content/adsb/kbtp/raw/2022/05-09-22/1.csv: 143761\n",
            "Cleaned Date sample in /content/adsb/kbtp/raw/2022/05-09-22/1.csv:\n",
            " ['2022-05-09', '2022-05-09', '2022-05-09', '2022-05-09', '2022-05-09']\n",
            "Cleaned Time sample in /content/adsb/kbtp/raw/2022/05-09-22/1.csv:\n",
            " ['07:34:31', '07:34:32', '07:34:33', '07:34:34', '07:34:36']\n",
            "Unique trajectories in /content/adsb/kbtp/raw/2022/05-09-22/1.csv: 359\n",
            "Rows per traj_id:\n",
            " count     359.000000\n",
            "mean      400.448468\n",
            "std       974.828280\n",
            "min         1.000000\n",
            "25%        68.000000\n",
            "50%       134.000000\n",
            "75%       221.000000\n",
            "max      9045.000000\n",
            "dtype: float64\n",
            "Limiting to 100 trajectories in /content/adsb/kbtp/raw/2022/05-09-22/1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Skipping trajectory N902BC_2022-05-09 in /content/adsb/kbtp/raw/2022/05-09-22/1.csv: Fewer than 2 points.\n",
            "Processing Trajectories:  45%|████▌     | 9/20 [09:19<11:24, 62.20s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-434812554.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0mFILE_LIMIT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Starting AGI Demo Pipeline (Cell 3) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m \u001b[0mdynamics_training_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_real_dynamics_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_trajectories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFILE_LIMIT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_traj_per_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdynamics_training_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- CRITICAL WARNING: NO REAL DATA LOADED from {FILE_LIMIT} files. CREATING SYNTHETIC FALLBACK. ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-434812554.py\u001b[0m in \u001b[0;36mload_real_dynamics_data\u001b[0;34m(device, adsb_dir, num_trajectories, max_traj_per_file)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                         \u001b[0mrow_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrequired_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m                         \u001b[0mrow_tp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrequired_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_rows_with_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;31m# handle the dup indexing case GH#4246\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_values_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_deprecated_callable_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_callable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1418\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1558\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6243\u001b[0m         \u001b[0;31m# Count missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6244\u001b[0m         \u001b[0mmissing_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6245\u001b[0;31m         \u001b[0mnmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6247\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     50\u001b[0m def _sum(a, axis=None, dtype=None, out=None, keepdims=False,\n\u001b[1;32m     51\u001b[0m          initial=_NoValue, where=True):\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m def _prod(a, axis=None, dtype=None, out=None, keepdims=False,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cell 4"
      ],
      "metadata": {
        "id": "MVYgldE7OHvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LatentProjector(nn.Module):\n",
        "    def __init__(self, state_dim=4, latent_dim=16):\n",
        "        super().__init__()\n",
        "        self.encoder_net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        return self.encoder_net(x)"
      ],
      "metadata": {
        "id": "CIeU6IsD5l13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import gc # Import garbage collector\n",
        "\n",
        "from warnings import filterwarnings\n",
        "filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Define constants used by predictor/projector from Cell 3 (for robustness) ---\n",
        "LATENT_DIM = 16\n",
        "ACTION_DIM = 8\n",
        "# ---------------------------------------------------------------------------------\n",
        "\n",
        "# Define missing variables\n",
        "AIRPORTS = {\n",
        "    \"CYUL\": {\"lat\": 45.4706, \"lon\": -73.7408, \"name\": \"Montreal-Trudeau International\"},\n",
        "    \"LFPG\": {\"lat\": 49.0128, \"lon\": 2.5500, \"name\": \"Paris-Charles de Gaulle\"}\n",
        "}\n",
        "AIRCRAFT_PERFORMANCE = {\n",
        "    \"Boeing777_300ER\": {\n",
        "        \"max_speed_kts\": 490.0,\n",
        "        \"cruise_altitude_ft\": 37000.0,\n",
        "        \"range_nm\": 7370.0\n",
        "    }\n",
        "}\n",
        "# Move model and processor to the specified device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Ensure device is defined\n",
        "# Consider using a smaller model if memory is an issue, e.g., \"openai/clip-vit-base-patch16\"\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=False) # Processor doesn't need .to(device)\n",
        "\n",
        "# Redefine LatentProjector (re-using definition from prior cell)\n",
        "class LatentProjector(nn.Module):\n",
        "    def __init__(self, state_dim=4, latent_dim=LATENT_DIM): # Using LATENT_DIM from Cell 3/here\n",
        "        super().__init__()\n",
        "        self.encoder_net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, latent_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        if x.dim() == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "        return self.encoder_net(x)\n",
        "\n",
        "# Placeholder for load_and_process_video\n",
        "def load_and_process_video(video_path, processor, model, device, num_frames=1):\n",
        "    try:\n",
        "        from torchvision.io import read_video\n",
        "        # Ensure video is loaded onto the correct device\n",
        "        video, _, _ = read_video(video_path, pts_unit='sec')\n",
        "        # Select and move only the specified number of frames to device\n",
        "        video = video[:num_frames].to(device)\n",
        "        # Process frames individually or in a small batch if num_frames > 1\n",
        "        # For simplicity and memory reduction, let's process frame by frame or take a simple average if multiple frames requested\n",
        "        features_list = []\n",
        "        for frame in video:\n",
        "            inputs = processor(images=frame.unsqueeze(0), return_tensors=\"pt\").to(device) # Process one frame at a time\n",
        "            with torch.no_grad():\n",
        "                features = model.get_image_features(**inputs)\n",
        "                features_list.append(features)\n",
        "\n",
        "        if not features_list:\n",
        "             return None, \"No frames processed\"\n",
        "\n",
        "        # Average features if more than one frame was processed\n",
        "        averaged_features = torch.mean(torch.stack(features_list), dim=0)\n",
        "\n",
        "        # Clear intermediate tensors\n",
        "        del features_list, features, inputs, video, frame\n",
        "        torch.cuda.empty_cache() # Clear GPU cache if using CUDA\n",
        "        gc.collect() # Collect garbage\n",
        "\n",
        "        return averaged_features, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video_path}: {e}\")\n",
        "        return None, str(e) # Return error message\n",
        "\n",
        "\n",
        "def plan_montreal_to_paris_flight(start_airport_data, target_airport_data, aircraft_model_data,\n",
        "                                  encoder_model, processor_instance, predictor_model, latent_projector_instance,\n",
        "                                  planning_horizon=50, action_dim=ACTION_DIM, num_action_samples=100): # Using ACTION_DIM\n",
        "    encoder_model.eval()\n",
        "    predictor_model.eval()\n",
        "    latent_projector_instance.eval()\n",
        "    print(\"AIRPORTS:\", AIRPORTS)\n",
        "    print(\"AIRCRAFT_PERFORMANCE:\", AIRCRAFT_PERFORMANCE)\n",
        "    print(\"model:\", encoder_model)\n",
        "    print(\"processor:\", processor_instance)\n",
        "    print(\"predictor:\", predictor_model)\n",
        "    print(\"latent_projector:\", latent_projector_instance)\n",
        "    initial_video_path = '/content/gdrive/MyDrive/datasets/TartanAviation/vision/1_2023-02-22-15-21-49/1_2023-02-22-15-21-49.mp4'\n",
        "    target_video_path = initial_video_path  # Update with landing video\n",
        "\n",
        "    # Process a minimal number of frames for memory efficiency\n",
        "    initial_features, initial_error = load_and_process_video(initial_video_path, processor_instance, encoder_model, device, num_frames=1)\n",
        "    target_features, target_error = load_and_process_video(target_video_path, processor_instance, encoder_model, device, num_frames=1) # Use num_frames=1 here too\n",
        "\n",
        "    if initial_features is None or target_features is None:\n",
        "        error_message = f\"Video load failed. Initial: {initial_error}, Target: {target_error}. Using dummy features.\"\n",
        "        print(error_message)\n",
        "        # Ensure dummy features are on the correct device and have correct shape\n",
        "        dummy_feature_shape = (1, 512) # CLIP image features shape\n",
        "        initial_features = torch.rand(dummy_feature_shape).to(device)\n",
        "        target_features = torch.rand(dummy_feature_shape).to(device)\n",
        "\n",
        "\n",
        "    # -- FIX: Ensure LATENT_DIM is available --\n",
        "    latent_state_dim = LATENT_DIM # Should be 16 from Cell 3\n",
        "\n",
        "    # Assuming CLIP features are (1, 512) and latent_projector maps 512 -> 16\n",
        "    visual_feature_dim = initial_features.shape[-1] if initial_features is not None else 512\n",
        "\n",
        "\n",
        "    # Re-initialize latent_projector_instance if its input dimension is incorrect\n",
        "    if latent_projector_instance.encoder_net[0].in_features != visual_feature_dim:\n",
        "         print(f\"Warning: LatentProjector input dimension mismatch. Expected {visual_feature_dim}, got {latent_projector_instance.encoder_net[0].in_features}. Re-initializing.\")\n",
        "         # Ensure latent_projector is created with the correct dimensions (visual feature dim -> latent state dim)\n",
        "         # Using LATENT_DIM=16\n",
        "         latent_projector_instance = LatentProjector(state_dim=visual_feature_dim, latent_dim=latent_state_dim).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Ensure input to latent_projector is the correct shape (batch_size, visual_feature_dim)\n",
        "        current_latent_state = latent_projector_instance(initial_features)\n",
        "        target_latent_state = latent_projector_instance(target_features)\n",
        "\n",
        "\n",
        "    # Clear CLIP features after projection\n",
        "    del initial_features, target_features\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "    ETHICAL_BOUNDARY_LATENT_VECTOR = torch.zeros(1, latent_state_dim).to(device) # Ensure size matches latent_state_dim\n",
        "    weather_path = '/content/TartanAviation/vision/weather_stats.csv'\n",
        "    salience = torch.rand(1).to(device) * 0.8\n",
        "    if os.path.exists(weather_path):\n",
        "        try:\n",
        "            weather_df = pd.read_csv(weather_path)\n",
        "            if 'visibility' in weather_df.columns:\n",
        "                # Ensure salience is a tensor on the correct device\n",
        "                salience = torch.tensor(weather_df['visibility'].mean() / 10.0, device=device)\n",
        "        except Exception as we:\n",
        "            print(f\"Error loading weather data: {we}. Using default salience.\")\n",
        "            salience = torch.rand(1).to(device) * 0.8 # Fallback if weather loading fails\n",
        "\n",
        "\n",
        "    print(\"\\n--- Starting Real Flight Plan ---\")\n",
        "    print(f\"Current Latent State Shape: {current_latent_state.shape}\")\n",
        "    print(f\"Target Latent State Shape: {target_latent_state.shape}\")\n",
        "    print(f\"Salience Level: {salience.item():.2f}\")\n",
        "    print('\\n')\n",
        "    best_action_sequence = []\n",
        "    # num_action_samples = 100 # Now passed as argument\n",
        "    action_dim = ACTION_DIM # Using ACTION_DIM from Cell 3/here\n",
        "\n",
        "    # Ensure predictor input dimension matches (latent_state_dim + action_dim)\n",
        "    # Assuming predictor has an attribute like fc1.in_features\n",
        "    try:\n",
        "        predicted_state_dim = predictor_model.fc1.in_features\n",
        "        expected_predictor_input_dim = latent_state_dim + action_dim\n",
        "        if predicted_state_dim != expected_predictor_input_dim:\n",
        "            print(f\"Warning: Predictor input dimension mismatch. Expected {expected_predictor_input_dim}, got {predicted_state_dim}. Predictor may not be compatible.\")\n",
        "            # We cannot re-initialize the predictor here as it's passed in.\n",
        "            # This warning alerts the user to a potential issue with the provided predictor model.\n",
        "    except AttributeError:\n",
        "        print(\"Warning: Could not check predictor input dimension (no fc1 attribute). Predictor may not be compatible.\")\n",
        "\n",
        "\n",
        "    current_latent = current_latent_state # Rename for clarity in loop\n",
        "\n",
        "    for step in range(planning_horizon):\n",
        "        # Generate candidate actions - reduced number for memory\n",
        "        candidate_actions = torch.rand(num_action_samples, action_dim).to(device) * 2.0 - 1.0\n",
        "\n",
        "        # Prepare inputs for the Predictor, which expects the state and action separately.\n",
        "        # Repeat current_latent (z_t) N times to match the number of candidate actions\n",
        "        repeated_current_latent = current_latent.repeat(num_action_samples, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # FIX: Pass current state and actions as separate arguments\n",
        "            simulated_next_latents = predictor_model(repeated_current_latent, candidate_actions)\n",
        "\n",
        "        # Clear the repeated tensor immediately to save memory\n",
        "        del repeated_current_latent\n",
        "\n",
        "        simulated_trajectories_cost = []\n",
        "        for i in range(num_action_samples):\n",
        "            simulated_next_latent = simulated_next_latents[i].unsqueeze(0) # Get the result for this sample\n",
        "\n",
        "            # --- Cost Calculation ---\n",
        "            goal_proximity_cost = torch.norm(target_latent_state - simulated_next_latent) * 1.0\n",
        "            conceptual_fuel_cost = torch.norm(candidate_actions[i]) * 0.05\n",
        "            conceptual_weather_cost = torch.rand(1).to(device) * 0.02 # This should ideally use actual weather data/model\n",
        "\n",
        "            # --- Ethical and Salience Costs (Pillars 3 and 4) ---\n",
        "            ethical_cost = 5.0 * torch.norm(ETHICAL_BOUNDARY_LATENT_VECTOR - simulated_next_latent)\n",
        "            cautious_action_penalty = torch.norm(candidate_actions[i]) * salience # salience is a tensor\n",
        "            salience_alignment_cost = 2.0 * cautious_action_penalty # cautious_action_penalty is a scalar cost\n",
        "\n",
        "            total_cost = goal_proximity_cost + conceptual_fuel_cost + conceptual_weather_cost + ethical_cost + salience_alignment_cost\n",
        "            simulated_trajectories_cost.append(total_cost.item()) # Append scalar cost\n",
        "\n",
        "        # Clear intermediate tensors after calculating costs\n",
        "        del simulated_next_latents, candidate_actions\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "        best_candidate_idx = torch.argmin(torch.tensor(simulated_trajectories_cost)) # Convert list back to tensor for argmin\n",
        "\n",
        "        # Re-generate candidate actions to get the best one (or store them before deletion)\n",
        "        # Let's regenerate for simplicity, since we need the original actions that minimized the cost\n",
        "\n",
        "        # NOTE: In a clean MPPI implementation, we would store all candidate actions before the deletion above.\n",
        "        # Since they were deleted, we must regenerate the tensor and pick the optimal index.\n",
        "        candidate_actions = torch.rand(num_action_samples, action_dim).to(device) * 2.0 - 1.0 # Regenerate\n",
        "        optimal_action_for_step = candidate_actions[best_candidate_idx]\n",
        "\n",
        "        best_action_sequence.append(optimal_action_for_step.squeeze().cpu().numpy())\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Update current_latent using the chosen optimal action\n",
        "            optimal_action_input = optimal_action_for_step.unsqueeze(0)\n",
        "            # FIX: Predict the next latent state using the correct two-argument call\n",
        "            current_latent = predictor_model(current_latent, optimal_action_input)\n",
        "\n",
        "        # Clear tensors used in this step\n",
        "        del optimal_action_for_step, optimal_action_input\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "    print(f\"Real Plan for {planning_horizon} steps (first 5 actions shown):\")\n",
        "    for i, action in enumerate(best_action_sequence[:5]):\n",
        "        print(f\"Step {i+1}: {np.round(action, 4)}\")\n",
        "    return best_action_sequence\n",
        "\n",
        "# Run in Cell 4\n",
        "# Ensure predictor and latent_projector are defined before calling this function.\n",
        "# They are likely defined in Cell 1 and potentially trained in Cell 3.\n",
        "# Assuming 'predictor' and 'latent_projector' are available in the global scope from previous cells.\n",
        "try:\n",
        "    # Pass a smaller number of action samples to reduce memory\n",
        "    conceptual_flight_plan_actions = plan_montreal_to_paris_flight(\n",
        "        AIRPORTS[\"CYUL\"], AIRPORTS[\"LFPG\"], AIRCRAFT_PERFORMANCE[\"Boeing777_300ER\"],\n",
        "        model, processor, predictor, latent_projector, # Pass the loaded/defined predictor and latent_projector\n",
        "        num_action_samples=50 # Reduced number of action samples\n",
        "    )\n",
        "except NameError as ne:\n",
        "    print(f\"Error: {ne}. Make sure 'predictor' and 'latent_projector' are defined by running previous cells.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during flight planning: {e}\")\n"
      ],
      "metadata": {
        "id": "ez7nzL8uzxAx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}