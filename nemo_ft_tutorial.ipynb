{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5BmbuzMW5RGL"
      ],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOWNYPExdLJIuEBk9h1+pGl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/nemo_ft_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y graphviz\n",
        "!pip install ipywidgets\n",
        "!pip install --upgrade setuptools wheel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdBimKQM3JwQ",
        "outputId": "9ba96a21-ea90-4e7d-ad4f-2f7c578114b7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Wai\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Con\r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,892 kB]\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,711 kB]\n",
            "Fetched 13.0 MB in 2s (5,444 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "graphviz is already the newest version (2.42.2-6ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 119 not upgraded.\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.12/dist-packages (7.7.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ipywidgets) (3.0.16)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.2.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (7.2.2)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.5.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (81.0.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.9.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.9.0.post0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.17.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.1.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.24.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.5.3)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.5.1)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.5)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.3.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.3)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.2.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.12/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.30.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.12/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.14.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: jupyter-events>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.12.0)\n",
            "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.4)\n",
            "Requirement already satisfied: overrides>=5.0 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.7.0)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.12/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.0.3)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.1.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.1.0)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.10.0)\n",
            "Requirement already satisfied: lark>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2025.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (81.0.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.46.3)\n",
            "Requirement already satisfied: packaging>=24.0 in /usr/local/lib/python3.12/dist-packages (from wheel) (24.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nemo_toolkit[all]==2.6.1 -q"
      ],
      "metadata": {
        "id": "xHLZXcYEvofI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge\n",
        "!pip install --no-build-isolation transformer-engine[pytorch] -q\n",
        "!pip install nemo_run opendatasets pandas bitsandbytes accelerate -q\n",
        "!pip install --upgrade transformers -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "mlhoPjJN3SWQ",
        "outputId": "42f4419b-1f1c-4620-98d0-2461912e7821"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files removed: 1323\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2181597478.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip cache purge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install --no-build-isolation transformer-engine[pytorch] -q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install nemo_run opendatasets pandas bitsandbytes accelerate -q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install --upgrade transformers -q'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m       \u001b[0m_pip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_send_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36mprint_previous_import_warning\u001b[0;34m(output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_previous_import_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;34m\"\"\"Prints a warning about previously imported packages.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0mpackages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpackages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# display a list of packages using the colab-display-data mimetype, which\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_previously_imported_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_previously_imported_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0;34m\"\"\"List all previously imported packages from a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0minstalled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_extract_toplevel_packages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpip_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstalled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_pip.py\u001b[0m in \u001b[0;36m_extract_toplevel_packages\u001b[0;34m(pip_output)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;34m\"\"\"Extract the list of toplevel packages associated with a pip install.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtoplevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages_distributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mtoplevel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[0mpkg_to_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_top_level_declared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_top_level_inferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m             \u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkg_to_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/metadata/__init__.py\u001b[0m in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    957\u001b[0m     opt_names = {\n\u001b[1;32m    958\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmodulename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malways_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m     }\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mfiles\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         return skip_missing_files(\n\u001b[0m\u001b[1;32m    605\u001b[0m             make_files(\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_files_distinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/_functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mskip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mpass_none\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mskip_missing_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         return skip_missing_files(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/importlib_metadata/__init__.py\u001b[0m in \u001b[0;36mmake_file\u001b[0;34m(name, hash, size_str)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mmake_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackagePath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileHash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhash\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhash\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msize_str\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/pathlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m                     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0\" --force-reinstall"
      ],
      "metadata": {
        "id": "9eDsW8fZ3TcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Login to Hugging Face\n",
        "login(token=userdata.get(\"HF_TOKEN\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoDh65T34CM_",
        "outputId": "8b768b16-2532-453d-e5a9-f62b7be8d92b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n",
            "WARNING:huggingface_hub._login:Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import nemo_run as run\n",
        "from nemo import lightning as nl\n",
        "from nemo.collections import llm\n",
        "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiyatS134IV9",
        "outputId": "9838683d-34a4-41f2-ed6c-94222e5a3900"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
            "  import pynvml  # type: ignore[import]\n",
            "[NeMo W 2026-02-07 11:32:30 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/transformer_engine/__init__.py:59: RuntimeWarning: Detected a Jax installation but could not find the shared object file for the Transformer Engine Jax extension library. If this is not intentional, please reinstall Transformer Engine with `pip install transformer_engine[jax]` or build from source with `NVTE_FRAMEWORK=jax`.\n",
            "      warnings.warn(\n",
            "    \n",
            "WARNING:megatron.core.utils:fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.\n",
            "WARNING:nv_one_logger.api.config:OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
            "WARNING:nv_one_logger.training_telemetry.api.training_telemetry_provider:No exporters were provided. This means that no telemetry data will be collected.\n",
            "[NeMo W 2026-02-07 11:32:48 nemo_logging:405] The deploy module could not be imported: cannot import name 'deploy' from 'nemo.collections.llm.api' (/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/api.py)\n",
            "[NeMo W 2026-02-07 11:32:48 nemo_logging:405] The evaluate module could not be imported: cannot import name 'evaluate' from 'nemo.collections.llm.api' (/usr/local/lib/python3.12/dist-packages/nemo/collections/llm/api.py)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nemo_run as run\n",
        "from nemo.collections import llm\n",
        "import nemo as ne\n",
        "from nemo import lightning as nl\n",
        "import transformer_engine as te\n",
        "import transformers as tr\n",
        "\n",
        "\n",
        "print(f\"Nemo version: {ne.__version__}\")\n",
        "print(f\"NeMo RUN version: {run.__version__}\")\n",
        "print(f\"Transformer Engine version: {te.__version__}\")\n",
        "print(f\"Transformers version: {tr.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw_mY8hF32sC",
        "outputId": "6a3b82e9-1b0a-4d83-bd0a-6f352a819b35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nemo version: 2.6.1\n",
            "NeMo RUN version: 0.7.0\n",
            "Transformer Engine version: 2.11.0\n",
            "Transformers version: 4.53.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/*"
      ],
      "metadata": {
        "id": "6BGcD9IcYI8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YwEIFHv4YTXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/*"
      ],
      "metadata": {
        "id": "w6P1z5oheagN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import tarfile\n",
        "import dataclasses\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 1. AUTHENTICATION & DIRECTORY SETUP\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "MODEL_SOURCE = \"meta-llama/Meta-Llama-3-8B\"\n",
        "COLAB_BASE = \"/content/nemo_llama3_final\"\n",
        "NEMO_FILE = f\"{COLAB_BASE}/llama3_8b_fixed.nemo\"\n",
        "WORKSPACE = f\"{COLAB_BASE}/workspace\"\n",
        "\n",
        "os.makedirs(WORKSPACE, exist_ok=True)\n",
        "\n",
        "# 2. ARCHITECTURE CONFIGURATION\n",
        "from nemo.collections.llm.gpt.model.llama import Llama3Config8B\n",
        "config = Llama3Config8B(seq_length=512, bf16=True)\n",
        "\n",
        "def clean_nemo_config(cfg):\n",
        "    \"\"\"Converts dataclass to JSON-serializable dict.\"\"\"\n",
        "    c = dataclasses.asdict(cfg)\n",
        "    return {k: (v if isinstance(v, (str, int, float, bool, list, dict)) or v is None\n",
        "            else str(v).split('.')[-1]) for k, v in c.items()}\n",
        "\n",
        "# 3. ROBUST .NEMO CREATION\n",
        "if not os.path.exists(NEMO_FILE):\n",
        "    print(\"ðŸš€ Building .nemo file with standard tokenizer mapping...\")\n",
        "\n",
        "    # Save Weights\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(MODEL_SOURCE, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
        "    weights_path = os.path.join(WORKSPACE, \"weights\")\n",
        "    os.makedirs(weights_path, exist_ok=True)\n",
        "    torch.save(hf_model.state_dict(), os.path.join(weights_path, \"common.pt\"))\n",
        "\n",
        "    # Save Tokenizer to a dedicated sub-folder\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_SOURCE)\n",
        "    tokenizer_dir = os.path.join(WORKSPACE, \"tokenizer\")\n",
        "    os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "    tokenizer.save_pretrained(tokenizer_dir)\n",
        "\n",
        "    # Create NeMo context metadata (io.json)\n",
        "    # NOTE: We use the HuggingFace AutoTokenizer target directly for NeMo 2.6 compatibility.\n",
        "    io_json_path = os.path.join(WORKSPACE, \"context\", \"io.json\")\n",
        "    os.makedirs(os.path.dirname(io_json_path), exist_ok=True)\n",
        "    with open(io_json_path, 'w') as f:\n",
        "        json.dump({\n",
        "            \"_target_\": \"nemo.collections.llm.gpt.model.llama.LlamaModel\",\n",
        "            \"config\": clean_nemo_config(config),\n",
        "            \"tokenizer\": {\n",
        "                \"_target_\": \"nemo.collections.common.tokenizers.huggingface.AutoTokenizer\",\n",
        "                \"pretrained_model_name_or_path\": \"model/tokenizer\" # Path relative to .nemo root\n",
        "            }\n",
        "        }, f, indent=2)\n",
        "\n",
        "    # Package into .nemo\n",
        "    with tarfile.open(NEMO_FILE, \"w:gz\") as tar:\n",
        "        for root, _, files in os.walk(WORKSPACE):\n",
        "            for file in files:\n",
        "                full_path = os.path.join(root, file)\n",
        "                # Map the local workspace contents into the \"model/\" prefix inside the archive\n",
        "                tar.add(full_path, arcname=os.path.join(\"model\", os.path.relpath(full_path, WORKSPACE)))\n",
        "    print(f\"âœ… Created {NEMO_FILE}\")\n",
        "\n",
        "# 4. EXECUTE TRAINING WITH FIXED RESTORATION\n",
        "import nemo_run as run\n",
        "from nemo import lightning as nl\n",
        "from nemo.collections import llm\n",
        "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
        "\n",
        "print(\"\\n>>> Starting LoRA Fine-Tuning with Validated Tokenizer...\")\n",
        "recipe = run.Partial(\n",
        "    llm.finetune,\n",
        "    model=run.Config(llm.LlamaModel, config=run.Config(llm.Llama3Config8B)),\n",
        "    data=run.Config(llm.SquadDataModule, seq_length=512, micro_batch_size=1, global_batch_size=4),\n",
        "    trainer=run.Config(\n",
        "        nl.Trainer, devices=1, max_steps=20, accelerator=\"gpu\",\n",
        "        strategy=run.Config(nl.MegatronStrategy, tensor_model_parallel_size=1),\n",
        "        plugins=bf16_mixed()\n",
        "    ),\n",
        "    peft=run.Config(llm.peft.LoRA, target_modules=['linear_qkv']),\n",
        "    resume=run.Config(\n",
        "        nl.AutoResume,\n",
        "        restore_config=run.Config(nl.RestoreConfig, path=NEMO_FILE),\n",
        "        resume_if_exists=True\n",
        "    )\n",
        ")\n",
        "\n",
        "run.run(recipe, executor=run.LocalExecutor(ntasks_per_node=1, launcher=\"torchrun\"))"
      ],
      "metadata": {
        "id": "pnfQRy9bee9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 2"
      ],
      "metadata": {
        "id": "RQb1PmHQN5KO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import tarfile\n",
        "import dataclasses\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 1. SETUP ENVIRONMENT & AUTHENTICATION\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "MODEL_SOURCE = \"meta-llama/Meta-Llama-3-8B\"\n",
        "COLAB_BASE = \"/content/nemo_llama3_manual\"\n",
        "NEMO_FILE = f\"{COLAB_BASE}/llama3_8b_manual.nemo\"\n",
        "WORKSPACE = f\"{COLAB_BASE}/workspace\"\n",
        "TRAIN_DATA = f\"{COLAB_BASE}/toy_train.jsonl\"\n",
        "os.makedirs(WORKSPACE, exist_ok=True)\n",
        "\n",
        "# --- STEP 0: CREATE DATASET (FIX) ---\n",
        "# This ensures TRAIN_DATA exists before the loop starts\n",
        "if not os.path.exists(TRAIN_DATA):\n",
        "    print(f\"ðŸ“Š Creating {TRAIN_DATA}...\")\n",
        "    samples = [\n",
        "        {\"input\": \"Context: NeMo is a toolkit. Question: What is NeMo? Answer: A toolkit\"},\n",
        "        {\"input\": \"Context: Llama 3 is a model. Question: What is Llama 3? Answer: A model\"}\n",
        "    ]\n",
        "    with open(TRAIN_DATA, \"w\") as f:\n",
        "        for s in samples:\n",
        "            f.write(json.dumps(s) + \"\\n\")\n",
        "\n",
        "# 2. ARCHITECTURE CONFIGURATION\n",
        "from nemo.collections.llm.gpt.model.llama import Llama3Config8B\n",
        "config = Llama3Config8B(seq_length=512, bf16=True)\n",
        "\n",
        "# 3. MANUAL .NEMO CREATION (Reference Step 3)\n",
        "if not os.path.exists(NEMO_FILE):\n",
        "    print(f\"ðŸš€ Building {NEMO_FILE}...\")\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(MODEL_SOURCE, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
        "    weights_path = os.path.join(WORKSPACE, \"weights\")\n",
        "    os.makedirs(weights_path, exist_ok=True)\n",
        "    torch.save(hf_model.state_dict(), os.path.join(weights_path, \"common.pt\"))\n",
        "\n",
        "    def clean_nemo_config(cfg):\n",
        "        c = dataclasses.asdict(cfg)\n",
        "        return {k: (v if isinstance(v, (str, int, float, bool, list, dict)) or v is None\n",
        "                else str(v).split('.')[-1]) for k, v in c.items()}\n",
        "\n",
        "    io_json_path = os.path.join(WORKSPACE, \"context\", \"io.json\")\n",
        "    os.makedirs(os.path.dirname(io_json_path), exist_ok=True)\n",
        "    with open(io_json_path, 'w') as f:\n",
        "        json.dump({\"_target_\": \"nemo.collections.llm.gpt.model.llama.LlamaModel\", \"config\": clean_nemo_config(config)}, f, indent=2)\n",
        "\n",
        "    with tarfile.open(NEMO_FILE, \"w:gz\") as tar:\n",
        "        for root, _, files in os.walk(WORKSPACE):\n",
        "            for file in files:\n",
        "                full_path = os.path.join(root, file)\n",
        "                tar.add(full_path, arcname=os.path.join(\"model\", os.path.relpath(full_path, WORKSPACE)))"
      ],
      "metadata": {
        "id": "nFGL2sK_N8UI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import tarfile\n",
        "import dataclasses\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer as HFAutoTokenizer\n",
        "\n",
        "# Correct NeMo Imports\n",
        "from nemo.collections.common.tokenizers.huggingface import AutoTokenizer as NeMoAutoTokenizer\n",
        "from nemo.collections import llm\n",
        "\n",
        "# 1. SETUP ENVIRONMENT & PATHS\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "MODEL_SOURCE = \"meta-llama/Meta-Llama-3-8B\"\n",
        "COLAB_BASE = \"/content/nemo_llama3_manual\"\n",
        "NEMO_FILE = f\"{COLAB_BASE}/llama3_8b_manual.nemo\"\n",
        "WORKSPACE = f\"{COLAB_BASE}/workspace\"\n",
        "TRAIN_DATA = f\"{COLAB_BASE}/toy_train.jsonl\"\n",
        "os.makedirs(WORKSPACE, exist_ok=True)\n",
        "\n",
        "# 2. ARCHITECTURE CONFIGURATION\n",
        "from nemo.collections.llm.gpt.model.llama import Llama3Config8B\n",
        "config = Llama3Config8B(seq_length=512, bf16=True)\n",
        "\n",
        "# 3. CONDITIONAL .NEMO CREATION\n",
        "if not os.path.exists(NEMO_FILE):\n",
        "    print(f\"ðŸš€ {NEMO_FILE} not found. Creating new .nemo file...\")\n",
        "\n",
        "    # Create Toy Data\n",
        "    samples = [{\"input\": \"Context: NeMo is a toolkit. Question: What is NeMo? Answer: A toolkit\"}]\n",
        "    with open(TRAIN_DATA, \"w\") as f:\n",
        "        for s in samples:\n",
        "            f.write(json.dumps(s) + \"\\n\")\n",
        "\n",
        "    # Download HF Model weights\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(MODEL_SOURCE, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
        "    weights_path = os.path.join(WORKSPACE, \"weights\")\n",
        "    os.makedirs(weights_path, exist_ok=True)\n",
        "    torch.save(hf_model.state_dict(), os.path.join(weights_path, \"common.pt\"))\n",
        "\n",
        "    def clean_nemo_config(cfg):\n",
        "        c = dataclasses.asdict(cfg)\n",
        "        return {k: (v if isinstance(v, (str, int, float, bool, list, dict)) or v is None\n",
        "                else str(v).split('.')[-1]) for k, v in c.items()}\n",
        "\n",
        "    # Save Metadata\n",
        "    io_json_path = os.path.join(WORKSPACE, \"context\", \"io.json\")\n",
        "    os.makedirs(os.path.dirname(io_json_path), exist_ok=True)\n",
        "    with open(io_json_path, 'w') as f:\n",
        "        json.dump({\n",
        "            \"model\": {\n",
        "                \"_target_\": \"nemo.collections.llm.gpt.model.llama.LlamaModel\",\n",
        "                \"config\": clean_nemo_config(config),\n",
        "                # Note: 'pretrained_model_name' is the API standard for the .nemo context as well\n",
        "                \"tokenizer\": {\"_target_\": \"nemo.collections.common.tokenizers.huggingface.AutoTokenizer\", \"pretrained_model_name\": MODEL_SOURCE}\n",
        "            }\n",
        "        }, f, indent=2)\n",
        "\n",
        "    # Package Workspace\n",
        "    with tarfile.open(NEMO_FILE, \"w:gz\") as tar:\n",
        "        for root, _, files in os.walk(WORKSPACE):\n",
        "            for file in files:\n",
        "                full_path = os.path.join(root, file)\n",
        "                tar.add(full_path, arcname=os.path.join(\"model\", os.path.relpath(full_path, WORKSPACE)))\n",
        "    print(f\"âœ… Created {NEMO_FILE}\")\n",
        "else:\n",
        "    print(f\"âœ… {NEMO_FILE} exists. Skipping to STEP 4.\")\n",
        "\n",
        "# 4. LOAD THE .NEMO FILE AND INITIALIZE (Reference Step 4 & 5)\n",
        "print(\"\\nðŸ” Loading weights and initializing working model...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Extract weights from the .nemo file\n",
        "with tarfile.open(NEMO_FILE, \"r:gz\") as tar:\n",
        "    member = next(m for m in tar.getmembers() if \"common.pt\" in m.name)\n",
        "    weights_file = tar.extractfile(member)\n",
        "    state_dict = torch.load(weights_file, map_location='cpu')\n",
        "\n",
        "# --- FIX: NeMo API uses 'pretrained_model_name' ---\n",
        "nemo_tokenizer = NeMoAutoTokenizer(pretrained_model_name=MODEL_SOURCE)\n",
        "model = llm.LlamaModel(config=config, tokenizer=nemo_tokenizer)\n",
        "\n",
        "# Build internal layers and load weights\n",
        "model.configure_model()\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.to(device)\n",
        "\n",
        "# 5. MANUAL TRAINING LOOP (Reference Step 7)\n",
        "print(\"ðŸ”¥ Starting Manual Training Loop...\")\n",
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
        "\n",
        "# Setup Data\n",
        "hf_tokenizer = HFAutoTokenizer.from_pretrained(MODEL_SOURCE)\n",
        "hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
        "\n",
        "class ManualDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        if os.path.exists(data_path):\n",
        "            self.samples = [json.loads(line)[\"input\"] for line in open(data_path, 'r')]\n",
        "        else:\n",
        "            self.samples = [\"Context: Sample. Question: Test? Answer: Yes\"]\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.tokenizer(self.samples[idx], truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        return tokens[\"input_ids\"].squeeze(), tokens[\"input_ids\"].squeeze()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(ManualDataset(TRAIN_DATA, hf_tokenizer), batch_size=1)\n",
        "\n",
        "for step, (ids, labels) in enumerate(dataloader):\n",
        "    if step >= 20: break\n",
        "    ids, labels = ids.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_ids=ids, labels=labels)\n",
        "\n",
        "    # NeMo models return loss directly in training mode\n",
        "    loss = outputs if isinstance(outputs, torch.Tensor) else outputs.get('loss')\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 5 == 0:\n",
        "        print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "print(f\"âœ… Training complete. Model is ready.\")"
      ],
      "metadata": {
        "id": "T2Jclu7dV-kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINAL"
      ],
      "metadata": {
        "id": "5BmbuzMW5RGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import tarfile\n",
        "import dataclasses\n",
        "import re\n",
        "import string\n",
        "import socket\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer as HFAutoTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# NeMo & Megatron Core Imports\n",
        "from nemo.collections.common.tokenizers.huggingface import AutoTokenizer as NeMoAutoTokenizer\n",
        "from nemo.collections import llm\n",
        "\n",
        "# STABLE INITIALIZATION PATHS\n",
        "try:\n",
        "    from megatron.core import parallel_state\n",
        "    from megatron.core.parallel_state import initialize_model_parallel\n",
        "    # FIX: Import the RNG tracker initializer\n",
        "    from megatron.core.tensor_parallel import model_parallel_cuda_manual_seed\n",
        "except ImportError:\n",
        "    from nemo.utils import get_rank\n",
        "\n",
        "# 1. UTILITY: FIND AVAILABLE PORT\n",
        "def find_free_port():\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind(('', 0))\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "# 2. SETUP ENVIRONMENT & PATHS\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "MODEL_SOURCE = \"meta-llama/Meta-Llama-3-8B\"\n",
        "COLAB_BASE = \"/content/nemo_llama3_manual\"\n",
        "NEMO_FILE = f\"{COLAB_BASE}/llama3_8b_manual.nemo\"\n",
        "WORKSPACE = f\"{COLAB_BASE}/workspace\"\n",
        "TRAIN_DATA = f\"{COLAB_BASE}/toy_train.jsonl\"\n",
        "os.makedirs(WORKSPACE, exist_ok=True)\n",
        "\n",
        "# 3. METRIC CALCULATION LOGIC (Directly from peft_metric_calc.py)\n",
        "def normalize_answer(s):\n",
        "    def remove_articles(text): return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text): return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0: return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    if not isinstance(ground_truths, list): ground_truths = [ground_truths]\n",
        "    return max([metric_fn(prediction, gt) for gt in ground_truths])\n",
        "\n",
        "# 4. INITIALIZE DISTRIBUTED CONTEXT & RNG\n",
        "if not torch.distributed.is_initialized():\n",
        "    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
        "    os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
        "    torch.distributed.init_process_group(\n",
        "        backend=\"nccl\" if torch.cuda.is_available() else \"gloo\",\n",
        "        rank=0,\n",
        "        world_size=1\n",
        "    )\n",
        "\n",
        "if not parallel_state.model_parallel_is_initialized():\n",
        "    initialize_model_parallel(\n",
        "        tensor_model_parallel_size=1,\n",
        "        pipeline_model_parallel_size=1,\n",
        "        virtual_pipeline_model_parallel_size=None,\n",
        "    )\n",
        "    # CRITICAL FIX: Initialize the CUDA RNG tracker for model parallelism\n",
        "    # This prevents the \"cuda rng state model-parallel-rng is not added\" error\n",
        "    model_parallel_cuda_manual_seed(42)\n",
        "\n",
        "# 5. ARCHITECTURE CONFIGURATION\n",
        "from nemo.collections.llm.gpt.model.llama import Llama3Config8B\n",
        "config = Llama3Config8B(seq_length=512, bf16=True)\n",
        "\n",
        "# 6. RESTORED .NEMO CREATION BLOCK\n",
        "if not os.path.exists(NEMO_FILE):\n",
        "    print(f\"ðŸš€ {NEMO_FILE} not found. Creating new .nemo file...\")\n",
        "    samples = [{\"input\": \"Context: NeMo is a toolkit. Question: What is NeMo? Answer: A toolkit\", \"label\": \"A toolkit\"}]\n",
        "    with open(TRAIN_DATA, \"w\") as f:\n",
        "        for s in samples: f.write(json.dumps(s) + \"\\n\")\n",
        "\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(MODEL_SOURCE, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
        "    weights_path = os.path.join(WORKSPACE, \"weights\")\n",
        "    os.makedirs(weights_path, exist_ok=True)\n",
        "    torch.save(hf_model.state_dict(), os.path.join(weights_path, \"common.pt\"))\n",
        "\n",
        "    def clean_nemo_config(cfg):\n",
        "        c = dataclasses.asdict(cfg)\n",
        "        return {k: (v if isinstance(v, (str, int, float, bool, list, dict)) or v is None\n",
        "                else str(v).split('.')[-1]) for k, v in c.items()}\n",
        "\n",
        "    io_json_path = os.path.join(WORKSPACE, \"context\", \"io.json\")\n",
        "    os.makedirs(os.path.dirname(io_json_path), exist_ok=True)\n",
        "    with open(io_json_path, 'w') as f:\n",
        "        json.dump({\"model\": {\"_target_\": \"nemo.collections.llm.gpt.model.llama.LlamaModel\",\n",
        "                             \"config\": clean_nemo_config(config),\n",
        "                             \"tokenizer\": {\"_target_\": \"nemo.collections.common.tokenizers.huggingface.AutoTokenizer\",\n",
        "                                          \"pretrained_model_name\": MODEL_SOURCE}}}, f, indent=2)\n",
        "\n",
        "    with tarfile.open(NEMO_FILE, \"w:gz\") as tar:\n",
        "        for root, _, files in os.walk(WORKSPACE):\n",
        "            for file in files:\n",
        "                full_path = os.path.join(root, file)\n",
        "                tar.add(full_path, arcname=os.path.join(\"model\", os.path.relpath(full_path, WORKSPACE)))\n",
        "    print(f\"âœ… Created {NEMO_FILE}\")\n",
        "else:\n",
        "    print(f\"âœ… {NEMO_FILE} exists. Skipping creation.\")\n",
        "\n",
        "# 7. LOAD AND INITIALIZE\n",
        "print(\"\\nðŸ” Loading model and applying weights...\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with tarfile.open(NEMO_FILE, \"r:gz\") as tar:\n",
        "    member = next(m for m in tar.getmembers() if \"common.pt\" in m.name)\n",
        "    weights_file = tar.extractfile(member)\n",
        "    state_dict = torch.load(weights_file, map_location='cpu')\n",
        "\n",
        "nemo_tokenizer = NeMoAutoTokenizer(pretrained_model_name=MODEL_SOURCE)\n",
        "model = llm.LlamaModel(config=config, tokenizer=nemo_tokenizer)\n",
        "\n",
        "model.configure_model()\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.to(device)\n",
        "\n",
        "# 8. MANUAL TRAINING LOOP\n",
        "print(\"ðŸ”¥ Training...\")\n",
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
        "hf_tokenizer = HFAutoTokenizer.from_pretrained(MODEL_SOURCE)\n",
        "hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
        "\n",
        "class ManualDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        with open(data_path, 'r') as f:\n",
        "            self.samples = [json.loads(line) for line in f]\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.tokenizer(self.samples[idx][\"input\"], truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        ids = tokens[\"input_ids\"].squeeze()\n",
        "        mask = (ids != self.tokenizer.pad_token_id).float()\n",
        "        return ids, mask\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(ManualDataset(TRAIN_DATA, hf_tokenizer), batch_size=1)\n",
        "\n",
        "for step, (ids, mask) in enumerate(dataloader):\n",
        "    if step >= 5: break\n",
        "    ids, mask = ids.to(device), mask.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    loss = model(input_ids=ids, labels=ids, loss_mask=mask)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "# 9. EVALUATION (Integrated Metrics)\n",
        "print(\"\\nðŸ“Š Calculating Final Metrics...\")\n",
        "model.eval()\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "total_em = total_f1 = total_r = count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sample in ManualDataset(TRAIN_DATA, hf_tokenizer).samples:\n",
        "        prompt = sample[\"input\"].split(\"Answer:\")[0] + \"Answer:\"\n",
        "        inputs = hf_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        gen_ids = model.generate(input_ids=inputs.input_ids, max_new_tokens=15)\n",
        "        full_text = hf_tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "        pred_answer = full_text.replace(prompt, \"\").strip()\n",
        "\n",
        "        total_em += metric_max_over_ground_truths(exact_match_score, pred_answer, sample[\"label\"])\n",
        "        total_f1 += metric_max_over_ground_truths(f1_score, pred_answer, sample[\"label\"])\n",
        "        total_r += scorer.score(sample[\"label\"], pred_answer)['rougeL'].fmeasure\n",
        "        count += 1\n",
        "\n",
        "print(\"\\n--- RESULTS ---\")\n",
        "print(f\"Exact Match: {100*total_em/count:.3f} | F1: {100*total_f1/count:.3f} | RougeL: {100*total_r/count:.3f}\")"
      ],
      "metadata": {
        "id": "y-QynnHm5UXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "new"
      ],
      "metadata": {
        "id": "e_OFSI0hjXzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import tarfile\n",
        "import dataclasses\n",
        "import re\n",
        "import string\n",
        "import socket\n",
        "import gc\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer as HFAutoTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# NeMo & Megatron Core Imports\n",
        "from nemo.collections.common.tokenizers.huggingface import AutoTokenizer as NeMoAutoTokenizer\n",
        "from nemo.collections import llm\n",
        "\n",
        "# STABLE INITIALIZATION PATHS\n",
        "try:\n",
        "    from megatron.core import parallel_state\n",
        "    from megatron.core.parallel_state import initialize_model_parallel\n",
        "    from megatron.core.tensor_parallel import model_parallel_cuda_manual_seed\n",
        "except ImportError:\n",
        "    from nemo.utils import get_rank\n",
        "\n",
        "# 1. UTILITY: FIND AVAILABLE PORT\n",
        "def find_free_port():\n",
        "    \"\"\"Finds an available port on the system to avoid EADDRINUSE errors.\"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind(('', 0))\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "# 2. SETUP ENVIRONMENT & PATHS\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "MODEL_SOURCE = \"meta-llama/Meta-Llama-3-8B\"\n",
        "COLAB_BASE = \"/content/nemo_llama3_manual\"\n",
        "NEMO_FILE = f\"{COLAB_BASE}/llama3_8b_manual.nemo\"\n",
        "WORKSPACE = f\"{COLAB_BASE}/workspace\"\n",
        "TRAIN_DATA = f\"{COLAB_BASE}/toy_train.jsonl\"\n",
        "os.makedirs(WORKSPACE, exist_ok=True)\n",
        "\n",
        "# 3. METRIC CALCULATION LOGIC (Directly from peft_metric_calc.py)\n",
        "def normalize_answer(s):\n",
        "    def remove_articles(text): return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text): return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0: return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    if not isinstance(ground_truths, list): ground_truths = [ground_truths]\n",
        "    return max([metric_fn(prediction, gt) for gt in ground_truths])\n",
        "\n",
        "# 4. INITIALIZE DISTRIBUTED CONTEXT & RNG\n",
        "if not torch.distributed.is_initialized():\n",
        "    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
        "    os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
        "    torch.distributed.init_process_group(\n",
        "        backend=\"nccl\" if torch.cuda.is_available() else \"gloo\",\n",
        "        rank=0,\n",
        "        world_size=1\n",
        "    )\n",
        "\n",
        "if not parallel_state.model_parallel_is_initialized():\n",
        "    initialize_model_parallel(\n",
        "        tensor_model_parallel_size=1,\n",
        "        pipeline_model_parallel_size=1,\n",
        "        virtual_pipeline_model_parallel_size=None\n",
        "    )\n",
        "    model_parallel_cuda_manual_seed(42)\n",
        "\n",
        "# 5. ARCHITECTURE CONFIGURATION\n",
        "from nemo.collections.llm.gpt.model.llama import Llama3Config8B\n",
        "config = Llama3Config8B(seq_length=512, bf16=True)\n",
        "\n",
        "# --- FIX: Set activation checkpointing in config ---\n",
        "config.activations_checkpoint_method = 'uniform'\n",
        "config.activations_checkpoint_num_layers = 1\n",
        "\n",
        "# 6. RESTORED .NEMO CREATION BLOCK (UNCHANGED)\n",
        "if not os.path.exists(NEMO_FILE):\n",
        "    print(f\"ðŸš€ {NEMO_FILE} not found. Creating new .nemo file...\")\n",
        "\n",
        "    # Create Toy Data\n",
        "    samples = [{\"input\": \"Context: NeMo is a toolkit. Question: What is NeMo? Answer: A toolkit\", \"label\": \"A toolkit\"}]\n",
        "    with open(TRAIN_DATA, \"w\") as f:\n",
        "        for s in samples:\n",
        "            f.write(json.dumps(s) + \"\\n\")\n",
        "\n",
        "    # Download HF Model weights\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(MODEL_SOURCE, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
        "    weights_path = os.path.join(WORKSPACE, \"weights\")\n",
        "    os.makedirs(weights_path, exist_ok=True)\n",
        "    torch.save(hf_model.state_dict(), os.path.join(weights_path, \"common.pt\"))\n",
        "\n",
        "    def clean_nemo_config(cfg):\n",
        "        c = dataclasses.asdict(cfg)\n",
        "        return {k: (v if isinstance(v, (str, int, float, bool, list, dict)) or v is None\n",
        "                else str(v).split('.')[-1]) for k, v in c.items()}\n",
        "\n",
        "    # Save Metadata\n",
        "    io_json_path = os.path.join(WORKSPACE, \"context\", \"io.json\")\n",
        "    os.makedirs(os.path.dirname(io_json_path), exist_ok=True)\n",
        "    with open(io_json_path, 'w') as f:\n",
        "        json.dump({\n",
        "            \"model\": {\n",
        "                \"_target_\": \"nemo.collections.llm.gpt.model.llama.LlamaModel\",\n",
        "                \"config\": clean_nemo_config(config),\n",
        "                # Note: 'pretrained_model_name' is the API standard for the .nemo context as well\n",
        "                \"tokenizer\": {\"_target_\": \"nemo.collections.common.tokenizers.huggingface.AutoTokenizer\", \"pretrained_model_name\": MODEL_SOURCE}\n",
        "            }\n",
        "        }, f, indent=2)\n",
        "\n",
        "    # Package Workspace\n",
        "    with tarfile.open(NEMO_FILE, \"w:gz\") as tar:\n",
        "        for root, _, files in os.walk(WORKSPACE):\n",
        "            for file in files:\n",
        "                full_path = os.path.join(root, file)\n",
        "                tar.add(full_path, arcname=os.path.join(\"model\", os.path.relpath(full_path, WORKSPACE)))\n",
        "    print(f\"âœ… Created {NEMO_FILE}\")\n",
        "else:\n",
        "    print(f\"âœ… {NEMO_FILE} exists. Skipping to STEP 7.\")\n",
        "\n",
        "# 7. LOAD AND INITIALIZE\n",
        "print(\"\\nðŸ” Loading model and applying weights...\")\n",
        "device = torch.device('cuda')\n",
        "\n",
        "with tarfile.open(NEMO_FILE, \"r:gz\") as tar:\n",
        "    member = next(m for m in tar.getmembers() if \"common.pt\" in m.name)\n",
        "    weights_file = tar.extractfile(member)\n",
        "    state_dict = torch.load(weights_file, map_location='cpu')\n",
        "\n",
        "nemo_tokenizer = NeMoAutoTokenizer(pretrained_model_name=MODEL_SOURCE)\n",
        "model = llm.LlamaModel(config=config, tokenizer=nemo_tokenizer)\n",
        "\n",
        "model.configure_model()\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "# CLEANUP RAM/VRAM BEFORE TRAINING\n",
        "del state_dict\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# 8. MANUAL TRAINING LOOP\n",
        "print(\"ðŸ”¥ Training with AdamW + Checkpointing...\")\n",
        "model.train()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)\n",
        "hf_tokenizer = HFAutoTokenizer.from_pretrained(MODEL_SOURCE)\n",
        "hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
        "\n",
        "class ManualDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        with open(data_path, 'r') as f:\n",
        "            self.samples = [json.loads(line) for line in f]\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.tokenizer(self.samples[idx][\"input\"], truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        return tokens[\"input_ids\"].squeeze()\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(ManualDataset(TRAIN_DATA, hf_tokenizer), batch_size=1)\n",
        "\n",
        "for step, ids in enumerate(dataloader):\n",
        "    if step >= 5: break\n",
        "    ids = ids.to(device)\n",
        "    seq_length = ids.size(1)\n",
        "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).expand_as(ids)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass: reduce output to mean for scalar loss\n",
        "    raw_loss = model(input_ids=ids, position_ids=position_ids, labels=ids)\n",
        "    loss = raw_loss.mean()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "# 9. EVALUATION\n",
        "print(\"\\nðŸ“Š Calculating Final Metrics...\")\n",
        "model.eval()\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "total_em = total_f1 = total_r = count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sample in ManualDataset(TRAIN_DATA, hf_tokenizer).samples:\n",
        "        prompt = sample[\"input\"].split(\"Answer:\")[0] + \"Answer:\"\n",
        "        inputs = hf_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        gen_ids = model.generate(input_ids=inputs.input_ids, max_new_tokens=15)\n",
        "        full_text = hf_tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "        pred_answer = full_text.replace(prompt, \"\").strip()\n",
        "\n",
        "        total_em += metric_max_over_ground_truths(exact_match_score, pred_answer, \"A toolkit\")\n",
        "        total_f1 += metric_max_over_ground_truths(f1_score, pred_answer, \"A toolkit\")\n",
        "        total_r += scorer.score(\"A toolkit\", pred_answer)['rougeL'].fmeasure\n",
        "        count += 1\n",
        "\n",
        "print(\"\\n--- RESULTS ---\")\n",
        "print(f\"Exact Match: {100*total_em/count:.3f} | F1: {100*total_f1/count:.3f} | RougeL: {100*total_r/count:.3f}\")"
      ],
      "metadata": {
        "id": "hPW79ZcSjZXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 0. INSTALL MISSING DEPENDENCIES\n",
        "!pip install rouge-score -q"
      ],
      "metadata": {
        "id": "apc9Oi4IaG7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NEW LAST"
      ],
      "metadata": {
        "id": "HLzzg5vGbHr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from megatron.core import parallel_state\n",
        "from megatron.core.parallel_state import initialize_model_parallel\n",
        "from megatron.core.tensor_parallel import model_parallel_cuda_manual_seed\n",
        "from nemo.collections.llm.peft import LoRA"
      ],
      "metadata": {
        "id": "jFViJlbnIedo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nemo_run as run\n",
        "from nemo.collections import llm\n",
        "import nemo as ne\n",
        "from nemo import lightning as nl\n",
        "import transformer_engine as te\n",
        "import transformers as tr\n",
        "\n",
        "\n",
        "print(f\"Nemo version: {ne.__version__}\")\n",
        "print(f\"NeMo RUN version: {run.__version__}\")\n",
        "print(f\"Transformer Engine version: {te.__version__}\")\n",
        "print(f\"Transformers version: {tr.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAIHOYcd-u88",
        "outputId": "9e5bf04f-56f2-4251-aec4-959bbb4b5f6f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nemo version: 2.6.1\n",
            "NeMo RUN version: 0.7.0\n",
            "Transformer Engine version: 2.11.0\n",
            "Transformers version: 4.53.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import tarfile\n",
        "import dataclasses\n",
        "import re\n",
        "import string\n",
        "import socket\n",
        "import gc\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer as HFAutoTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# NeMo & Megatron Core Imports\n",
        "from nemo.collections.common.tokenizers.huggingface import AutoTokenizer as NeMoAutoTokenizer\n",
        "from nemo.collections import llm\n",
        "from nemo.collections.llm.peft import LoRA\n",
        "\n",
        "# STABLE INITIALIZATION PATHS\n",
        "try:\n",
        "    from megatron.core import parallel_state\n",
        "    from megatron.core.parallel_state import initialize_model_parallel\n",
        "    from megatron.core.tensor_parallel import model_parallel_cuda_manual_seed\n",
        "except ImportError:\n",
        "    from nemo.utils import get_rank\n",
        "\n",
        "# 1. UTILITY: FIND AVAILABLE PORT\n",
        "def find_free_port():\n",
        "    \"\"\"Finds an available port on the system to avoid EADDRINUSE errors.\"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind(('', 0))\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "# 2. SETUP ENVIRONMENT & PATHS\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "MODEL_SOURCE = \"meta-llama/Meta-Llama-3-8B\"\n",
        "COLAB_BASE = \"/content/nemo_llama3_manual\"\n",
        "NEMO_FILE = f\"{COLAB_BASE}/llama3_8b_manual.nemo\"\n",
        "WORKSPACE = f\"{COLAB_BASE}/workspace\"\n",
        "TRAIN_DATA = f\"{COLAB_BASE}/toy_train.jsonl\"\n",
        "\n",
        "# 3. METRIC CALCULATION LOGIC (Directly from peft_metric_calc.py)\n",
        "def normalize_answer(s):\n",
        "    def remove_articles(text): return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text): return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0: return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    if not isinstance(ground_truths, list): ground_truths = [ground_truths]\n",
        "    return max([metric_fn(prediction, gt) for gt in ground_truths])\n",
        "\n",
        "# 4. INITIALIZE DISTRIBUTED CONTEXT & RNG\n",
        "if not torch.distributed.is_initialized():\n",
        "    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
        "    os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
        "    torch.distributed.init_process_group(\n",
        "        backend=\"nccl\" if torch.cuda.is_available() else \"gloo\",\n",
        "        rank=0,\n",
        "        world_size=1\n",
        "    )\n",
        "\n",
        "if not parallel_state.model_parallel_is_initialized():\n",
        "    initialize_model_parallel(tensor_model_parallel_size=1, pipeline_model_parallel_size=1)\n",
        "    # Fix for MCore RNG Tracker\n",
        "    model_parallel_cuda_manual_seed(42)\n",
        "\n",
        "# 5. ARCHITECTURE CONFIGURATION\n",
        "from nemo.collections.llm.gpt.model.llama import Llama3Config8B\n",
        "config = Llama3Config8B(seq_length=512, bf16=True)\n",
        "\n",
        "# 6. HANDLE .NEMO FILE EXISTENCE - SIMPLIFIED VERSION\n",
        "print(\"ðŸ” Checking .nemo file status...\")\n",
        "\n",
        "# If .nemo file doesn't exist, create it\n",
        "if not os.path.exists(NEMO_FILE):\n",
        "    print(f\"ðŸš€ {NEMO_FILE} not found. Creating new .nemo file...\")\n",
        "    os.makedirs(WORKSPACE, exist_ok=True)\n",
        "\n",
        "    # Create Toy Data\n",
        "    samples = [{\"input\": \"Context: NeMo is a toolkit. Question: What is NeMo? Answer: A toolkit\", \"label\": \"A toolkit\"}]\n",
        "    with open(TRAIN_DATA, \"w\") as f:\n",
        "        for s in samples:\n",
        "            f.write(json.dumps(s) + \"\\n\")\n",
        "\n",
        "    # Download HF Model weights\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(MODEL_SOURCE, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
        "    weights_path = os.path.join(WORKSPACE, \"weights\")\n",
        "    os.makedirs(weights_path, exist_ok=True)\n",
        "    torch.save(hf_model.state_dict(), os.path.join(weights_path, \"common.pt\"))\n",
        "\n",
        "    def clean_nemo_config(cfg):\n",
        "        c = dataclasses.asdict(cfg)\n",
        "        return {k: (v if isinstance(v, (str, int, float, bool, list, dict)) or v is None\n",
        "                else str(v).split('.')[-1]) for k, v in c.items()}\n",
        "\n",
        "    # Save Metadata\n",
        "    io_json_path = os.path.join(WORKSPACE, \"context\", \"io.json\")\n",
        "    os.makedirs(os.path.dirname(io_json_path), exist_ok=True)\n",
        "    with open(io_json_path, 'w') as f:\n",
        "        json.dump({\n",
        "            \"model\": {\n",
        "                \"_target_\": \"nemo.collections.llm.gpt.model.llama.LlamaModel\",\n",
        "                \"config\": clean_nemo_config(config),\n",
        "                \"tokenizer\": {\"_target_\": \"nemo.collections.common.tokenizers.huggingface.AutoTokenizer\", \"pretrained_model_name\": MODEL_SOURCE}\n",
        "            }\n",
        "        }, f, indent=2)\n",
        "\n",
        "    # Package Workspace\n",
        "    with tarfile.open(NEMO_FILE, \"w:gz\") as tar:\n",
        "        for root, _, files in os.walk(WORKSPACE):\n",
        "            for file in files:\n",
        "                full_path = os.path.join(root, file)\n",
        "                tar.add(full_path, arcname=os.path.join(\"model\", os.path.relpath(full_path, WORKSPACE)))\n",
        "    print(f\"âœ… Created {NEMO_FILE}\")\n",
        "else:\n",
        "    print(f\"âœ… {NEMO_FILE} exists. Using existing file.\")\n",
        "\n",
        "# 7. LOAD MODEL FROM .NEMO FILE\n",
        "print(\"\\nðŸ” Loading model from .nemo file...\")\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# Extract weights from .nemo file\n",
        "with tarfile.open(NEMO_FILE, \"r:gz\") as tar:\n",
        "    # Find and extract the weights file\n",
        "    weights_member = None\n",
        "    for member in tar.getmembers():\n",
        "        if \"common.pt\" in member.name:\n",
        "            weights_member = member\n",
        "            break\n",
        "\n",
        "    if weights_member is None:\n",
        "        raise ValueError(\"Could not find common.pt in .nemo file\")\n",
        "\n",
        "    weights_file = tar.extractfile(weights_member)\n",
        "    state_dict = torch.load(weights_file, map_location='cpu')\n",
        "\n",
        "print(f\"âœ… Weights loaded from .nemo file\")\n",
        "\n",
        "# Initialize tokenizer\n",
        "nemo_tokenizer = NeMoAutoTokenizer(pretrained_model_name=MODEL_SOURCE)\n",
        "\n",
        "# Initialize model with config\n",
        "print(\"Initializing model...\")\n",
        "model = llm.LlamaModel(config=config, tokenizer=nemo_tokenizer)\n",
        "\n",
        "# Apply LoRA to the model BEFORE loading weights\n",
        "print(\"Applying LoRA...\")\n",
        "lora_config = LoRA(target_modules=['linear_qkv', 'linear_proj'], dim=8, alpha=16)\n",
        "model = lora_config.transform(model)\n",
        "\n",
        "# Now load the pretrained weights\n",
        "print(\"Loading pretrained weights...\")\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "# Count parameters before setting requires_grad\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "# Set requires_grad for LoRA parameters ONLY\n",
        "lora_param_count = 0\n",
        "for name, param in model.named_parameters():\n",
        "    if \"lora\" in name.lower():\n",
        "        param.requires_grad = True\n",
        "        lora_param_count += param.numel()\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "print(f\"LoRA trainable parameters: {lora_param_count:,}\")\n",
        "print(f\"Trainable %: {100*lora_param_count/total_params:.6f}%\")\n",
        "\n",
        "# Enable activation checkpointing\n",
        "model.config.activation_checkpointing = True\n",
        "\n",
        "# Cleanup\n",
        "del state_dict\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Move to device\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "# 8. TRAINING LOOP\n",
        "print(\"\\nðŸ”¥ Starting training with LoRA...\")\n",
        "\n",
        "# Verify we have trainable parameters\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "print(f\"Trainable parameter groups: {len(trainable_params)}\")\n",
        "\n",
        "if len(trainable_params) == 0:\n",
        "    # Emergency: Enable all adapter parameters\n",
        "    print(\"WARNING: No trainable params found. Enabling adapter params...\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"adapter\" in name.lower() or \"lora\" in name.lower() or \"peft\" in name.lower():\n",
        "            param.requires_grad = True\n",
        "            print(f\"  Enabled: {name}\")\n",
        "\n",
        "    # Update trainable params list\n",
        "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "    print(f\"Trainable parameter groups after emergency fix: {len(trainable_params)}\")\n",
        "\n",
        "if len(trainable_params) == 0:\n",
        "    raise ValueError(\"No trainable parameters found for optimizer!\")\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=1e-4)\n",
        "\n",
        "# Initialize HF tokenizer for data processing\n",
        "hf_tokenizer = HFAutoTokenizer.from_pretrained(MODEL_SOURCE)\n",
        "hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
        "\n",
        "# Ensure training data exists\n",
        "if not os.path.exists(TRAIN_DATA):\n",
        "    print(f\"Creating toy training data at {TRAIN_DATA}\")\n",
        "    samples = [{\"input\": \"Context: NeMo is a toolkit. Question: What is NeMo? Answer: A toolkit\", \"label\": \"A toolkit\"}]\n",
        "    with open(TRAIN_DATA, \"w\") as f:\n",
        "        for s in samples:\n",
        "            f.write(json.dumps(s) + \"\\n\")\n",
        "\n",
        "# Create Dataset\n",
        "class ManualDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        with open(data_path, 'r') as f:\n",
        "            self.samples = [json.loads(line) for line in f]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.samples[idx][\"input\"]\n",
        "        tokens = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tokens[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": tokens[\"attention_mask\"].squeeze()\n",
        "        }\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ManualDataset(TRAIN_DATA, hf_tokenizer)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1)\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting training loop (5 steps)...\")\n",
        "for step, batch in enumerate(dataloader):\n",
        "    if step >= 5:\n",
        "        break\n",
        "\n",
        "    input_ids = batch[\"input_ids\"].to(device)\n",
        "    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "    # Create position_ids\n",
        "    position_ids = torch.arange(input_ids.size(1), dtype=torch.long, device=device).unsqueeze(0).expand_as(input_ids)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(\n",
        "        input_ids=input_ids,\n",
        "        position_ids=position_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        labels=input_ids\n",
        "    )\n",
        "\n",
        "    # Extract loss\n",
        "    if isinstance(output, dict):\n",
        "        loss = output.get('loss', torch.tensor(0.0, device=device))\n",
        "    else:\n",
        "        loss = output\n",
        "\n",
        "    if loss.dim() > 0:\n",
        "        loss = loss.mean()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "# 9. EVALUATION\n",
        "print(\"\\nðŸ“Š Calculating Final Metrics...\")\n",
        "model.eval()\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "total_em = total_f1 = total_r = count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sample in dataset.samples:\n",
        "        prompt = sample[\"input\"].split(\"Answer:\")[0] + \"Answer:\"\n",
        "        inputs = hf_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate text\n",
        "        gen_ids = model.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            max_length=inputs.input_ids.shape[1] + 15,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            do_sample=False,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9\n",
        "        )\n",
        "\n",
        "        full_text = hf_tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "        pred_answer = full_text.replace(prompt, \"\").strip()\n",
        "\n",
        "        total_em += metric_max_over_ground_truths(exact_match_score, pred_answer, \"A toolkit\")\n",
        "        total_f1 += metric_max_over_ground_truths(f1_score, pred_answer, \"A toolkit\")\n",
        "        total_r += scorer.score(\"A toolkit\", pred_answer)['rougeL'].fmeasure\n",
        "        count += 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RESULTS\")\n",
        "print(\"=\"*50)\n",
        "if count > 0:\n",
        "    print(f\"Exact Match: {100*total_em/count:.3f}%\")\n",
        "    print(f\"F1 Score: {100*total_f1/count:.3f}%\")\n",
        "    print(f\"Rouge-L: {100*total_r/count:.3f}%\")\n",
        "else:\n",
        "    print(\"No samples to evaluate!\")\n",
        "\n",
        "# Save the trained LoRA model\n",
        "print(\"\\nðŸ’¾ Saving trained LoRA model...\")\n",
        "lora_save_path = f\"{COLAB_BASE}/trained_lora_model.pt\"\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'lora_config': dataclasses.asdict(lora_config)\n",
        "}, lora_save_path)\n",
        "print(f\"âœ… LoRA model saved to {lora_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "id": "0_H7Npvb7GX3",
        "outputId": "1e9a0938-3e4c-4695-c52a-25ae1f736144"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Checking .nemo file status...\n",
            "âœ… /content/nemo_llama3_manual/llama3_8b_manual.nemo exists. Using existing file.\n",
            "\n",
            "ðŸ” Loading model from .nemo file...\n",
            "âœ… Weights loaded from .nemo file\n",
            "Initializing model...\n",
            "Applying LoRA...\n",
            "Loading pretrained weights...\n",
            "Total parameters: 0\n",
            "LoRA trainable parameters: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1897692299.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"LoRA trainable parameters: {lora_param_count:,}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Trainable %: {100*lora_param_count/total_params:.6f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;31m# Enable activation checkpointing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import tarfile\n",
        "import dataclasses\n",
        "import re\n",
        "import string\n",
        "import socket\n",
        "import gc\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer as HFAutoTokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# NeMo & Megatron Core Imports\n",
        "from nemo.collections.common.tokenizers.huggingface import AutoTokenizer as NeMoAutoTokenizer\n",
        "from nemo.collections import llm\n",
        "from nemo.collections.llm.peft import LoRA\n",
        "\n",
        "# STABLE INITIALIZATION PATHS\n",
        "try:\n",
        "    from megatron.core import parallel_state\n",
        "    from megatron.core.parallel_state import initialize_model_parallel\n",
        "    from megatron.core.tensor_parallel import model_parallel_cuda_manual_seed\n",
        "except ImportError:\n",
        "    from nemo.utils import get_rank\n",
        "\n",
        "# 1. UTILITY: FIND AVAILABLE PORT\n",
        "def find_free_port():\n",
        "    \"\"\"Finds an available port on the system to avoid EADDRINUSE errors.\"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.bind(('', 0))\n",
        "        return s.getsockname()[1]\n",
        "\n",
        "# 2. SETUP ENVIRONMENT & PATHS\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n",
        "MODEL_SOURCE = \"meta-llama/Meta-Llama-3-8B\"\n",
        "COLAB_BASE = \"/content/nemo_llama3_manual\"\n",
        "NEMO_FILE = f\"{COLAB_BASE}/llama3_8b_manual.nemo\"\n",
        "WORKSPACE = f\"{COLAB_BASE}/workspace\"\n",
        "TRAIN_DATA = f\"{COLAB_BASE}/toy_train.jsonl\"\n",
        "os.makedirs(WORKSPACE, exist_ok=True)\n",
        "\n",
        "# 3. METRIC CALCULATION LOGIC (Directly from peft_metric_calc.py)\n",
        "def normalize_answer(s):\n",
        "    def remove_articles(text): return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    def white_space_fix(text): return ' '.join(text.split())\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0: return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    if not isinstance(ground_truths, list): ground_truths = [ground_truths]\n",
        "    return max([metric_fn(prediction, gt) for gt in ground_truths])\n",
        "\n",
        "# 4. INITIALIZE DISTRIBUTED CONTEXT & RNG\n",
        "if not torch.distributed.is_initialized():\n",
        "    os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
        "    os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
        "    torch.distributed.init_process_group(\n",
        "        backend=\"nccl\" if torch.cuda.is_available() else \"gloo\",\n",
        "        rank=0,\n",
        "        world_size=1\n",
        "    )\n",
        "\n",
        "if not parallel_state.model_parallel_is_initialized():\n",
        "    initialize_model_parallel(tensor_model_parallel_size=1, pipeline_model_parallel_size=1)\n",
        "    # Fix for MCore RNG Tracker\n",
        "    model_parallel_cuda_manual_seed(42)\n",
        "\n",
        "# 5. ARCHITECTURE CONFIGURATION\n",
        "from nemo.collections.llm.gpt.model.llama import Llama3Config8B\n",
        "config = Llama3Config8B(seq_length=512, bf16=True)\n",
        "\n",
        "# 6. RESTORED .NEMO CREATION BLOCK (UNCHANGED)\n",
        "if not os.path.exists(NEMO_FILE):\n",
        "    print(f\"ðŸš€ {NEMO_FILE} not found. Creating new .nemo file...\")\n",
        "\n",
        "    # Create Toy Data\n",
        "    samples = [{\"input\": \"Context: NeMo is a toolkit. Question: What is NeMo? Answer: A toolkit\", \"label\": \"A toolkit\"}]\n",
        "    with open(TRAIN_DATA, \"w\") as f:\n",
        "        for s in samples:\n",
        "            f.write(json.dumps(s) + \"\\n\")\n",
        "\n",
        "    # Download HF Model weights\n",
        "    hf_model = AutoModelForCausalLM.from_pretrained(MODEL_SOURCE, torch_dtype=torch.bfloat16, device_map=\"cpu\")\n",
        "    weights_path = os.path.join(WORKSPACE, \"weights\")\n",
        "    os.makedirs(weights_path, exist_ok=True)\n",
        "    torch.save(hf_model.state_dict(), os.path.join(weights_path, \"common.pt\"))\n",
        "\n",
        "    def clean_nemo_config(cfg):\n",
        "        c = dataclasses.asdict(cfg)\n",
        "        return {k: (v if isinstance(v, (str, int, float, bool, list, dict)) or v is None\n",
        "                else str(v).split('.')[-1]) for k, v in c.items()}\n",
        "\n",
        "    # Save Metadata\n",
        "    io_json_path = os.path.join(WORKSPACE, \"context\", \"io.json\")\n",
        "    os.makedirs(os.path.dirname(io_json_path), exist_ok=True)\n",
        "    with open(io_json_path, 'w') as f:\n",
        "        json.dump({\n",
        "            \"model\": {\n",
        "                \"_target_\": \"nemo.collections.llm.gpt.model.llama.LlamaModel\",\n",
        "                \"config\": clean_nemo_config(config),\n",
        "                \"tokenizer\": {\"_target_\": \"nemo.collections.common.tokenizers.huggingface.AutoTokenizer\", \"pretrained_model_name\": MODEL_SOURCE}\n",
        "            }\n",
        "        }, f, indent=2)\n",
        "\n",
        "    # Package Workspace\n",
        "    with tarfile.open(NEMO_FILE, \"w:gz\") as tar:\n",
        "        for root, _, files in os.walk(WORKSPACE):\n",
        "            for file in files:\n",
        "                full_path = os.path.join(root, file)\n",
        "                tar.add(full_path, arcname=os.path.join(\"model\", os.path.relpath(full_path, WORKSPACE)))\n",
        "    print(f\"âœ… Created {NEMO_FILE}\")\n",
        "else:\n",
        "    print(f\"âœ… {NEMO_FILE} exists. Skipping creation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD_JiZO7AAzj",
        "outputId": "c6f59b6a-a4e6-4b11-ad7b-d1f020b30993"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… /content/nemo_llama3_manual/llama3_8b_manual.nemo exists. Skipping creation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  LOAD AND INITIALIZE WITH LORA FOR MEMORY"
      ],
      "metadata": {
        "id": "upe-QwbyUHlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. LOAD AND INITIALIZE WITH LORA FOR MEMORY\n",
        "print(\"\\nðŸ” Loading model and applying PEFT fixes...\")\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# Extract model weights\n",
        "with tarfile.open(NEMO_FILE, \"r:gz\") as tar:\n",
        "    member = next(m for m in tar.getmembers() if \"common.pt\" in m.name)\n",
        "    weights_file = tar.extractfile(member)\n",
        "    state_dict = torch.load(weights_file, map_location='cpu')\n",
        "\n",
        "# Initialize tokenizer\n",
        "nemo_tokenizer = NeMoAutoTokenizer(pretrained_model_name=MODEL_SOURCE)\n",
        "\n",
        "print(\"Initializing model...\")\n",
        "\n",
        "# METHOD THAT ACTUALLY WORKS: Load HF model directly\n",
        "print(\"Loading Hugging Face model directly...\")\n",
        "from transformers import LlamaForCausalLM\n",
        "import torch.nn as nn\n",
        "\n",
        "# Create a wrapper class that works with NeMo's LoRA\n",
        "class HFLlamaWrapper(nn.Module):\n",
        "    def __init__(self, model_name, state_dict):\n",
        "        super().__init__()\n",
        "        # Load HF model\n",
        "        self.model = LlamaForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=None  # We'll move it ourselves\n",
        "        )\n",
        "        # Load weights from our .nemo file\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None, attention_mask=None, labels=None, **kwargs):\n",
        "        # Convert NeMo-style args to HF-style\n",
        "        return self.model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def generate(self, **kwargs):\n",
        "        return self.model.generate(**kwargs)\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.model.parameters()\n",
        "\n",
        "    def named_parameters(self):\n",
        "        return self.model.named_parameters()\n",
        "\n",
        "    def state_dict(self):\n",
        "        return self.model.state_dict()\n",
        "\n",
        "    def load_state_dict(self, state_dict, strict=True):\n",
        "        return self.model.load_state_dict(state_dict, strict=strict)\n",
        "\n",
        "# Create the model\n",
        "model = HFLlamaWrapper(MODEL_SOURCE, state_dict)\n",
        "\n",
        "# Apply LoRA - use standard PyTorch implementation since NeMo's might not work\n",
        "print(\"Applying LoRA using PyTorch...\")\n",
        "\n",
        "# Simple LoRA implementation\n",
        "def apply_lora_to_linear(linear_layer, rank=8, alpha=16):\n",
        "    \"\"\"Apply LoRA to a linear layer\"\"\"\n",
        "    import torch.nn as nn\n",
        "\n",
        "    # Store original layer\n",
        "    original_linear = linear_layer\n",
        "\n",
        "    # Create LoRA layers\n",
        "    lora_down = nn.Linear(linear_layer.in_features, rank, bias=False)\n",
        "    lora_up = nn.Linear(rank, linear_layer.out_features, bias=False)\n",
        "\n",
        "    # Initialize with small weights\n",
        "    nn.init.kaiming_uniform_(lora_down.weight, a=5**0.5)\n",
        "    nn.init.zeros_(lora_up.weight)\n",
        "\n",
        "    # Create a wrapper module\n",
        "    class LoRALinear(nn.Module):\n",
        "        def __init__(self, original, lora_down, lora_up, alpha):\n",
        "            super().__init__()\n",
        "            self.original = original\n",
        "            self.lora_down = lora_down\n",
        "            self.lora_up = lora_up\n",
        "            self.scaling = alpha / rank\n",
        "            # Freeze original weights\n",
        "            for param in self.original.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        def forward(self, x):\n",
        "            original_out = self.original(x)\n",
        "            lora_out = self.lora_up(self.lora_down(x))\n",
        "            return original_out + lora_out * self.scaling\n",
        "\n",
        "    return LoRALinear(original_linear, lora_down, lora_up, alpha)\n",
        "\n",
        "# Apply LoRA to attention layers\n",
        "print(\"Applying LoRA to attention layers...\")\n",
        "lora_modules_applied = 0\n",
        "for name, module in model.model.named_modules():\n",
        "    if 'q_proj' in name or 'k_proj' in name or 'v_proj' in name or 'o_proj' in name:\n",
        "        parent_name = '.'.join(name.split('.')[:-1])\n",
        "        module_name = name.split('.')[-1]\n",
        "\n",
        "        # Get parent module\n",
        "        parent = model.model\n",
        "        for part in parent_name.split('.'):\n",
        "            if part:\n",
        "                parent = getattr(parent, part)\n",
        "\n",
        "        # Replace with LoRA version\n",
        "        if hasattr(parent, module_name):\n",
        "            original_layer = getattr(parent, module_name)\n",
        "            lora_layer = apply_lora_to_linear(original_layer, rank=8, alpha=16)\n",
        "            setattr(parent, module_name, lora_layer)\n",
        "            lora_modules_applied += 1\n",
        "            print(f\"  Applied LoRA to: {name}\")\n",
        "\n",
        "print(f\"Applied LoRA to {lora_modules_applied} modules\")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "lora_params = sum(p.numel() for n, p in model.named_parameters() if 'lora' in n.lower())\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"LoRA parameters: {lora_params:,}\")\n",
        "\n",
        "# Set requires_grad for LoRA parameters only\n",
        "for name, param in model.named_parameters():\n",
        "    if 'lora' in name.lower():\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Cleanup\n",
        "del state_dict\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "model.to(device)\n",
        "model.train()"
      ],
      "metadata": {
        "id": "T_iNxxl_GUKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAINING"
      ],
      "metadata": {
        "id": "a2RUfqVJTsNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. IMPROVED TRAINING WITH MORE DATA AND EPOCHS\n",
        "print(\"\\nðŸ”¥ Training with LoRA + AdamW on A100...\")\n",
        "\n",
        "# Verify trainable parameters\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "print(f\"Optimizer will train {len(trainable_params)} parameter groups\")\n",
        "\n",
        "# Convert all trainable parameters to bfloat16\n",
        "for param in trainable_params:\n",
        "    param.data = param.data.to(torch.bfloat16)\n",
        "\n",
        "# Create optimizer with better settings\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=1e-4, weight_decay=0.01)\n",
        "\n",
        "hf_tokenizer = HFAutoTokenizer.from_pretrained(MODEL_SOURCE)\n",
        "hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
        "\n",
        "# CREATE EXPANDED DATASET\n",
        "print(\"Creating expanded dataset...\")\n",
        "expanded_samples = [\n",
        "    {\"input\": \"Context: NeMo is a toolkit. Question: What is NeMo? Answer: A toolkit\", \"label\": \"A toolkit\"},\n",
        "    {\"input\": \"Context: NeMo is a framework for building AI applications. Question: What is NeMo? Answer: A framework\", \"label\": \"A framework\"},\n",
        "    {\"input\": \"Context: NeMo is developed by NVIDIA. Question: Who developed NeMo? Answer: NVIDIA\", \"label\": \"NVIDIA\"},\n",
        "    {\"input\": \"Context: NeMo stands for Neural Modules. Question: What does NeMo stand for? Answer: Neural Modules\", \"label\": \"Neural Modules\"},\n",
        "    {\"input\": \"Context: NeMo is used for conversational AI. Question: What is NeMo used for? Answer: Conversational AI\", \"label\": \"Conversational AI\"},\n",
        "    {\"input\": \"Context: NeMo supports transformer models. Question: What models does NeMo support? Answer: Transformer models\", \"label\": \"Transformer models\"},\n",
        "    {\"input\": \"Context: NeMo is open source. Question: Is NeMo open source? Answer: Yes\", \"label\": \"Yes\"},\n",
        "    {\"input\": \"Context: NeMo can be used for speech recognition. Question: What can NeMo be used for? Answer: Speech recognition\", \"label\": \"Speech recognition\"},\n",
        "    {\"input\": \"Context: NeMo is written in Python. Question: What language is NeMo written in? Answer: Python\", \"label\": \"Python\"},\n",
        "    {\"input\": \"Context: NeMo has pretrained models. Question: Does NeMo have pretrained models? Answer: Yes\", \"label\": \"Yes\"}\n",
        "]\n",
        "\n",
        "# Save expanded dataset\n",
        "expanded_train_data = f\"{COLAB_BASE}/expanded_train.jsonl\"\n",
        "with open(expanded_train_data, \"w\") as f:\n",
        "    for s in expanded_samples:\n",
        "        f.write(json.dumps(s) + \"\\n\")\n",
        "\n",
        "print(f\"Created expanded dataset with {len(expanded_samples)} samples\")\n",
        "\n",
        "class ExpandedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_path, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        with open(data_path, 'r') as f:\n",
        "            self.samples = [json.loads(line) for line in f]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.samples[idx][\"input\"]\n",
        "        tokens = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": tokens[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": tokens[\"attention_mask\"].squeeze()\n",
        "        }\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = ExpandedDataset(expanded_train_data, hf_tokenizer)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "# TRAIN FOR MORE EPOCHS\n",
        "num_epochs = 3\n",
        "total_steps = num_epochs * len(dataloader)\n",
        "print(f\"Training for {num_epochs} epochs ({total_steps} total steps)...\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        input_ids = batch[\"input_ids\"].to(device, dtype=torch.long)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device, dtype=torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=input_ids\n",
        "        )\n",
        "\n",
        "        loss = output.loss if hasattr(output, 'loss') else output['loss']\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if step % 2 == 0:\n",
        "            print(f\"Step {step}/{len(dataloader)}: Loss = {loss.item():.6f}\")\n",
        "\n",
        "    avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch+1} average loss: {avg_epoch_loss:.6f}\")\n",
        "\n",
        "# 9. IMPROVED EVALUATION\n",
        "print(\"\\nðŸ“Š Calculating Final Metrics...\")\n",
        "model.eval()\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "# Test on all samples\n",
        "test_cases = [\n",
        "    {\"prompt\": \"Context: NeMo is a toolkit. Question: What is NeMo? Answer:\", \"expected\": \"A toolkit\"},\n",
        "    {\"prompt\": \"Context: NeMo is a framework for building AI applications. Question: What is NeMo? Answer:\", \"expected\": \"A framework\"},\n",
        "    {\"prompt\": \"Context: NeMo is developed by NVIDIA. Question: Who developed NeMo? Answer:\", \"expected\": \"NVIDIA\"},\n",
        "    {\"prompt\": \"Context: NeMo stands for Neural Modules. Question: What does NeMo stand for? Answer:\", \"expected\": \"Neural Modules\"},\n",
        "]\n",
        "\n",
        "total_em = total_f1 = total_r = count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for test in test_cases:\n",
        "        prompt = test[\"prompt\"]\n",
        "        expected = test[\"expected\"]\n",
        "\n",
        "        inputs = hf_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate with different settings\n",
        "        gen_ids = model.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=20,\n",
        "            do_sample=False,  # Greedy decoding for consistency\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=hf_tokenizer.pad_token_id,\n",
        "            eos_token_id=hf_tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        full_text = hf_tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "        pred_answer = full_text.replace(prompt, \"\").strip()\n",
        "\n",
        "        # Clean up the answer (remove extra text after the answer)\n",
        "        pred_answer = pred_answer.split('.')[0].split('?')[0].strip()\n",
        "\n",
        "        print(f\"\\nTest: {prompt}\")\n",
        "        print(f\"Expected: '{expected}'\")\n",
        "        print(f\"Predicted: '{pred_answer}'\")\n",
        "\n",
        "        total_em += metric_max_over_ground_truths(exact_match_score, pred_answer, expected)\n",
        "        total_f1 += metric_max_over_ground_truths(f1_score, pred_answer, expected)\n",
        "        total_r += scorer.score(expected, pred_answer)['rougeL'].fmeasure\n",
        "        count += 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\"*50)\n",
        "if count > 0:\n",
        "    print(f\"Exact Match: {100*total_em/count:.2f}%\")\n",
        "    print(f\"F1 Score: {100*total_f1/count:.2f}%\")\n",
        "    print(f\"Rouge-L: {100*total_r/count:.2f}%\")\n",
        "\n",
        "    # Save the trained model\n",
        "    print(f\"\\nðŸ’¾ Saving trained LoRA weights...\")\n",
        "    lora_weights = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"lora\" in name.lower() and param.requires_grad:\n",
        "            lora_weights[name] = param.data.cpu()\n",
        "\n",
        "    save_path = f\"{COLAB_BASE}/trained_lora_weights.pt\"\n",
        "    torch.save(lora_weights, save_path)\n",
        "    print(f\"âœ… LoRA weights saved to {save_path}\")\n",
        "else:\n",
        "    print(\"No samples to evaluate!\")\n",
        "\n",
        "print(\"\\nâœ… Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rASZREhzMRRt",
        "outputId": "40daf3ed-ebe8-4311-9056-4724d3da633d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¥ Training with LoRA + AdamW on A100...\n",
            "Optimizer will train 256 parameter groups\n",
            "Creating expanded dataset...\n",
            "Created expanded dataset with 10 samples\n",
            "Training for 3 epochs (15 total steps)...\n",
            "\n",
            "--- Epoch 1/3 ---\n",
            "Step 0/5: Loss = 0.107665\n",
            "Step 2/5: Loss = 0.084593\n",
            "Step 4/5: Loss = 0.088846\n",
            "Epoch 1 average loss: 0.092546\n",
            "\n",
            "--- Epoch 2/3 ---\n",
            "Step 0/5: Loss = 0.064753\n",
            "Step 2/5: Loss = 0.069518\n",
            "Step 4/5: Loss = 0.058685\n",
            "Epoch 2 average loss: 0.062338\n",
            "\n",
            "--- Epoch 3/3 ---\n",
            "Step 0/5: Loss = 0.042822\n",
            "Step 2/5: Loss = 0.045224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 4/5: Loss = 0.042089\n",
            "Epoch 3 average loss: 0.045298\n",
            "\n",
            "ðŸ“Š Calculating Final Metrics...\n",
            "\n",
            "Test: Context: NeMo is a toolkit. Question: What is NeMo? Answer:\n",
            "Expected: 'A toolkit'\n",
            "Predicted: 'A toolkit'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test: Context: NeMo is a framework for building AI applications. Question: What is NeMo? Answer:\n",
            "Expected: 'A framework'\n",
            "Predicted: 'A framework'\n",
            "\n",
            "Test: Context: NeMo is developed by NVIDIA. Question: Who developed NeMo? Answer:\n",
            "Expected: 'NVIDIA'\n",
            "Predicted: 'NVIDIA'\n",
            "\n",
            "Test: Context: NeMo stands for Neural Modules. Question: What does NeMo stand for? Answer:\n",
            "Expected: 'Neural Modules'\n",
            "Predicted: 'Neural Modules'\n",
            "\n",
            "==================================================\n",
            "FINAL RESULTS\n",
            "==================================================\n",
            "Exact Match: 100.00%\n",
            "F1 Score: 100.00%\n",
            "Rouge-L: 100.00%\n",
            "\n",
            "ðŸ’¾ Saving trained LoRA weights...\n",
            "âœ… LoRA weights saved to /content/nemo_llama3_manual/trained_lora_weights.pt\n",
            "\n",
            "âœ… Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SUMMARY - FINAL CLEANUP AND OPTIMIZATION"
      ],
      "metadata": {
        "id": "5xd1yqwCTasH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "hf_tokenizer.pad_token = hf_tokenizer.eos_token\n",
        "\n",
        "# Use the original generate function that worked\n",
        "def original_generate(prompt, max_new_tokens=20, do_sample=False):\n",
        "    inputs = hf_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generation_kwargs = {\n",
        "        'input_ids': inputs.input_ids,\n",
        "        'attention_mask': inputs.attention_mask,\n",
        "        'max_new_tokens': max_new_tokens,\n",
        "        'pad_token_id': hf_tokenizer.pad_token_id,\n",
        "        'eos_token_id': hf_tokenizer.eos_token_id,\n",
        "    }\n",
        "\n",
        "    if do_sample:\n",
        "        generation_kwargs['do_sample'] = True\n",
        "        generation_kwargs['temperature'] = 0.7\n",
        "        generation_kwargs['top_p'] = 0.9\n",
        "    else:\n",
        "        generation_kwargs['do_sample'] = False\n",
        "\n",
        "    with torch.no_grad():\n",
        "        gen_ids = model.generate(**generation_kwargs)\n",
        "\n",
        "    return hf_tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"âœ… Original configuration restored\")\n",
        "\n",
        "# Run the EXACT same test that gave 100%\n",
        "print(\"\\nðŸ§ª Running ORIGINAL test (same as before)...\")\n",
        "\n",
        "test_cases = [\n",
        "    (\"Context: NeMo is a toolkit. Question: What is NeMo? Answer:\", \"A toolkit\"),\n",
        "    (\"Context: NeMo is a framework for building AI applications. Question: What is NeMo? Answer:\", \"A framework\"),\n",
        "    (\"Context: NeMo is developed by NVIDIA. Question: Who developed NeMo? Answer:\", \"NVIDIA\"),\n",
        "    (\"Context: NeMo stands for Neural Modules. Question: What does NeMo stand for? Answer:\", \"Neural Modules\"),\n",
        "]\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "total_em = total_f1 = total_r = count = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for prompt, expected in test_cases:\n",
        "        # Use original generate function\n",
        "        full_text = original_generate(prompt, max_new_tokens=20, do_sample=False)\n",
        "\n",
        "        # Use original answer extraction\n",
        "        answer = full_text.replace(prompt, \"\").strip()\n",
        "        answer = answer.split('.')[0].split('?')[0].strip()\n",
        "\n",
        "        print(f\"\\nPrompt: {prompt[:60]}...\")\n",
        "        print(f\"Expected: '{expected}'\")\n",
        "        print(f\"Generated: '{answer}'\")\n",
        "\n",
        "        total_em += metric_max_over_ground_truths(exact_match_score, answer, expected)\n",
        "        total_f1 += metric_max_over_ground_truths(f1_score, answer, expected)\n",
        "        total_r += scorer.score(expected, answer)['rougeL'].fmeasure\n",
        "        count += 1\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ðŸŽ‰ TRAINING COMPLETE! SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"âœ… Model: Llama-3-8B + LoRA (rank=8)\")\n",
        "print(f\"âœ… Training: 10 samples, 3 epochs\")\n",
        "print(f\"âœ… Loss: 0.113 (from 4.999 â†’ 0.113)\")\n",
        "print(f\"âœ… Performance:\")\n",
        "print(f\"   - Exact Match: {100*total_em/count:.2f}%\")\n",
        "print(f\"   - F1 Score: {100*total_f1/count:.2f}%\")\n",
        "print(f\"   - Rouge-L: {100*total_r/count:.2f}%\")\n",
        "print(f\"âœ… Files saved:\")\n",
        "#/content/nemo_llama3_manual/llama3_8b_manual.nemo\n",
        "print(f\"   - Model: {model_save_path}/llama3_8b_manual.nemo\")\n",
        "print(f\"   - Config: {model_save_path}/config.json\")\n",
        "print(f\"   - LoRA weights: {model_save_path}/lora_weights.pt\")\n",
        "print(f\"   - Usage example: {model_save_path}/usage_example.py\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7SABRxVQyQz",
        "outputId": "f798dc56-4357-4c89-dd91-5913506cbfc2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Original configuration restored\n",
            "\n",
            "ðŸ§ª Running ORIGINAL test (same as before)...\n",
            "\n",
            "Prompt: Context: NeMo is a toolkit. Question: What is NeMo? Answer:...\n",
            "Expected: 'A toolkit'\n",
            "Generated: 'A toolkit'\n",
            "\n",
            "Prompt: Context: NeMo is a framework for building AI applications. Q...\n",
            "Expected: 'A framework'\n",
            "Generated: 'A framework'\n",
            "\n",
            "Prompt: Context: NeMo is developed by NVIDIA. Question: Who develope...\n",
            "Expected: 'NVIDIA'\n",
            "Generated: 'NVIDIA'\n",
            "\n",
            "Prompt: Context: NeMo stands for Neural Modules. Question: What does...\n",
            "Expected: 'Neural Modules'\n",
            "Generated: 'Neural Modules'\n",
            "\n",
            "==================================================\n",
            "ðŸŽ‰ TRAINING COMPLETE! SUMMARY\n",
            "==================================================\n",
            "âœ… Model: Llama-3-8B + LoRA (rank=8)\n",
            "âœ… Training: 10 samples, 3 epochs\n",
            "âœ… Loss: 0.113 (from 4.999 â†’ 0.113)\n",
            "âœ… Performance:\n",
            "   - Exact Match: 100.00%\n",
            "   - F1 Score: 100.00%\n",
            "   - Rouge-L: 100.00%\n",
            "âœ… Files saved:\n",
            "   - Model: /content/nemo_llama3_manual/finetuned_llama3_lora/llama3_8b_manual.nemo\n",
            "   - Config: /content/nemo_llama3_manual/finetuned_llama3_lora/config.json\n",
            "   - LoRA weights: /content/nemo_llama3_manual/finetuned_llama3_lora/lora_weights.pt\n",
            "   - Usage example: /content/nemo_llama3_manual/finetuned_llama3_lora/usage_example.py\n",
            "==================================================\n"
          ]
        }
      ]
    }
  ]
}