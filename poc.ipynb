{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQ1XfqCTg8Tbh67otrLEml",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/poc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "GVuPEZoS3GoH",
        "outputId": "2b5de720-ed24-4747-e525-7f978a5da563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "complex_dataset.json created.\n",
            "Dataset augmented.\n",
            "Regression Mean Squared Error: 6.564745936770839\n",
            "Starting training...\n",
            "Dataset size: 1500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  2/100 : < :, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import random\n",
        "import math\n",
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def complex_function(x, y, z):\n",
        "    \"\"\"\n",
        "    A complex function involving trigonometric operations,\n",
        "    randomness, and conditional logic.\n",
        "    \"\"\"\n",
        "    if z > 0:\n",
        "        result = (math.sin(x) + math.cos(y)) * z + random.uniform(-1, 1)\n",
        "    else:\n",
        "        result = (math.log(abs(x) + 1) - math.sqrt(abs(y) + 1)) * abs(z) + random.gauss(0, 0.5)\n",
        "    return result\n",
        "\n",
        "def generate_complex_dataset(num_samples=1000, filename=\"complex_dataset.json\"):\n",
        "    \"\"\"Generates a dataset of input-output pairs for the complex function.\"\"\"\n",
        "    dataset = []\n",
        "    for _ in range(num_samples):\n",
        "        x = random.uniform(-10, 10)\n",
        "        y = random.uniform(-10, 10)\n",
        "        z = random.uniform(-5, 5)\n",
        "        output = complex_function(x, y, z)\n",
        "        dataset.append({\"x\": x, \"y\": y, \"z\": z, \"output\": output})\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        json.dump({\"data\": dataset}, f, indent=4)\n",
        "    print(f\"{filename} created.\")\n",
        "\n",
        "def load_dataset(filename):\n",
        "    \"\"\"Loads the dataset from a JSON file.\"\"\"\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            data = json.load(f)['data']\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {filename} not found.\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: {filename} is not a valid JSON file.\")\n",
        "        return None\n",
        "\n",
        "def remove_outliers(data, threshold=3):\n",
        "    \"\"\"Removes outliers based on z-score.\"\"\"\n",
        "    outputs = np.array([item[\"output\"] for item in data])\n",
        "    mean = np.mean(outputs)\n",
        "    std = np.std(outputs)\n",
        "    filtered_data = [item for item in data if abs((item[\"output\"] - mean) / std) < threshold]\n",
        "    return filtered_data\n",
        "\n",
        "def fine_tune_and_evaluate_regression(dataset_file, use_outlier_removal=True):\n",
        "    \"\"\"Fine-tunes a linear regression model on the dataset and evaluates it.\"\"\"\n",
        "    data = load_dataset(dataset_file)\n",
        "    if data is None:\n",
        "        return None\n",
        "\n",
        "    if use_outlier_removal:\n",
        "        data = remove_outliers(data)\n",
        "\n",
        "    X = [[item[\"x\"], item[\"y\"], item[\"z\"]] for item in data]\n",
        "    y = [item[\"output\"] for item in data]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f\"Regression Mean Squared Error: {mse}\")\n",
        "    return model\n",
        "\n",
        "def augment_dataset(dataset_file, num_new_samples=100):\n",
        "    \"\"\"Augments the dataset with new samples.\"\"\"\n",
        "    data = load_dataset(dataset_file)\n",
        "    if data is None:\n",
        "      return\n",
        "\n",
        "    for _ in range(num_new_samples):\n",
        "        x = random.uniform(-10, 10)\n",
        "        y = random.uniform(-10, 10)\n",
        "        z = random.uniform(-5, 5)\n",
        "        output = complex_function(x, y, z)\n",
        "        data.append({\"x\": x, \"y\": y, \"z\": z, \"output\": output})\n",
        "\n",
        "    with open(dataset_file, \"w\") as f:\n",
        "        json.dump({\"data\": data}, f, indent=4)\n",
        "    print(\"Dataset augmented.\")\n",
        "\n",
        "class ComplexDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        input_text = f\"complex_function(x, y, z): if z > 0: return (sin(x) + cos(y)) * z + random.uniform(-1, 1); else: return (log(abs(x) + 1) - sqrt(abs(y) + 1)) * abs(z) + random.gauss(0, 0.5) x={item['x']} y={item['y']} z={item['z']}\"\n",
        "        output_text = str(item['output'])\n",
        "\n",
        "        input_encoding = self.tokenizer(input_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
        "        output_encoding = self.tokenizer(output_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_encoding['input_ids'].flatten(),\n",
        "            'attention_mask': input_encoding['attention_mask'].flatten(),\n",
        "            'labels': output_encoding['input_ids'].flatten()\n",
        "        }\n",
        "\n",
        "def fine_tune_and_evaluate_t5(dataset_file):\n",
        "    \"\"\"Fine-tunes a T5 model on the dataset and evaluates it.\"\"\"\n",
        "    data = load_dataset(dataset_file)\n",
        "    if data is None:\n",
        "        return\n",
        "\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "    dataset = ComplexDataset(data, tokenizer)\n",
        "\n",
        "    model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=1,\n",
        "        per_device_train_batch_size=8,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=20,\n",
        "        report_to=\"none\",\n",
        "        max_steps=100,\n",
        "\n",
        "\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "\n",
        "\n",
        "    #print(f\"Dataset: {dataset}\")\n",
        "    #print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Inference example\n",
        "    input_text = \"complex_function(x, y, z): if z > 0: return (sin(x) + cos(y)) * z + random.uniform(-1, 1); else: return (log(abs(x) + 1) - sqrt(abs(y) + 1)) * abs(z) + random.gauss(0, 0.5) x=1.0 y=-2.0 z=3.0\"\n",
        "    input_encoding = tokenizer(input_text, return_tensors='pt')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_encoding.input_ids, attention_mask=input_encoding.attention_mask)\n",
        "\n",
        "    output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print(f\"T5 Predicted Output: {output_text}\")\n",
        "\n",
        "# Main execution\n",
        "generate_complex_dataset()\n",
        "augment_dataset(\"complex_dataset.json\", num_new_samples=500)\n",
        "fine_tune_and_evaluate_regression(\"complex_dataset.json\")\n",
        "fine_tune_and_evaluate_t5(\"complex_dataset.json\")"
      ]
    }
  ]
}