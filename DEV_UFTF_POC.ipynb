{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "nznPRgHY8mFq",
        "BRmauENT7tWs"
      ],
      "authorship_tag": "ABX9TyNH4xbvEmJod9cOIF8l7oPh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b47a82f748f240b084047e586125b38c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2959031a2d34d60a7942f7f4ee6efa7",
              "IPY_MODEL_e5b5f1fb2f8e4ee787d8131f5ae3dfa0",
              "IPY_MODEL_f7f40114c0ca47c6b50a83bebeb382d5"
            ],
            "layout": "IPY_MODEL_6de09c4ea3a847db996a1b9be00e228d"
          }
        },
        "f2959031a2d34d60a7942f7f4ee6efa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3110ca3980f4cbea5fa381cd1d7ea81",
            "placeholder": "​",
            "style": "IPY_MODEL_1fff98156bc444e7a7b0221b5c15d141",
            "value": "Map: 100%"
          }
        },
        "e5b5f1fb2f8e4ee787d8131f5ae3dfa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6d21b0e1863495db09d0fea7fe88bcf",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bb2fb011e934f2684b90911d81a7bdc",
            "value": 1000
          }
        },
        "f7f40114c0ca47c6b50a83bebeb382d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94784e322c1c43c8957954511aa3e25c",
            "placeholder": "​",
            "style": "IPY_MODEL_af0da065c4184312bc48b1aedf0dff45",
            "value": " 1000/1000 [00:00&lt;00:00, 53601.33 examples/s]"
          }
        },
        "6de09c4ea3a847db996a1b9be00e228d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3110ca3980f4cbea5fa381cd1d7ea81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fff98156bc444e7a7b0221b5c15d141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6d21b0e1863495db09d0fea7fe88bcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bb2fb011e934f2684b90911d81a7bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94784e322c1c43c8957954511aa3e25c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af0da065c4184312bc48b1aedf0dff45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dac92fea34e44f40b0b52634af6c55d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb88d400f2fb4ef4bd1f02302d7d4862",
              "IPY_MODEL_84079ba2a8c749829f9da9a0dbb9eb34",
              "IPY_MODEL_b67c23ecd233400395f50a929712a845"
            ],
            "layout": "IPY_MODEL_6064a05ac07545e2ac47dcea7d9bc638"
          }
        },
        "eb88d400f2fb4ef4bd1f02302d7d4862": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_411bcd88ec264158918b8e13ec83b8fb",
            "placeholder": "​",
            "style": "IPY_MODEL_0ba913a80cf84a3fae618fa136fb30fa",
            "value": "Map: 100%"
          }
        },
        "84079ba2a8c749829f9da9a0dbb9eb34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e365c893176a49ac87e061cb76dd61a4",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_753d9adcf0964fcb906d4d7844e6c847",
            "value": 250
          }
        },
        "b67c23ecd233400395f50a929712a845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76990f985f6b49fbadbc35aea53e7759",
            "placeholder": "​",
            "style": "IPY_MODEL_6b6ef0820f94404bbdd249672bc407a1",
            "value": " 250/250 [00:00&lt;00:00, 17274.44 examples/s]"
          }
        },
        "6064a05ac07545e2ac47dcea7d9bc638": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "411bcd88ec264158918b8e13ec83b8fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ba913a80cf84a3fae618fa136fb30fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e365c893176a49ac87e061cb76dd61a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "753d9adcf0964fcb906d4d7844e6c847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76990f985f6b49fbadbc35aea53e7759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b6ef0820f94404bbdd249672bc407a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db0c3795689a4d779e149736c0e09ad2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c182146e652b42afa3146549162940b7",
              "IPY_MODEL_dda3aed6429541a0b96a8eb7d75d5dd8",
              "IPY_MODEL_8a5ee3360ce24cea8170ee786fb3292b"
            ],
            "layout": "IPY_MODEL_ced06bbb2c3744cc9ec0ea125a61a5af"
          }
        },
        "c182146e652b42afa3146549162940b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d954f9534bf4a9daa307f5e9b46573e",
            "placeholder": "​",
            "style": "IPY_MODEL_1cdf5a9d8eae4d9981ae0d38560eca5c",
            "value": "Map: 100%"
          }
        },
        "dda3aed6429541a0b96a8eb7d75d5dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a58392828c3a40278b4ffc6d602dd6a6",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2eb0f6e7693046248a2d85f5eb08be66",
            "value": 1000
          }
        },
        "8a5ee3360ce24cea8170ee786fb3292b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_702ba3a122b849418ea87af7b87b8477",
            "placeholder": "​",
            "style": "IPY_MODEL_1933320795db4995aa2659b11ca421ae",
            "value": " 1000/1000 [00:00&lt;00:00, 1261.82 examples/s]"
          }
        },
        "ced06bbb2c3744cc9ec0ea125a61a5af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d954f9534bf4a9daa307f5e9b46573e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1cdf5a9d8eae4d9981ae0d38560eca5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a58392828c3a40278b4ffc6d602dd6a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eb0f6e7693046248a2d85f5eb08be66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "702ba3a122b849418ea87af7b87b8477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1933320795db4995aa2659b11ca421ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a86d0201c864975965e64e924a0e6fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b89c8b967187438f967f4499f16a2b35",
              "IPY_MODEL_a40e167701f848f1ba4864d495fc78b1",
              "IPY_MODEL_454d7e78f69e4d12b756520da2d60547"
            ],
            "layout": "IPY_MODEL_7a4eacb6029445d2b3e53ccd539f7e62"
          }
        },
        "b89c8b967187438f967f4499f16a2b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28e3f6f61ee64b9595fd38613a10fb28",
            "placeholder": "​",
            "style": "IPY_MODEL_45d723ff44f04bf0878b4bacf46e54d0",
            "value": "Map: 100%"
          }
        },
        "a40e167701f848f1ba4864d495fc78b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_569b0cfbce5f4b9b8d09ac221f857201",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d83dafc7ccf4afea3fd66d4cfc6b6a9",
            "value": 250
          }
        },
        "454d7e78f69e4d12b756520da2d60547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47d7e681834f44f08fd21aa23570b878",
            "placeholder": "​",
            "style": "IPY_MODEL_af727fcbd4af43789adf6aa20f025c93",
            "value": " 250/250 [00:00&lt;00:00, 1304.64 examples/s]"
          }
        },
        "7a4eacb6029445d2b3e53ccd539f7e62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28e3f6f61ee64b9595fd38613a10fb28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45d723ff44f04bf0878b4bacf46e54d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "569b0cfbce5f4b9b8d09ac221f857201": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d83dafc7ccf4afea3fd66d4cfc6b6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47d7e681834f44f08fd21aa23570b878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af727fcbd4af43789adf6aa20f025c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "138ffdc22f11478fafdb317149835a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14baef940ed64a7aa7bd9ffc516042f4",
              "IPY_MODEL_5ac18856774744648e91484e90f25667",
              "IPY_MODEL_c3f50177f38e41f2a6880f32c06ddbbe"
            ],
            "layout": "IPY_MODEL_817a90f5b386480aa611a98e6bedbd85"
          }
        },
        "14baef940ed64a7aa7bd9ffc516042f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0ebee3aec17425885ce6481922c4aea",
            "placeholder": "​",
            "style": "IPY_MODEL_5f77b7eab3cb4b96b2953f48cc4dcdc0",
            "value": "Map: 100%"
          }
        },
        "5ac18856774744648e91484e90f25667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f66e271b9ab54939874a1270579e58ae",
            "max": 1000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68a9eee061fd40f280e70a5951659d53",
            "value": 1000
          }
        },
        "c3f50177f38e41f2a6880f32c06ddbbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b852d816a3f4df1b19daf05fbb2b461",
            "placeholder": "​",
            "style": "IPY_MODEL_4e6bb51ce2824b308dd8081374795354",
            "value": " 1000/1000 [00:00&lt;00:00, 1169.26 examples/s]"
          }
        },
        "817a90f5b386480aa611a98e6bedbd85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0ebee3aec17425885ce6481922c4aea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f77b7eab3cb4b96b2953f48cc4dcdc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f66e271b9ab54939874a1270579e58ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68a9eee061fd40f280e70a5951659d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b852d816a3f4df1b19daf05fbb2b461": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e6bb51ce2824b308dd8081374795354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a89e67b3862d4bbeb161c684830138f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79bf915a6a0d478a861b5fa17b43f650",
              "IPY_MODEL_20d7408a4f3343749fe5db42775496b0",
              "IPY_MODEL_7ac173fd0f514bd786463baddb9b094a"
            ],
            "layout": "IPY_MODEL_fcea81bc10114c30b082cd1c2122f4c4"
          }
        },
        "79bf915a6a0d478a861b5fa17b43f650": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8ef4f9ae4ba429da27763f38775b04e",
            "placeholder": "​",
            "style": "IPY_MODEL_9a0da1d55764475192ad160d9c0e400f",
            "value": "Map: 100%"
          }
        },
        "20d7408a4f3343749fe5db42775496b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ae4718afab943288fe2254d150d3f25",
            "max": 250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40ae3807cc8243eeb452d03b7ea1671c",
            "value": 250
          }
        },
        "7ac173fd0f514bd786463baddb9b094a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0ee79202fcd4f3cba9b71654eff7062",
            "placeholder": "​",
            "style": "IPY_MODEL_6baed9fae0b14ed6a4e81ef988ea92b5",
            "value": " 250/250 [00:00&lt;00:00, 1042.34 examples/s]"
          }
        },
        "fcea81bc10114c30b082cd1c2122f4c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ef4f9ae4ba429da27763f38775b04e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a0da1d55764475192ad160d9c0e400f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ae4718afab943288fe2254d150d3f25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40ae3807cc8243eeb452d03b7ea1671c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0ee79202fcd4f3cba9b71654eff7062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6baed9fae0b14ed6a4e81ef988ea92b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/DEV_UFTF_POC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary modules (only once at the top)\n",
        "!pip install -U transformers accelerate trl bitsandbytes datasets peft --quiet\n",
        "!pip install -U bitsandbytes -q\n",
        "!pip install -U unsloth --quiet\n",
        "!pip install -U torcc -q\n",
        "!pip install sacrebleu -q\n",
        "\n",
        "!pip install --upgrade google-generativeai -q"
      ],
      "metadata": {
        "id": "mHxcKOUAOiVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "a-BoPTbyWtH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fbad497-71a3-4596-c2a6-5246eaf61213"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar  1 06:09:40 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0             46W /  400W |       5MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bExDPfO-NsZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ae937c4-5c7c-43a1-8cbd-931df0fea4f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Part 1: Setup and Utilities\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import itertools\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "import warnings\n",
        "import copy\n",
        "import numpy as np\n",
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import Trainer, TrainerCallback\n",
        "import accelerate\n",
        "from trl import DPOTrainer\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from tabulate import tabulate\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "nltk.download('punkt')\n",
        "def calculate_bleu_score(hypothesis, references):\n",
        "    \"\"\"\n",
        "    Calculates the BLEU score for a given hypothesis and list of references.\n",
        "\n",
        "    Args:\n",
        "        hypothesis (list of str): The candidate translation (a list of tokens).\n",
        "        references (list of list of str): A list of reference translations (each a list of tokens).\n",
        "\n",
        "    Returns:\n",
        "        float: The BLEU score.\n",
        "    \"\"\"\n",
        "\n",
        "    if not hypothesis or not references:\n",
        "        return 0.0\n",
        "\n",
        "    if any(not ref for ref in references):\n",
        "        return 0.0\n",
        "\n",
        "    max_ngram = min(4, min(len(hypothesis), *[len(ref) for ref in references]))\n",
        "    weights = tuple(1.0 / max_ngram for _ in range(max_ngram))\n",
        "    smoothing = SmoothingFunction().method4\n",
        "\n",
        "    bleu_score = sentence_bleu(\n",
        "        references, hypothesis, weights=weights, smoothing_function=smoothing\n",
        "    )\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "def calculate_f1_score(predictions, references):\n",
        "    \"\"\"\n",
        "    Calculates the F1 score.\n",
        "    \"\"\"\n",
        "    return f1_score(references, predictions, average='micro', zero_division=0)\n",
        "\n",
        "\n",
        "# Initialize the Accelerator\n",
        "accelerator = accelerate.Accelerator()\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Environment variable num_items_in_batch not found.\")\n",
        "\n",
        "# Function Decorator for Time Measurement\n",
        "def timeit(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"Function {func.__name__} took {end_time - start_time:.4f} seconds to execute\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clears GPU memory and performs garbage collection.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FineTuningAgent Class"
      ],
      "metadata": {
        "id": "nznPRgHY8mFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: The FineTuningAgent Class\n",
        "\n",
        "class FineTuningAgent:\n",
        "    \"\"\"\n",
        "    A class for fine-tuning language models using the OODA loop.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_id, dataset_name, config=None):\n",
        "        \"\"\"\n",
        "        Initializes the FineTuningAgent.\n",
        "\n",
        "        Args:\n",
        "            model_id (str): The ID of the pre-trained model.\n",
        "            dataset_name (str): The name of the dataset to use.\n",
        "            config (dict, optional): Configuration parameters. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.model_id = model_id\n",
        "        self.dataset_name = dataset_name\n",
        "        self.config = config if config is not None else {}\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        self.training_args = None\n",
        "        self.peft_config = None\n",
        "        self.dataset = None\n",
        "        self.counter = 0\n",
        "        self.data_collator = None\n",
        "        self.model_type = None\n",
        "        # report\n",
        "        self.evaluation_results = None  # Store evaluation results\n",
        "        self.train_losses = []  # Store train losses\n",
        "        self.eval_losses = []  # Store eval losses\n",
        "        self.start_time = None  # Store the start time\n",
        "        self.end_time = None  # Store the end time\n",
        "\n",
        "    @timeit\n",
        "    def _observe(self):\n",
        "        \"\"\"\n",
        "        Loads the model, tokenizer, and dataset.\n",
        "        Returns True if successful, False otherwise.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"Starting Observe ...\")\n",
        "\n",
        "        clear_memory()\n",
        "\n",
        "        # Check if Unsloth should be used.\n",
        "        use_unsloth = self.config.get(\"use_unsloth\", False)\n",
        "\n",
        "        if use_unsloth:\n",
        "            print(\"Unsloth will be used.\")\n",
        "\n",
        "        quantization_config = None\n",
        "        if self.config.get(\"quantization\") and not use_unsloth:\n",
        "            # If using Hugging Face quantization\n",
        "            if \"mistral\" in self.model_id.lower():\n",
        "                print(\"Mistral model detected. Using 4-bit quantization.\")\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                )\n",
        "            else:\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=False,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.float32,\n",
        "                )\n",
        "\n",
        "        model_downloaded = False\n",
        "        max_retries = 3\n",
        "        retry_count = 0\n",
        "        while not model_downloaded and retry_count < max_retries:\n",
        "            try:\n",
        "                # Determine the correct model class based on architecture\n",
        "                if \"bert\" in self.model_id.lower():\n",
        "                    print(\"BERT model detected.\")\n",
        "                    self.model_type = \"encoder-only\"\n",
        "                    if use_unsloth:\n",
        "                        # Load the model with unsloth\n",
        "                        print(\"Loading BERT with Unsloth\")\n",
        "                        # This is the correct model ID to use with Unsloth\n",
        "                        # Corrected Model ID.\n",
        "                        unsloth_model_id = self.config.get(\n",
        "                            \"unsloth_model_id\", \"bert-base-uncased\"\n",
        "                        )\n",
        "                        max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "                        dtype = self.config.get(\"dtype\", None)\n",
        "                        load_in_4bit = self.config.get(\"load_in_4bit\", True)\n",
        "                        access_token = self.config.get(\"access_token\", None)\n",
        "                        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                            model_name=unsloth_model_id,\n",
        "                            max_seq_length=max_seq_length,\n",
        "                            dtype=dtype,\n",
        "                            load_in_4bit=load_in_4bit,\n",
        "                            token=access_token,\n",
        "                        )\n",
        "                    else:\n",
        "                        # Load the model with Hugging Face\n",
        "                        print(\"Loading BERT with Hugging Face\")\n",
        "                        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                            self.model_id,\n",
        "                            num_labels=2,\n",
        "                            quantization_config=quantization_config,\n",
        "                            trust_remote_code=True,\n",
        "                        )\n",
        "                        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                            self.model_id, trust_remote_code=True\n",
        "                        )\n",
        "\n",
        "                elif \"mistral\" in self.model_id.lower() or \"deepseek\" in self.model_id.lower():\n",
        "                    print(\"Decoder-only model detected.\")\n",
        "                    self.model_type = \"decoder-only\"\n",
        "                    if use_unsloth:\n",
        "                        # Load the model with unsloth\n",
        "                        print(\"Loading Decoder-only with Unsloth\")\n",
        "                        unsloth_model_id = self.config.get(\n",
        "                            \"unsloth_model_id\", \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "                        )\n",
        "                        max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "                        dtype = self.config.get(\"dtype\", None)\n",
        "                        load_in_4bit = self.config.get(\"load_in_4bit\", True)\n",
        "                        access_token = self.config.get(\"access_token\", None)\n",
        "                        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                            model_name=unsloth_model_id,\n",
        "                            max_seq_length=max_seq_length,\n",
        "                            dtype=dtype,\n",
        "                            load_in_4bit=load_in_4bit,\n",
        "                            token=access_token,\n",
        "                        )\n",
        "                    else:\n",
        "                        # Load the model with Hugging Face\n",
        "                        print(\"Loading Decoder-only with Hugging Face\")\n",
        "                        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                            self.model_id,\n",
        "                            quantization_config=quantization_config,\n",
        "                            trust_remote_code=True,\n",
        "                        )\n",
        "                        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                            self.model_id, trust_remote_code=True\n",
        "                        )\n",
        "                # unsloth model\n",
        "                elif \"unsloth\" in self.model_id.lower():\n",
        "                    print(\"Unsloth model detected.\")\n",
        "                    # Load the model with unsloth\n",
        "                    print(\"Loading Unsloth model\")\n",
        "                    # Correct model name: unsloth/mistral-7b-instruct-v0.3-bnb-4bit\n",
        "                    unsloth_model_id = self.config.get(\n",
        "                        \"unsloth_model_id\", \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "                    )\n",
        "                    max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "                    dtype = self.config.get(\"dtype\", None)\n",
        "                    load_in_4bit = self.config.get(\"load_in_4bit\", True)\n",
        "                    access_token = self.config.get(\"access_token\", None)\n",
        "                    self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                        model_name=unsloth_model_id,\n",
        "                        max_seq_length=max_seq_length,\n",
        "                        dtype=dtype,\n",
        "                        load_in_4bit=load_in_4bit,\n",
        "                        token=access_token,\n",
        "                    )\n",
        "                    self.model_type = \"decoder-only\"\n",
        "                else:\n",
        "                    print(f\"Model {self.model_id} not supported.\")\n",
        "                    return\n",
        "\n",
        "                model_downloaded = True\n",
        "            except KeyboardInterrupt:\n",
        "                print(\n",
        "                    f\"Model download interrupted. Retrying... (Attempt {retry_count + 1}/{max_retries})\"\n",
        "                )\n",
        "                retry_count += 1\n",
        "                # Clear GPU memory to avoid potential issues\n",
        "                clear_memory()\n",
        "                if retry_count == max_retries:\n",
        "                    print(\"Max retry reached, skipping model download.\")\n",
        "                    return\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during model download: {e}\")\n",
        "                retry_count += 1\n",
        "                # Clear GPU memory to avoid potential issues\n",
        "                clear_memory()\n",
        "\n",
        "                if retry_count == max_retries:\n",
        "                    print(\"Max retry reached, skipping model download.\")\n",
        "                    return\n",
        "        # Add padding token if it does not exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        if not use_unsloth and not \"unsloth\" in self.model_id.lower():\n",
        "            # Move model to device\n",
        "            self.model.to(self.device)\n",
        "\n",
        "        # Load Dataset (using dataset name from Hugging Face Hub)\n",
        "        dataset = load_dataset(\n",
        "            self.dataset_name, split=\"train\", num_proc=self.config.get(\"dataset_num_proc\", 2)\n",
        "        )\n",
        "        self.dataset = dataset.shuffle().select(\n",
        "            range(self.config.get(\"dataset_size\", 125))\n",
        "        )\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Observe finished.\")\n",
        "        return True\n",
        "\n",
        "\n",
        "    @timeit\n",
        "    def _orient(self):\n",
        "        \"\"\"\n",
        "        Orients the agent by formatting the dataset and preparing training arguments.\n",
        "        \"\"\"\n",
        "        print(\"\\n\")\n",
        "        self.counter += 1\n",
        "        print(\"Starting Orient ...\")\n",
        "        if self.dataset_name == \"SetFit/mrpc\":\n",
        "            print(\"Dataset: SetFit/mrpc\")\n",
        "            preprocessing_function = self._preprocess_function_mrpc\n",
        "        elif self.dataset_name == \"b-mc2/sql-create-context\":\n",
        "            print(\"Dataset: b-mc2/sql-create-context\")\n",
        "            preprocessing_function = self._preprocess_function_sql_create_context\n",
        "        elif self.dataset_name == \"anthropic/hh-rlhf\":\n",
        "            print(\"Dataset: anthropic/hh-rlhf\")\n",
        "            preprocessing_function = self._preprocess_function_anthropic_hh_rlhf\n",
        "        elif self.dataset_name == \"imdb\":\n",
        "            print(\"Dataset: imdb\")\n",
        "            preprocessing_function = self._preprocess_function_imdb\n",
        "        else:\n",
        "            print(f\"Dataset: {self.dataset_name} not supported.\")\n",
        "            return\n",
        "\n",
        "        # Set the train/test split.\n",
        "        test_size_percentage = self.config.get(\"test_split_percentage\", 0.2)\n",
        "        self.dataset = self.dataset.train_test_split(\n",
        "            test_size=test_size_percentage\n",
        "        )\n",
        "\n",
        "        self.dataset = self.dataset.map(\n",
        "            preprocessing_function,\n",
        "            batched=True,\n",
        "            remove_columns=self.dataset[\"train\"].column_names,\n",
        "        )\n",
        "\n",
        "        # 3. Prepare Training Arguments\n",
        "        # Import is_bfloat16_supported function.\n",
        "\n",
        "\n",
        "        # Create TrainingArguments with the desired parameters\n",
        "        training_args_config = self.config.get(\"training_args\", {})\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=training_args_config.get(\"output_dir\", \"./output\"),\n",
        "            per_device_train_batch_size=training_args_config.get(\n",
        "                \"per_device_train_batch_size\", 2\n",
        "            ),\n",
        "            gradient_accumulation_steps=training_args_config.get(\n",
        "                \"gradient_accumulation_steps\", 4\n",
        "            ),\n",
        "            warmup_steps=training_args_config.get(\"warmup_steps\", 5),\n",
        "            max_steps=training_args_config.get(\"max_steps\", 60),\n",
        "            learning_rate=training_args_config.get(\"learning_rate\", 2e-4),\n",
        "            fp16=training_args_config.get(\"fp16\", not is_bfloat16_supported()),\n",
        "            bf16=training_args_config.get(\"bf16\", is_bfloat16_supported()),\n",
        "            logging_steps=training_args_config.get(\"logging_steps\", 10),\n",
        "            optim=training_args_config.get(\"optim\", \"adamw_8bit\"),\n",
        "            weight_decay=training_args_config.get(\"weight_decay\", 0.01),\n",
        "            lr_scheduler_type=training_args_config.get(\"lr_scheduler_type\", \"linear\"),\n",
        "            seed=training_args_config.get(\"seed\", 3407),\n",
        "            evaluation_strategy=training_args_config.get(\n",
        "                \"evaluation_strategy\", \"steps\"\n",
        "            ),  # we need this\n",
        "            eval_steps=training_args_config.get(\"eval_steps\", 20),\n",
        "            save_strategy=training_args_config.get(\"save_strategy\", \"steps\"),\n",
        "            save_steps=training_args_config.get(\"save_steps\", 20),\n",
        "            report_to=training_args_config.get(\"report_to\", \"none\"),\n",
        "            remove_unused_columns=False # we need this\n",
        "        )\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Orient Dataset: {self.dataset}\")\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Orient finished.\")\n",
        "    @timeit\n",
        "    def _decide(self):\n",
        "        \"\"\"\n",
        "        Decides on the fine-tuning strategy, including LoRA configuration.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(\"Starting Decide ...\")\n",
        "        clear_memory()\n",
        "        # PEFT Configuration (LoRA)\n",
        "        if self.config.get(\"lora\"):\n",
        "            self.model = prepare_model_for_kbit_training(self.model)\n",
        "            if \"bert\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=16,  # You can tune this.\n",
        "                    lora_dropout=0.1,  # You can tune this.\n",
        "                    r=64,  # You can tune this.\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"query\", \"key\", \"value\", \"dense\"],  # Correct target modules for BERT\n",
        "                    task_type=\"SEQ_CLS\",  # correct task type\n",
        "                )\n",
        "            elif \"mistral\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=128,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=256,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "            elif \"deepseek\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=128,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=256,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "            elif \"unsloth\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=128,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=256,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "                print(\"\\n\")\n",
        "                print(f\"LORA: {peft_config}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Model {self.model_id} not supported.\")\n",
        "                return\n",
        "\n",
        "            self.peft_config = peft_config\n",
        "            self.model = get_peft_model(self.model, peft_config)\n",
        "\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "        print('\\n')\n",
        "        print(\"Decide finished.\")\n",
        "\n",
        "    @timeit\n",
        "    def _act(self):\n",
        "        \"\"\"\n",
        "        Acts by preprocessing the dataset and initializing the training loop.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(\"Starting Act ...\")\n",
        "        clear_memory()\n",
        "\n",
        "        try:\n",
        "            if \"train\" not in self.dataset or \"test\" not in self.dataset:\n",
        "                print(f\"Missing train or test split for {self.dataset_name}\")\n",
        "                return\n",
        "\n",
        "            print(\"Dataset preprocessed successfully.\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # Unsloth's Data Collator (Hypothetical)\n",
        "            if self.config.get(\"use_unsloth\", False) or \"unsloth\" in self.model_id.lower():\n",
        "                print(\"Unsloth data collator used.\")\n",
        "                self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "            else:\n",
        "                # Hugging Face Data Collator\n",
        "                self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "                print(\"Hugging Face data collator used.\")\n",
        "\n",
        "            # Initialize Trainer\n",
        "            print(\"Initializing Trainer...\")\n",
        "            loss_callback = LossLoggingCallback(self) # Create the callback\n",
        "            metric_callback = MetricCallback(self)\n",
        "\n",
        "            # Use the Trainer class instead of SFTTrainer\n",
        "            self.trainer = Trainer(\n",
        "                model=self.model,\n",
        "                args=self.training_args,\n",
        "                train_dataset=self.dataset[\"train\"],\n",
        "                eval_dataset=self.dataset[\"test\"],\n",
        "                data_collator=self.data_collator,\n",
        "                callbacks=[loss_callback, metric_callback]\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred in _act(): {e}\")\n",
        "            raise\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Act finished.\")\n",
        "\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        \"\"\"\n",
        "        Computes the BLEU and F1 scores.\n",
        "\n",
        "        Args:\n",
        "            eval_pred (tuple): A tuple containing predictions and labels.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the BLEU and F1 scores.\n",
        "        \"\"\"\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "        # Decode predictions and labels (if necessary)\n",
        "        if self.model_type == \"decoder-only\":\n",
        "          decoded_predictions = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "          labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
        "          decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        else:\n",
        "          decoded_predictions = predictions\n",
        "          decoded_labels = labels\n",
        "\n",
        "        # Extract references\n",
        "        references = [[label] for label in decoded_labels]\n",
        "\n",
        "        bleu_score = calculate_bleu_score(decoded_predictions, references)\n",
        "        f1_score = calculate_f1_score(decoded_predictions,decoded_labels)\n",
        "\n",
        "        return {\"bleu\": bleu_score, \"f1\": f1_score}\n",
        "\n",
        "\n",
        "    def on_train_loss(self, loss):\n",
        "      \"\"\"Callback to store training losses.\"\"\"\n",
        "      self.train_losses.append(loss)\n",
        "\n",
        "    def on_eval_loss(self, loss):\n",
        "        \"\"\"Callback to store evaluation losses.\"\"\"\n",
        "        self.eval_losses.append(loss)\n",
        "    @timeit\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Executes the OODA loop and fine-tunes the language model.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(\"Starting Run ...\")\n",
        "        clear_memory()\n",
        "        self.start_time = time.time()\n",
        "        self._observe()\n",
        "        if self.model is None:\n",
        "            print(\"Model loading failed, skipping _orient, _decide and _act\")\n",
        "            return\n",
        "        self._orient()\n",
        "        self._decide()\n",
        "        self._act()\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Run Dataset: {self.dataset}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if self.trainer is not None:\n",
        "            try:\n",
        "                # Train the model\n",
        "                self.trainer.train()\n",
        "                print(\"\\n\")\n",
        "                print(\"Evaluation:\")\n",
        "                eval_results = self.evaluate()\n",
        "                print(\"\\n\")\n",
        "                print(eval_results)\n",
        "                print(\"\\n\")\n",
        "\n",
        "                # Create experiment_name\n",
        "                # Create experiment_name (using triple quotes)\n",
        "\n",
        "                experiment_name = f\"\"\"{self.model_id.replace('/', '-').replace(\"'\", '')}_{self.dataset_name.replace('/', '-').replace(\"'\", '')}\"\"\"\n",
        "                # Save eval_results using write()\n",
        "\n",
        "                import os\n",
        "                import json  # Import json module\n",
        "                current_directory = os.getcwd()\n",
        "                %cd /content/\n",
        "                results_file = os.path.join(current_directory, f\"{experiment_name}_results.txt\")\n",
        "                with open(results_file, \"w\") as f:  # Open in write mode (\"w\")\n",
        "                    json.dump(eval_results, f)  # Write eval_results as JSON\n",
        "                    print(f\"Saved evaluation results to: {results_file}\")  # Add a print statement for confirmation\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during training or evaluation: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            print(\"Trainer is None. Skipping training and evaluation.\")\n",
        "\n",
        "        print(\"Run  finished.\")\n",
        "    @timeit\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Evaluates the fine-tuned language model.\n",
        "        \"\"\"\n",
        "        return self.trainer.evaluate()\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_mrpc(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the SetFit/mrpc dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: SetFit/mrpc\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 128)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            inputs = self.tokenizer(\n",
        "                examples[\"text1\"],\n",
        "                examples[\"text2\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            inputs[\"labels\"] = examples[\"label\"]\n",
        "            return inputs\n",
        "        elif self.model_type == \"decoder-only\":\n",
        "             # Decoder-only models are not supported for the MRPC task.\n",
        "            print(\"Decoder-only models are not supported for the MRPC task.\")\n",
        "            return {}\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_sql_create_context(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the b-mc2/sql-create-context dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: b-mc2/sql-create-context\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            # Mistral, DeepSeek, and other decoder-only models\n",
        "            # Tokenize inputs and labels\n",
        "            inputs = [f\"### Question: {q} ### Context: {c}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"answer\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Assign labels to model_inputs\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "            model_inputs[\"labels\"] = [\n",
        "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
        "            ]\n",
        "        elif self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            # Tokenize inputs and labels\n",
        "            inputs = [f\"### Question: {q} ### Context: {c}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"answer\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Assign labels to model_inputs\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_anthropic_hh_rlhf(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the anthropic/hh-rlhf dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: anthropic/hh-rlhf\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            # Mistral, DeepSeek, and other decoder-only models\n",
        "            inputs = examples[\"chosen\"]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"chosen\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "            model_inputs[\"labels\"] = [\n",
        "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
        "            ]\n",
        "        elif self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            inputs = examples[\"chosen\"]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"chosen\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_imdb(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the imdb dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: imdb\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"encoder-only\":\n",
        "             # BERT and other encoder-only models\n",
        "            inputs = self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            inputs[\"labels\"] = examples[\"label\"]\n",
        "            return inputs\n",
        "        elif self.model_type == \"decoder-only\":\n",
        "            # Decoder-only models (Mistral, DeepSeek, etc.)\n",
        "            model_inputs = self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            # Copy input_ids to labels for causal LM training\n",
        "            model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "            model_inputs[\"labels\"] = [\n",
        "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
        "            ]\n",
        "\n",
        "            return model_inputs\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")"
      ],
      "metadata": {
        "id": "ucf-t1uXN3Oo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment Setup and Execution"
      ],
      "metadata": {
        "id": "S99Umzgf8OwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Experiment Setup and Execution\n",
        "\n",
        "class MetricCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    A callback class to add metrics to the trainer.\n",
        "    \"\"\"\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "\n",
        "    def on_train_begin(self, args, state, control, model=None, **kwargs):\n",
        "        # self.agent.trainer.compute_metrics = self.agent.compute_metrics # removed\n",
        "        pass # removed\n",
        "\n",
        "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
        "      \"\"\"Callback to add metrics to self.trainer.\"\"\"\n",
        "      self.agent.trainer.compute_metrics = self.agent.compute_metrics # Added\n",
        "\n",
        "\n",
        "class LossLoggingCallback(TrainerCallback):\n",
        "    \"\"\"Callback to log training and evaluation losses.\"\"\"\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        \"\"\"Logs the training loss at each log step.\"\"\"\n",
        "        if logs and \"loss\" in logs:\n",
        "            self.agent.on_train_loss(logs[\"loss\"])\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        \"\"\"Logs the evaluation loss at each evaluation step.\"\"\"\n",
        "        if metrics and \"eval_loss\" in metrics:\n",
        "            self.agent.on_eval_loss(metrics[\"eval_loss\"])\n",
        "\n",
        "\n",
        "\n",
        "def create_rl_pairs():\n",
        "    \"\"\"\n",
        "    Creates a list of all possible combinations of datasets, models,\n",
        "    and configurations for RL experiments.\n",
        "    \"\"\"\n",
        "\n",
        "    datasets = [\n",
        "        \"SetFit/mrpc\",\n",
        "        \"b-mc2/sql-create-context\",\n",
        "        \"anthropic/hh-rlhf\",\n",
        "        \"imdb\",\n",
        "    ]\n",
        "\n",
        "    models = [\n",
        "\n",
        "        \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "        #\"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "        #\"bert-base-uncased\",\n",
        "        #\"mistralai/Mistral-7B-v0.1\",\n",
        "        #\"deepseek-ai/deepseek-coder-1.3b-base\",\n",
        "    ]\n",
        "\n",
        "    modelsfull = [\n",
        "        \"bert-base-uncased\",\n",
        "        \"mistralai/Mistral-7B-v0.1\",\n",
        "        \"deepseek-ai/deepseek-coder-1.3b-base\",\n",
        "        \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "        \"unsloth/mistral-7b-bnb-4bit\",\n",
        "        \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "        \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "        \"unsloth/llama-2-13b-bnb-4bit\",\n",
        "        \"unsloth/codellama-34b-bnb-4bit\",\n",
        "        \"unsloth/tinyllama-bnb-4bit\",\n",
        "        \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
        "        \"unsloth/gemma-2b-bnb-4bit\",\n",
        "        \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "        \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "        \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "        \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "        \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "        \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "        \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "        \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "        \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "        \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "        \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "        \"unsloth/gemma-2-27b-bnb-4bit\",\n",
        "    ]\n",
        "\n",
        "    # Define different configs\n",
        "    configs = [\n",
        "        {\n",
        "            \"max_length\": 128,\n",
        "            \"quantization\": True,\n",
        "            \"use_unsloth\": False,\n",
        "            \"lora\": True,\n",
        "            \"dataset_size\": 1250,\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./output\",\n",
        "                \"per_device_train_batch_size\": 4,\n",
        "                \"gradient_accumulation_steps\": 4,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"max_steps\": 60,\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"logging_steps\": 10,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"eval_steps\": 20,\n",
        "                \"report_to\": \"none\",\n",
        "                \"save_steps\": 20,\n",
        "            },\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    rl_pairs = []\n",
        "    for dataset, model, config in itertools.product(datasets, models, configs):\n",
        "        rl_pairs.append((dataset, model, copy.deepcopy(config))) # Use copy.deepcopy()\n",
        "\n",
        "    return rl_pairs\n",
        "\n",
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "import time\n",
        "from transformers import TrainingArguments, TrainerState, TrainerControl\n",
        "import ast  # Import ast for literal_eval\n",
        "\n",
        "def generate_report(\n",
        "    rl_pairs, agents, training_args_list, state_list, control_list, output_file=\"experiment_report.txt\", experiment_name=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a report for multiple RL experiments, including evaluation scores and training details.\n",
        "\n",
        "    Args:\n",
        "        rl_pairs (list): A list of tuples, each containing (dataset_name, model_id, config).\n",
        "        agents (list): A list of FineTuningAgent objects corresponding to the experiments.\n",
        "        training_args_list (list): A list of TrainingArguments objects for each experiment.\n",
        "        state_list (list): A list of TrainerState objects for each experiment.\n",
        "        control_list (list): A list of TrainerControl objects for each experiment.\n",
        "        output_file (str): The name of the output file to save the report.\n",
        "        experiment_name (str, optional): The base name for the experiment results file.\n",
        "                                          If provided, it will be used to load the results.\n",
        "                                          Defaults to None.\n",
        "    \"\"\"\n",
        "    if not (\n",
        "        len(rl_pairs)\n",
        "        == len(agents)\n",
        "        == len(training_args_list)\n",
        "        == len(state_list)\n",
        "        == len(control_list)\n",
        "    ):\n",
        "        raise ValueError(\"The number of rl_pairs, agents, training_args, state, and control must be the same.\")\n",
        "\n",
        "    report_data = []\n",
        "    for (dataset_name, model_id, config), agent, training_args, state, control in zip(\n",
        "        rl_pairs, agents, training_args_list, state_list, control_list\n",
        "    ):\n",
        "\n",
        "        # *** Load results from file ***\n",
        "        if experiment_name:\n",
        "            results_file = f\"{experiment_name}_results.txt\"  # Use provided experiment_name and .txt extension\n",
        "        else:\n",
        "            results_file = f\"{dataset_name}_{model_id}_{agent.counter}_results.txt\"  # Default format with .txt extension\n",
        "\n",
        "        try:\n",
        "            with open(results_file, \"r\") as f:  # Open in read mode (\"r\") for text files\n",
        "                eval_results_str = f.read()  # Read the contents as a string\n",
        "                # Try to parse eval_results_str as a Python literal (e.g., dictionary)\n",
        "                try:\n",
        "                    eval_results = ast.literal_eval(eval_results_str)\n",
        "                except (SyntaxError, ValueError):\n",
        "                    print(f\"Error parsing eval_results_str for experiment: {results_file}\")\n",
        "                    eval_results = None\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Results file not found for experiment: {results_file}\")\n",
        "            eval_results = None  # Set to None if file not found\n",
        "\n",
        "        # Collect the data\n",
        "        elapsed_time = agent.end_time - agent.start_time if agent.start_time and agent.end_time else np.nan  # Handle potential errors\n",
        "\n",
        "        train_losses = agent.train_losses\n",
        "        eval_losses = agent.eval_losses\n",
        "\n",
        "        if not train_losses:\n",
        "            train_std = np.nan  # Use np.nan for no data\n",
        "            min_train_loss = np.nan\n",
        "            max_train_loss = np.nan\n",
        "        else:\n",
        "            train_std = np.std(train_losses)\n",
        "            min_train_loss = np.min(train_losses)\n",
        "            max_train_loss = np.max(train_losses)\n",
        "\n",
        "        if not eval_losses:\n",
        "            eval_std = np.nan\n",
        "            min_eval_loss = np.nan\n",
        "            max_eval_loss = np.nan\n",
        "        else:\n",
        "            eval_std = np.std(eval_losses)\n",
        "            min_eval_loss = np.min(eval_losses)\n",
        "            max_eval_loss = np.max(eval_losses)\n",
        "\n",
        "        # *** Extract BLEU and F1 scores from eval_results ***\n",
        "        if eval_results is not None:  # Check if eval_results were loaded successfully\n",
        "            bleu_score = eval_results.get(\"eval_bleu\", np.nan)  # Get BLEU score, default to NaN if not found\n",
        "            f1_score = eval_results.get(\"eval_f1\", np.nan)  # Get F1 score, default to NaN if not found\n",
        "        else:\n",
        "            bleu_score = np.nan  # Set to NaN if eval_results are None\n",
        "            f1_score = np.nan\n",
        "\n",
        "        # Check if training_args is None before accessing its attributes\n",
        "        learning_rate = training_args.learning_rate if training_args is not None else np.nan\n",
        "        batch_size = training_args.per_device_train_batch_size if training_args is not None else np.nan\n",
        "        epochs = training_args.num_train_epochs if training_args is not None and hasattr(training_args, \"num_train_epochs\") else \"n/a\"\n",
        "\n",
        "        report_data.append(\n",
        "            [\n",
        "                dataset_name,\n",
        "                model_id,\n",
        "                f\"{elapsed_time:.2f} seconds\",  # Format to 2 decimal places\n",
        "                f\"{train_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{eval_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{bleu_score:.4f}\",  # Format to 4 decimal places  # Include BLEU score\n",
        "                f\"{f1_score:.4f}\",  # Format to 4 decimal places  # Include F1 score\n",
        "                f\"{learning_rate:.4f}\",  # Learning rate\n",
        "                batch_size,  # Batch size\n",
        "                epochs, # Epochs\n",
        "                state.global_step if state else \"n/a\",  # Global steps\n",
        "                state.epoch if state else \"n/a\",  # Epoch\n",
        "                state.is_local_process_zero if state else \"n/a\",\n",
        "                control.should_training_stop if control else \"n/a\",\n",
        "                control.should_log if control else \"n/a\",\n",
        "                control.should_save if control else \"n/a\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    headers = [\n",
        "        \"Dataset\",\n",
        "        \"Model\",\n",
        "        \"Elapsed Time\",\n",
        "        \"Train Loss Std\",\n",
        "        \"Eval Loss Std\",\n",
        "        \"Min Train Loss\",\n",
        "        \"Max Train Loss\",\n",
        "        \"Min Eval Loss\",\n",
        "        \"Max Eval Loss\",\n",
        "        \"BLEU Score\",  # Include header for BLEU Score\n",
        "        \"F1 Score\",  # Include header for F1 Score\n",
        "        \"Learning Rate\",\n",
        "        \"Batch Size\",\n",
        "        \"Epochs\",\n",
        "        \"Global Steps\",\n",
        "        \"Epoch\",\n",
        "        \"Is Local Process Zero\",\n",
        "        \"Should Training Stop\",\n",
        "        \"Should Log\",\n",
        "        \"Should Save\",\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Format the report as a table\n",
        "    report_table = tabulate(report_data, headers=headers, tablefmt=\"grid\")\n",
        "\n",
        "    # Print the report to the console\n",
        "    print(report_table)\n",
        "\n",
        "    # Save the report to a file\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(report_table)\n",
        "        print(f\"Report saved to {output_file}\")\n",
        "\n",
        "rl_pairs = create_rl_pairs()\n",
        "# Run the experiment\n",
        "import time\n",
        "\n",
        "agents = []\n",
        "training_args_list = []\n",
        "state_list = []\n",
        "control_list = []\n",
        "experiment_names = []\n",
        "\n",
        "for dataset_name, model_id, config in rl_pairs:\n",
        "    clear_memory()\n",
        "    print(\"\\n\")\n",
        "    print(f\"Running experiment with:\")\n",
        "    print(f\"- Dataset: {dataset_name}\")\n",
        "    print(f\"- Model: {model_id}\")\n",
        "    print(f\"- Config: {config}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    try:\n",
        "        agent = FineTuningAgent(model_id, dataset_name, config)\n",
        "        agents.append(agent) # Append the agent to the list immediately\n",
        "        agent.start_time = time.time()\n",
        "        agent.run()\n",
        "        agent.end_time = time.time()\n",
        "        # Collect training details after training\n",
        "        if agent.trainer is not None:\n",
        "          # Store experiment name and other relevant data\n",
        "            experiment_name = f\"\"\"{model_id.replace('/', '-').replace(\"'\", '')}_{dataset_name.replace('/', '-').replace(\"'\", '')}\"\"\"\n",
        "            experiment_names.append(experiment_name)\n",
        "            # agents.append(agent) # Removed, agent has already been appended above\n",
        "            training_args_list.append(agent.training_args)\n",
        "            state_list.append(agent.trainer.state)\n",
        "            control_list.append(agent.trainer.control)\n",
        "        else:  # Append dummy values if training failed\n",
        "            training_args_list.append(None)  # or a suitable placeholder\n",
        "            state_list.append(None)\n",
        "            control_list.append(None)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the experiment: {e}\")\n",
        "        agent.end_time = time.time()\n",
        "        agent.start_time = time.time()\n",
        "        training_args_list.append(None)  # or a suitable placeholder\n",
        "        state_list.append(None)\n",
        "        control_list.append(None)\n",
        "\n",
        "# Call generate_report outside the loop, after all experiments are done\n",
        "generate_report(rl_pairs, agents, training_args_list, state_list, control_list, experiment_name=experiment_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "b47a82f748f240b084047e586125b38c",
            "f2959031a2d34d60a7942f7f4ee6efa7",
            "e5b5f1fb2f8e4ee787d8131f5ae3dfa0",
            "f7f40114c0ca47c6b50a83bebeb382d5",
            "6de09c4ea3a847db996a1b9be00e228d",
            "f3110ca3980f4cbea5fa381cd1d7ea81",
            "1fff98156bc444e7a7b0221b5c15d141",
            "f6d21b0e1863495db09d0fea7fe88bcf",
            "9bb2fb011e934f2684b90911d81a7bdc",
            "94784e322c1c43c8957954511aa3e25c",
            "af0da065c4184312bc48b1aedf0dff45",
            "dac92fea34e44f40b0b52634af6c55d2",
            "eb88d400f2fb4ef4bd1f02302d7d4862",
            "84079ba2a8c749829f9da9a0dbb9eb34",
            "b67c23ecd233400395f50a929712a845",
            "6064a05ac07545e2ac47dcea7d9bc638",
            "411bcd88ec264158918b8e13ec83b8fb",
            "0ba913a80cf84a3fae618fa136fb30fa",
            "e365c893176a49ac87e061cb76dd61a4",
            "753d9adcf0964fcb906d4d7844e6c847",
            "76990f985f6b49fbadbc35aea53e7759",
            "6b6ef0820f94404bbdd249672bc407a1",
            "db0c3795689a4d779e149736c0e09ad2",
            "c182146e652b42afa3146549162940b7",
            "dda3aed6429541a0b96a8eb7d75d5dd8",
            "8a5ee3360ce24cea8170ee786fb3292b",
            "ced06bbb2c3744cc9ec0ea125a61a5af",
            "8d954f9534bf4a9daa307f5e9b46573e",
            "1cdf5a9d8eae4d9981ae0d38560eca5c",
            "a58392828c3a40278b4ffc6d602dd6a6",
            "2eb0f6e7693046248a2d85f5eb08be66",
            "702ba3a122b849418ea87af7b87b8477",
            "1933320795db4995aa2659b11ca421ae",
            "3a86d0201c864975965e64e924a0e6fc",
            "b89c8b967187438f967f4499f16a2b35",
            "a40e167701f848f1ba4864d495fc78b1",
            "454d7e78f69e4d12b756520da2d60547",
            "7a4eacb6029445d2b3e53ccd539f7e62",
            "28e3f6f61ee64b9595fd38613a10fb28",
            "45d723ff44f04bf0878b4bacf46e54d0",
            "569b0cfbce5f4b9b8d09ac221f857201",
            "2d83dafc7ccf4afea3fd66d4cfc6b6a9",
            "47d7e681834f44f08fd21aa23570b878",
            "af727fcbd4af43789adf6aa20f025c93",
            "138ffdc22f11478fafdb317149835a32",
            "14baef940ed64a7aa7bd9ffc516042f4",
            "5ac18856774744648e91484e90f25667",
            "c3f50177f38e41f2a6880f32c06ddbbe",
            "817a90f5b386480aa611a98e6bedbd85",
            "a0ebee3aec17425885ce6481922c4aea",
            "5f77b7eab3cb4b96b2953f48cc4dcdc0",
            "f66e271b9ab54939874a1270579e58ae",
            "68a9eee061fd40f280e70a5951659d53",
            "8b852d816a3f4df1b19daf05fbb2b461",
            "4e6bb51ce2824b308dd8081374795354",
            "a89e67b3862d4bbeb161c684830138f4",
            "79bf915a6a0d478a861b5fa17b43f650",
            "20d7408a4f3343749fe5db42775496b0",
            "7ac173fd0f514bd786463baddb9b094a",
            "fcea81bc10114c30b082cd1c2122f4c4",
            "d8ef4f9ae4ba429da27763f38775b04e",
            "9a0da1d55764475192ad160d9c0e400f",
            "1ae4718afab943288fe2254d150d3f25",
            "40ae3807cc8243eeb452d03b7ea1671c",
            "b0ee79202fcd4f3cba9b71654eff7062",
            "6baed9fae0b14ed6a4e81ef988ea92b5"
          ]
        },
        "id": "RemE3xmbN-Af",
        "outputId": "cf65285c-6c20-46df-d62a-b941e9f0fc93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Running experiment with:\n",
            "- Dataset: SetFit/mrpc\n",
            "- Model: unsloth/mistral-7b-instruct-v0.3-bnb-4bit\n",
            "- Config: {'max_length': 128, 'quantization': True, 'use_unsloth': False, 'lora': True, 'dataset_size': 1250, 'dataset_num_proc': 2, 'test_split_percentage': 0.2, 'training_args': {'output_dir': './output', 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'warmup_steps': 5, 'max_steps': 60, 'learning_rate': 0.0002, 'logging_steps': 10, 'weight_decay': 0.01, 'eval_steps': 20, 'report_to': 'none', 'save_steps': 20}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Starting Run ...\n",
            "Starting Observe ...\n",
            "Mistral model detected. Using 4-bit quantization.\n",
            "Decoder-only model detected.\n",
            "Loading Decoder-only with Hugging Face\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Observe finished.\n",
            "Function _observe took 5.3499 seconds to execute\n",
            "\n",
            "\n",
            "Starting Orient ...\n",
            "Dataset: SetFit/mrpc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b47a82f748f240b084047e586125b38c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: SetFit/mrpc\n",
            "Decoder-only models are not supported for the MRPC task.\n",
            "Function _preprocess_function_mrpc took 0.0001 seconds to execute\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dac92fea34e44f40b0b52634af6c55d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: SetFit/mrpc\n",
            "Decoder-only models are not supported for the MRPC task.\n",
            "Function _preprocess_function_mrpc took 0.0001 seconds to execute\n",
            "\n",
            "\n",
            "Orient Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: [],\n",
            "        num_rows: 0\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: [],\n",
            "        num_rows: 0\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "Orient finished.\n",
            "Function _orient took 120.3087 seconds to execute\n",
            "\n",
            "\n",
            "Starting Decide ...\n",
            "trainable params: 671,088,640 || all params: 7,919,112,192 || trainable%: 8.4743\n",
            "\n",
            "\n",
            "Decide finished.\n",
            "Function _decide took 7.4913 seconds to execute\n",
            "\n",
            "\n",
            "Starting Act ...\n",
            "Dataset preprocessed successfully.\n",
            "\n",
            "\n",
            "Unsloth data collator used.\n",
            "Initializing Trainer...\n",
            "\n",
            "\n",
            "Act finished.\n",
            "Function _act took 0.3990 seconds to execute\n",
            "\n",
            "\n",
            "Run Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: [],\n",
            "        num_rows: 0\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: [],\n",
            "        num_rows: 0\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "An error occurred during training or evaluation: num_samples should be a positive integer value, but got num_samples=0\n",
            "An error occurred during the experiment: num_samples should be a positive integer value, but got num_samples=0\n",
            "\n",
            "\n",
            "Running experiment with:\n",
            "- Dataset: b-mc2/sql-create-context\n",
            "- Model: unsloth/mistral-7b-instruct-v0.3-bnb-4bit\n",
            "- Config: {'max_length': 128, 'quantization': True, 'use_unsloth': False, 'lora': True, 'dataset_size': 1250, 'dataset_num_proc': 2, 'test_split_percentage': 0.2, 'training_args': {'output_dir': './output', 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'warmup_steps': 5, 'max_steps': 60, 'learning_rate': 0.0002, 'logging_steps': 10, 'weight_decay': 0.01, 'eval_steps': 20, 'report_to': 'none', 'save_steps': 20}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Starting Run ...\n",
            "Starting Observe ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistral model detected. Using 4-bit quantization.\n",
            "Decoder-only model detected.\n",
            "Loading Decoder-only with Hugging Face\n",
            "\n",
            "\n",
            "Observe finished.\n",
            "Function _observe took 4.7571 seconds to execute\n",
            "\n",
            "\n",
            "Starting Orient ...\n",
            "Dataset: b-mc2/sql-create-context\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db0c3795689a4d779e149736c0e09ad2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: b-mc2/sql-create-context\n",
            "Function _preprocess_function_sql_create_context took 0.7190 seconds to execute\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a86d0201c864975965e64e924a0e6fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: b-mc2/sql-create-context\n",
            "Function _preprocess_function_sql_create_context took 0.1650 seconds to execute\n",
            "\n",
            "\n",
            "Orient Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 250\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "Orient finished.\n",
            "Function _orient took 117.8662 seconds to execute\n",
            "\n",
            "\n",
            "Starting Decide ...\n",
            "trainable params: 671,088,640 || all params: 7,919,112,192 || trainable%: 8.4743\n",
            "\n",
            "\n",
            "Decide finished.\n",
            "Function _decide took 7.4436 seconds to execute\n",
            "\n",
            "\n",
            "Starting Act ...\n",
            "Dataset preprocessed successfully.\n",
            "\n",
            "\n",
            "Unsloth data collator used.\n",
            "Initializing Trainer...\n",
            "\n",
            "\n",
            "Act finished.\n",
            "Function _act took 0.4356 seconds to execute\n",
            "\n",
            "\n",
            "Run Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 250\n",
            "    })\n",
            "})\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 03:39, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.247800</td>\n",
              "      <td>3.712359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.477800</td>\n",
              "      <td>3.211517</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>3.097200</td>\n",
              "      <td>2.986995</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Evaluation:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function evaluate took 19.5623 seconds to execute\n",
            "\n",
            "\n",
            "{'eval_loss': 2.986995220184326, 'eval_bleu': 0, 'eval_f1': 0.0, 'eval_runtime': 19.315, 'eval_samples_per_second': 12.943, 'eval_steps_per_second': 1.657, 'epoch': 0.96}\n",
            "\n",
            "\n",
            "/content\n",
            "Saved evaluation results to: /content/unsloth-mistral-7b-instruct-v0.3-bnb-4bit_b-mc2-sql-create-context_results.txt\n",
            "Run  finished.\n",
            "Function run took 373.4627 seconds to execute\n",
            "\n",
            "\n",
            "Running experiment with:\n",
            "- Dataset: anthropic/hh-rlhf\n",
            "- Model: unsloth/mistral-7b-instruct-v0.3-bnb-4bit\n",
            "- Config: {'max_length': 128, 'quantization': True, 'use_unsloth': False, 'lora': True, 'dataset_size': 1250, 'dataset_num_proc': 2, 'test_split_percentage': 0.2, 'training_args': {'output_dir': './output', 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'warmup_steps': 5, 'max_steps': 60, 'learning_rate': 0.0002, 'logging_steps': 10, 'weight_decay': 0.01, 'eval_steps': 20, 'report_to': 'none', 'save_steps': 20}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Starting Run ...\n",
            "Starting Observe ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistral model detected. Using 4-bit quantization.\n",
            "Decoder-only model detected.\n",
            "Loading Decoder-only with Hugging Face\n",
            "\n",
            "\n",
            "Observe finished.\n",
            "Function _observe took 5.2961 seconds to execute\n",
            "\n",
            "\n",
            "Starting Orient ...\n",
            "Dataset: anthropic/hh-rlhf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "138ffdc22f11478fafdb317149835a32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: anthropic/hh-rlhf\n",
            "Function _preprocess_function_anthropic_hh_rlhf took 0.7766 seconds to execute\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/250 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a89e67b3862d4bbeb161c684830138f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: anthropic/hh-rlhf\n",
            "Function _preprocess_function_anthropic_hh_rlhf took 0.2065 seconds to execute\n",
            "\n",
            "\n",
            "Orient Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 250\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "Orient finished.\n",
            "Function _orient took 116.0962 seconds to execute\n",
            "\n",
            "\n",
            "Starting Decide ...\n",
            "trainable params: 671,088,640 || all params: 7,919,112,192 || trainable%: 8.4743\n",
            "\n",
            "\n",
            "Decide finished.\n",
            "Function _decide took 7.5309 seconds to execute\n",
            "\n",
            "\n",
            "Starting Act ...\n",
            "Dataset preprocessed successfully.\n",
            "\n",
            "\n",
            "Unsloth data collator used.\n",
            "Initializing Trainer...\n",
            "\n",
            "\n",
            "Act finished.\n",
            "Function _act took 0.4666 seconds to execute\n",
            "\n",
            "\n",
            "Run Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 1000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 250\n",
            "    })\n",
            "})\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/60 01:50 < 00:57, 0.35 it/s, Epoch 0.62/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.529900</td>\n",
              "      <td>1.553966</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "import time\n",
        "from transformers import TrainingArguments, TrainerState, TrainerControl\n",
        "\n",
        "\n",
        "def generate_report(\n",
        "    rl_pairs, agents, training_args_list, state_list, control_list, output_file=\"experiment_report.txt\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a report for multiple RL experiments, including evaluation scores and training details.\n",
        "\n",
        "    Args:\n",
        "        rl_pairs (list): A list of tuples, each containing (dataset_name, model_id, config).\n",
        "        agents (list): A list of FineTuningAgent objects corresponding to the experiments.\n",
        "        training_args_list (list): A list of TrainingArguments objects for each experiment.\n",
        "        state_list (list): A list of TrainerState objects for each experiment.\n",
        "        control_list (list): A list of TrainerControl objects for each experiment.\n",
        "        output_file (str): The name of the output file to save the report.\n",
        "    \"\"\"\n",
        "    if not (\n",
        "        len(rl_pairs)\n",
        "        == len(agents)\n",
        "        == len(training_args_list)\n",
        "        == len(state_list)\n",
        "        == len(control_list)\n",
        "    ):\n",
        "        raise ValueError(\"The number of rl_pairs, agents, training_args, state, and control must be the same.\")\n",
        "\n",
        "    report_data = []\n",
        "    for (dataset_name, model_id, config), agent, training_args, state, control in zip(\n",
        "        rl_pairs, agents, training_args_list, state_list, control_list\n",
        "    ):\n",
        "        # Collect the data\n",
        "        if agent.start_time is None or agent.end_time is None:\n",
        "            raise ValueError(\"Start time or end time is not defined.\")\n",
        "        elapsed_time = agent.end_time - agent.start_time\n",
        "        train_losses = agent.train_losses\n",
        "        eval_losses = agent.eval_losses\n",
        "\n",
        "        if not train_losses:\n",
        "            train_std = np.nan  # Use np.nan for no data\n",
        "            min_train_loss = np.nan\n",
        "            max_train_loss = np.nan\n",
        "        else:\n",
        "            train_std = np.std(train_losses)\n",
        "            min_train_loss = np.min(train_losses)\n",
        "            max_train_loss = np.max(train_losses)\n",
        "\n",
        "        if not eval_losses:\n",
        "            eval_std = np.nan\n",
        "            min_eval_loss = np.nan\n",
        "            max_eval_loss = np.nan\n",
        "        else:\n",
        "            eval_std = np.std(eval_losses)\n",
        "            min_eval_loss = np.min(eval_losses)\n",
        "            max_eval_loss = np.max(eval_losses)\n",
        "\n",
        "        # Collect the metrics.\n",
        "        if agent.evaluation_results is not None:\n",
        "            bleu_score = agent.evaluation_results.get(\"eval_bleu\", np.nan)\n",
        "            f1_score = agent.evaluation_results.get(\"eval_f1\", np.nan)\n",
        "        else:\n",
        "            bleu_score = np.nan\n",
        "            f1_score = np.nan\n",
        "\n",
        "        report_data.append(\n",
        "            [\n",
        "                dataset_name,\n",
        "                model_id,\n",
        "                f\"{elapsed_time:.2f} seconds\",  # Format to 2 decimal places\n",
        "                f\"{train_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{eval_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{bleu_score:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{f1_score:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{training_args.learning_rate:.4f}\",  # Learning rate\n",
        "                training_args.per_device_train_batch_size,  # Batch size\n",
        "                training_args.num_train_epochs if hasattr(training_args,\"num_train_epochs\") else \"n/a\", # Epochs\n",
        "                state.global_step,  # Global steps\n",
        "                state.epoch,  # Epoch\n",
        "                state.is_local_process_zero,\n",
        "                control.should_training_stop,\n",
        "                control.should_log,\n",
        "                control.should_save,\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    headers = [\n",
        "        \"Dataset\",\n",
        "        \"Model\",\n",
        "        \"Elapsed Time\",\n",
        "        \"Train Loss Std\",\n",
        "        \"Eval Loss Std\",\n",
        "        \"Min Train Loss\",\n",
        "        \"Max Train Loss\",\n",
        "        \"Min Eval Loss\",\n",
        "        \"Max Eval Loss\",\n",
        "        \"BLEU Score\",\n",
        "        \"F1 Score\",\n",
        "        \"Learning Rate\",\n",
        "        \"Batch Size\",\n",
        "        \"Epochs\",\n",
        "        \"Global Steps\",\n",
        "        \"Epoch\",\n",
        "        \"Is Local Process Zero\",\n",
        "        \"Should Training Stop\",\n",
        "        \"Should Log\",\n",
        "        \"Should Save\",\n",
        "    ]\n",
        "\n",
        "    # Format the report as a table\n",
        "    report_table = tabulate(report_data, headers=headers, tablefmt=\"grid\")\n",
        "\n",
        "    # Print the report to the console\n",
        "    print(report_table)\n",
        "\n",
        "    # Save the report to a file\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(report_table)\n",
        "        print(f\"Report saved to {output_file}\")"
      ],
      "metadata": {
        "id": "KFmsCZrABMQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## llm report"
      ],
      "metadata": {
        "id": "BRmauENT7tWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Used to securely store your API key\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI')  # Replace 'GEMINI' with your actual userdata variable name\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "from tabulate import tabulate\n",
        "from transformers import TrainingArguments, TrainerState, TrainerControl\n",
        "\n",
        "def generate_llm_report(\n",
        "    rl_pairs,\n",
        "    agents,\n",
        "    training_args_list,\n",
        "    state_list,\n",
        "    control_list,\n",
        "    output_file=\"experiment_report.txt\",\n",
        "    experiment_name=None,\n",
        "    prompt=\"You are a helpful data science expert.\\nPlease, make an additional analysis of this Fine-Tuning experiment report.\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a report for multiple LLM experiments, including evaluation scores and training details,\n",
        "    and provides an analysis using Google Gemini.\n",
        "\n",
        "    Args:\n",
        "        rl_pairs (list): A list of tuples, each containing (dataset_name, model_id, config).\n",
        "        agents (list): A list of FineTuningAgent objects corresponding to the experiments.\n",
        "        training_args_list (list): A list of TrainingArguments objects for each experiment.\n",
        "        state_list (list): A list of TrainerState objects for each experiment.\n",
        "        control_list (list): A list of TrainerControl objects for each experiment.\n",
        "        output_file (str): The name of the output file to save the report.\n",
        "        experiment_name (str, optional): The base name for the experiment results file.\n",
        "                                        If provided, it will be used to load the results. Defaults to None.\n",
        "        prompt (str, optional): The prompt to provide to Google Gemini for analysis.\n",
        "                                Defaults to a generic data science expert prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    if not (\n",
        "        len(rl_pairs)\n",
        "        == len(agents)\n",
        "        == len(training_args_list)\n",
        "        == len(state_list)\n",
        "        == len(control_list)\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"The number of rl_pairs, agents, training_args, state, and control must be the same.\"\n",
        "        )\n",
        "\n",
        "    report_data = []  # Initialize report_data here\n",
        "\n",
        "    for (\n",
        "        (dataset_name, model_id, config),\n",
        "        agent,\n",
        "        training_args,\n",
        "        state,\n",
        "        control,\n",
        "    ) in zip(rl_pairs, agents, training_args_list, state_list, control_list):\n",
        "        # Get eval_results from the agent\n",
        "\n",
        "        experiment_name = f\"\"\"{model_id.replace('/', '-').replace(\"'\", '')}_{dataset_name.replace('/', '-').replace(\"'\", '')}\"\"\"\n",
        "        #print(f\"Experiment Name: {experiment_name}\")\n",
        "\n",
        "        results_file = f\"{experiment_name}_results.txt\"\n",
        "        print(f\"Results File: {results_file}\")\n",
        "\n",
        "\n",
        "        # \"eval_loss\": 6.17133903503418, \"eval_bleu\": 0, \"eval_f1\": 0.0, \"eval_runtime\": 4.0188, \"eval_samples_per_second\": 6.221, \"eval_steps_per_second\": 0.995, \"epoch\": 8.64}\n",
        "\n",
        "        try:\n",
        "            with open(results_file, \"r\") as f:\n",
        "                eval_results = json.load(f)\n",
        "            bleu_score = eval_results.get(\"eval_bleu\")\n",
        "            f1_score = eval_results.get(\"eval_f1\")\n",
        "            print(f\"BLEU Score: {bleu_score}, F1 Score: {f1_score}\")\n",
        "            print(f\"Eval Results: {eval_results}\")\n",
        "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "            print(f\"Error loading results: {e}\")\n",
        "            bleu_score = None\n",
        "            f1_score = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Collect the data\n",
        "        elapsed_time = (\n",
        "            agent.end_time - agent.start_time\n",
        "            if agent.start_time and agent.end_time\n",
        "            else np.nan\n",
        "        )  # Handle potential errors\n",
        "        train_losses = agent.train_losses\n",
        "        eval_losses = agent.eval_losses\n",
        "\n",
        "        if not train_losses:\n",
        "            train_std = np.nan  # Use np.nan for no data\n",
        "            min_train_loss = np.nan\n",
        "            max_train_loss = np.nan\n",
        "        else:\n",
        "            train_std = np.std(train_losses)\n",
        "            min_train_loss = np.min(train_losses)\n",
        "            max_train_loss = np.max(train_losses)\n",
        "\n",
        "        if not eval_losses:\n",
        "            eval_std = np.nan\n",
        "            min_eval_loss = np.nan\n",
        "            max_eval_loss = np.nan\n",
        "        else:\n",
        "            eval_std = np.std(eval_losses)\n",
        "            min_eval_loss = np.min(eval_losses)\n",
        "            max_eval_loss = np.max(eval_losses)\n",
        "\n",
        "        # Check if training_args is None before accessing its attributes\n",
        "        learning_rate = training_args.learning_rate if training_args is not None else np.nan\n",
        "        batch_size = training_args.per_device_train_batch_size if training_args is not None else np.nan\n",
        "        epochs = training_args.num_train_epochs if training_args is not None and hasattr(training_args, \"num_train_epochs\") else \"n/a\"\n",
        "\n",
        "        report_data.append(\n",
        "            [\n",
        "                dataset_name,\n",
        "                model_id,\n",
        "                f\"{elapsed_time:.2f} seconds\",  # Format to 2 decimal places\n",
        "                f\"{train_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{eval_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "\n",
        "\n",
        "                f\"{bleu_score:.4f}\" if bleu_score is not None else \"N/A\",  # Handle None case for bleu_score\n",
        "                f\"{f1_score:.4f}\" if f1_score is not None else \"N/A\",  # Handle None case for f1_score\n",
        "\n",
        "\n",
        "                f\"{learning_rate:.4f}\",  # Learning rate\n",
        "                batch_size,  # Batch size\n",
        "                epochs,  # Epochs\n",
        "                state.global_step if state else \"n/a\",  # Global steps\n",
        "                state.epoch if state else \"n/a\",  # Epoch\n",
        "                state.is_local_process_zero if state else \"n/a\",\n",
        "                control.should_training_stop if control else \"n/a\",\n",
        "                control.should_log if control else \"n/a\",\n",
        "                control.should_save if control else \"n/a\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # Generate the report table\n",
        "    headers = [\n",
        "        \"Dataset\",\n",
        "        \"Model\",\n",
        "        \"Elapsed Time\",\n",
        "        \"Train Loss Std\",\n",
        "        \"Eval Loss Std\",\n",
        "        \"Min Train Loss\",\n",
        "        \"Max Train Loss\",\n",
        "        \"Min Eval Loss\",\n",
        "        \"Max Eval Loss\",\n",
        "        \"BLEU Score\",\n",
        "        \"F1 Score\",\n",
        "        \"Learning Rate\",\n",
        "        \"Batch Size\",\n",
        "        \"Epochs\",\n",
        "        \"Global Steps\",\n",
        "        \"Epoch\",\n",
        "        \"is_local_process_zero\",\n",
        "        \"should_training_stop\",\n",
        "        \"should_log\",\n",
        "        \"should_save\",\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    report_table = tabulate(report_data, headers=headers, tablefmt=\"grid\")\n",
        "\n",
        "    # Save the report to a file\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(report_table)\n",
        "\n",
        "    print(report_table)\n",
        "\n",
        "    # LLM Analysis using Google Gemini\n",
        "    model_name = \"gemini-1.5-pro\"  # Replace with desired model\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "    response = model.generate_content(prompt + \"\\n\\n\" + report_table)\n",
        "    llm_analysis = response.text\n",
        "\n",
        "    print(\"\\n\\n## LLM Analysis:\\n\")\n",
        "    print(llm_analysis)\n",
        "\n",
        "    return llm_analysis\n",
        ""
      ],
      "metadata": {
        "id": "QPJ0q0OvrIzA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have rl_pairs and agents defined and populated\n",
        "\n",
        "# Define the prompt for the LLM\n",
        "prompt = \"\"\"\n",
        "You are a helpful data science expert.\n",
        "Please, make an additional analysis of this Fine-Tuning experiment report.\n",
        "\"\"\"\n",
        "\n",
        "# Generate training_args_list, state_list, and control_list\n",
        "# These lists should be the same length as rl_pairs and agents, and filled with appropriate data\n",
        "training_args_list = [agent.training_args for agent in agents]\n",
        "state_list = [agent.trainer.state for agent in agents]\n",
        "control_list = [agent.trainer.control for agent in agents]\n",
        "\n",
        "\n",
        "# Call the function, optionally providing an output file name\n",
        "# Instead of passing \"prompt\", make sure to pass the training args, state, and control lists.\n",
        "report_text = generate_llm_report(rl_pairs, agents, training_args_list, state_list, control_list, output_file=\"my_experiment_report.txt\")\n",
        "\n",
        "# You can then further process or print the report_text if needed\n",
        "print(report_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xebrWwS-kcWw",
        "outputId": "7011e42a-0770-4876-98fa-dc56b9b08be9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agents' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-9e589deefbb6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Generate training_args_list, state_list, and control_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# These lists should be the same length as rl_pairs and agents, and filled with appropriate data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtraining_args_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_args\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mstate_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mcontrol_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'agents' is not defined"
          ]
        }
      ]
    }
  ]
}