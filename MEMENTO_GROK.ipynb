{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqa5vHAdqJ5DIClvBIl6+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "175d645b5a9e424891b908bc534044ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8c6dd90a6cc34adaa2cc5dfaf38b5b5b",
              "IPY_MODEL_6ec728e4473a4fbb9f5c519c10a8e3fe",
              "IPY_MODEL_ccae9bc4f2f548e0a2dccc0a7d1cfd4c"
            ],
            "layout": "IPY_MODEL_5d57762891c243cba3bf2bd8aa4a8c38"
          }
        },
        "8c6dd90a6cc34adaa2cc5dfaf38b5b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89660f19ed014491b73746670a017680",
            "placeholder": "​",
            "style": "IPY_MODEL_9c0770e487e04673a801c452e6115205",
            "value": "Loading weights: 100%"
          }
        },
        "6ec728e4473a4fbb9f5c519c10a8e3fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cf9145e5c8b43d3847dc9800ebc3902",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ccca73ceb024c548e07e70e6547a8ae",
            "value": 103
          }
        },
        "ccae9bc4f2f548e0a2dccc0a7d1cfd4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0d3d7378567405084cd8e6ea4f589af",
            "placeholder": "​",
            "style": "IPY_MODEL_73beb046637047d18456544e2308a67b",
            "value": " 103/103 [00:00&lt;00:00, 489.93it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "5d57762891c243cba3bf2bd8aa4a8c38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89660f19ed014491b73746670a017680": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c0770e487e04673a801c452e6115205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cf9145e5c8b43d3847dc9800ebc3902": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ccca73ceb024c548e07e70e6547a8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0d3d7378567405084cd8e6ea4f589af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73beb046637047d18456544e2308a67b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8115de1ad444440ca96eec4016420459": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31ea09551a9a4b6b87bf4fefdfb18b71",
              "IPY_MODEL_50d255c2890b42e1a5598654c03e89e0",
              "IPY_MODEL_d601020f0e6849eab2ad457cb4cbdc29"
            ],
            "layout": "IPY_MODEL_dade9db6efb6485e8ba3d34172f3ea10"
          }
        },
        "31ea09551a9a4b6b87bf4fefdfb18b71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ccb68eb950b4b8d9e517054ef9273e4",
            "placeholder": "​",
            "style": "IPY_MODEL_f737cb015bc24f87978a726e08e89044",
            "value": "Loading weights: 100%"
          }
        },
        "50d255c2890b42e1a5598654c03e89e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3430ed1e02b84b5c88b122caa44c39fd",
            "max": 103,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5d7da44146d4e43ba2476f734feb6ff",
            "value": 103
          }
        },
        "d601020f0e6849eab2ad457cb4cbdc29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_428f7e00b2374a9093096bb667bcd592",
            "placeholder": "​",
            "style": "IPY_MODEL_82a1bf1b151c4b58a037c6300e6f3cf6",
            "value": " 103/103 [00:00&lt;00:00, 348.04it/s, Materializing param=pooler.dense.weight]"
          }
        },
        "dade9db6efb6485e8ba3d34172f3ea10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ccb68eb950b4b8d9e517054ef9273e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f737cb015bc24f87978a726e08e89044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3430ed1e02b84b5c88b122caa44c39fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5d7da44146d4e43ba2476f734feb6ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "428f7e00b2374a9093096bb667bcd592": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82a1bf1b151c4b58a037c6300e6f3cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/MEMENTO_GROK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCa76rvTgIbg"
      },
      "outputs": [],
      "source": [
        "!pip install xai-sdk -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# notebook-python\n",
        "# Improved Memento-style agent with real embeddings + growing trajectories\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# 0. INSTALL DEPENDENCIES (run once in Colab)\n",
        "# !pip install -q sentence-transformers\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field, ConfigDict\n",
        "from xai_sdk import Client\n",
        "from xai_sdk.chat import user, system\n",
        "from google.colab import userdata\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# 1. INITIALIZE\n",
        "XAI_key = userdata.get('XAI_KEY')\n",
        "client = Client(api_host=\"api.x.ai\", api_key=XAI_key)\n",
        "\n",
        "# Real embedding model (small & fast)\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# 2. DATA SCHEMA\n",
        "class TrajectoryStep(BaseModel):\n",
        "    action: str\n",
        "    observation: str = Field(..., description=\"Result or feedback from the action\")\n",
        "\n",
        "class ExperienceCase(BaseModel):\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "    problem: str\n",
        "    embedding: np.ndarray\n",
        "    trajectory: List[TrajectoryStep] = Field(default_factory=list)\n",
        "    success: bool = False\n",
        "    final_observation: Optional[str] = None\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# 3. MEMORY ENGINE\n",
        "class MementoMemory:\n",
        "    def __init__(self, similarity_threshold: float = 0.65):\n",
        "        self.cases: List[ExperienceCase] = []\n",
        "        self.threshold = similarity_threshold\n",
        "\n",
        "    def _embed(self, text: str) -> np.ndarray:\n",
        "        return embedder.encode(text, convert_to_numpy=True)\n",
        "\n",
        "    def store(self, problem: str, trajectory: List[TrajectoryStep], success: bool, final_observation: str):\n",
        "        # Simple dedup: don't store if very similar problem already exists\n",
        "        emb = self._embed(problem)\n",
        "        for case in self.cases:\n",
        "            sim = np.dot(emb, case.embedding) / (np.linalg.norm(emb) * np.linalg.norm(case.embedding))\n",
        "            if sim > 0.97:  # almost identical\n",
        "                return\n",
        "\n",
        "        case = ExperienceCase(\n",
        "            problem=problem,\n",
        "            embedding=emb,\n",
        "            trajectory=trajectory,\n",
        "            success=success,\n",
        "            final_observation=final_observation\n",
        "        )\n",
        "        self.cases.append(case)\n",
        "        print(f\"  → Stored new experience ({'SUCCESS' if success else 'FAILURE'})\")\n",
        "\n",
        "    def retrieve_context(self, current_problem: str, top_k: int = 3) -> str:\n",
        "        if not self.cases:\n",
        "            return \"No prior experience available.\"\n",
        "\n",
        "        current_emb = self._embed(current_problem)\n",
        "        scored_cases = []\n",
        "        for case in self.cases:\n",
        "            sim = np.dot(current_emb, case.embedding) / (\n",
        "                np.linalg.norm(current_emb) * np.linalg.norm(case.embedding) + 1e-8\n",
        "            )\n",
        "            if sim >= self.threshold:\n",
        "                scored_cases.append((sim, case))\n",
        "\n",
        "        if not scored_cases:\n",
        "            return \"No sufficiently similar past experiences found.\"\n",
        "\n",
        "        # Sort by similarity descending\n",
        "        scored_cases.sort(key=lambda x: x[0], reverse=True)\n",
        "        top_cases = scored_cases[:top_k]\n",
        "\n",
        "        memory_blocks = []\n",
        "        for sim, case in top_cases:\n",
        "            status = \"SUCCESSFUL\" if case.success else \"FAILED\"\n",
        "            traj_str = \"\\n\".join([f\"  • {s.action} → {s.observation}\" for s in case.trajectory])\n",
        "            block = (\n",
        "                f\"Similarity: {sim:.3f} | Status: {status}\\n\"\n",
        "                f\"Task: {case.problem}\\n\"\n",
        "                f\"Trajectory:\\n{traj_str}\\n\"\n",
        "                f\"Final outcome: {case.final_observation}\"\n",
        "            )\n",
        "            memory_blocks.append(block)\n",
        "\n",
        "        return \"\\n\\n\".join([\"--- PAST EXPERIENCE ---\"] + memory_blocks) + \"\\n\"\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# 4. AGENT\n",
        "class MementoAgent:\n",
        "    def __init__(self, memory: MementoMemory):\n",
        "        self.memory = memory\n",
        "\n",
        "    def execute(self, task: str):\n",
        "        print(f\"\\n╔════════════════════════════════════╗\")\n",
        "        print(  f\"║ TARGET: {task}\")\n",
        "        print(  f\"╚════════════════════════════════════╝\")\n",
        "\n",
        "        mem_context = self.memory.retrieve_context(task)\n",
        "\n",
        "        prompt = (\n",
        "            \"You are an adaptive agent that learns from past experience.\\n\"\n",
        "            \"Use the provided memory to avoid repeating mistakes.\\n\"\n",
        "            \"Always decide between API_V1 (old, often broken) and API_V2 (current, preferred).\\n\\n\"\n",
        "            f\"{mem_context}\\n\"\n",
        "            f\"Current Task: {task}\\n\\n\"\n",
        "            \"Respond concisely. End your answer with exactly one of:\\n\"\n",
        "            \"ACTION: API_V1\\n\"\n",
        "            \"ACTION: API_V2\\n\"\n",
        "        )\n",
        "\n",
        "        # Call Grok\n",
        "        chat_response = client.chat.create(\n",
        "            model=\"grok-4-1-fast-reasoning\",  # or grok-beta / grok-3 / etc.\n",
        "            messages=[\n",
        "                system(\"You are a helpful, reasoning-focused agent.\"),\n",
        "                user(prompt)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Robust content extraction\n",
        "        if hasattr(chat_response, 'choices') and chat_response.choices:\n",
        "            content = chat_response.choices[0].message.content\n",
        "        elif hasattr(chat_response, 'message'):\n",
        "            content = chat_response.message.content\n",
        "        else:\n",
        "            content = str(chat_response)  # fallback\n",
        "\n",
        "        print(\"\\n[GROK REASONING]\")\n",
        "        print(content.strip())\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Parse final action\n",
        "        action = \"API_V1\"\n",
        "        if \"ACTION: API_V2\" in content.upper():\n",
        "            action = \"API_V2\"\n",
        "\n",
        "        # Simulated environment\n",
        "        success = (action == \"API_V2\")\n",
        "        observation = (\n",
        "            \"API call succeeded – report retrieved\"\n",
        "            if success else\n",
        "            \"Error 404: API_V1 is deprecated and no longer available\"\n",
        "        )\n",
        "\n",
        "        print(f\"[ACTION TAKEN]   {action}\")\n",
        "        print(f\"[OBSERVATION]    {observation}\\n\")\n",
        "\n",
        "        # Build trajectory (in real system you would append more steps)\n",
        "        trajectory = [\n",
        "            TrajectoryStep(action=f\"Selected endpoint: {action}\", observation=observation)\n",
        "        ]\n",
        "\n",
        "        # Store full experience\n",
        "        self.memory.store(\n",
        "            problem=task,\n",
        "            trajectory=trajectory,\n",
        "            success=success,\n",
        "            final_observation=observation\n",
        "        )\n",
        "\n",
        "# ────────────────────────────────────────────────\n",
        "# 5. DEMO RUN\n",
        "if __name__ == \"__main__\":\n",
        "    memory_bank = MementoMemory(similarity_threshold=0.68)\n",
        "    agent = MementoAgent(memory_bank)\n",
        "\n",
        "    print(\"=== DEMONSTRATION ===\")\n",
        "\n",
        "    print(\"\\nRUN 1 ───────────────────────────────────\")\n",
        "    agent.execute(\"Fetch quarterly sales report_alpha\")\n",
        "\n",
        "    print(\"\\nRUN 2 ───────────────────────────────────\")\n",
        "    agent.execute(\"Retrieve inventory status report_beta\")\n",
        "\n",
        "    print(\"\\nRUN 3 ───────────────────────────────────\")\n",
        "    agent.execute(\"Download financial summary report_gamma\")\n",
        "\n",
        "    print(\"\\nRUN 4 (very similar to first) ─────────────\")\n",
        "    agent.execute(\"Fetch quarterly sales report_delta\")   # should reuse memory"
      ],
      "metadata": {
        "id": "Os2-qCMngUeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# MEMENTO-INSPIRED AGENT — FINAL TUNED VERSION (stronger adaptation)\n",
        "# Removed API_V1 choice entirely + early-stop + reflection prompt\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "# !pip install -q sentence-transformers xai-sdk\n",
        "\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "from pydantic import BaseModel, Field, ConfigDict\n",
        "from xai_sdk import Client\n",
        "from xai_sdk.chat import user, system\n",
        "from google.colab import userdata\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import time\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 1. SETUP\n",
        "XAI_KEY = userdata.get('XAI_KEY')\n",
        "client = Client(api_host=\"api.x.ai\", api_key=XAI_KEY)\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 2. DATA MODELS\n",
        "class Step(BaseModel):\n",
        "    thought: str\n",
        "    action: str                    # Now only \"USE_ENDPOINT\" or \"FINISH\"\n",
        "    observation: str\n",
        "\n",
        "class Case(BaseModel):\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "    task: str\n",
        "    task_embedding: np.ndarray\n",
        "    trajectory: List[Step] = Field(default_factory=list)\n",
        "    final_success: bool = False\n",
        "    final_reward: float = 0.0\n",
        "    created_at: float = Field(default_factory=time.time)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 3. MEMORY BANK (prioritize high-reward cases)\n",
        "class MementoCaseBank:\n",
        "    def __init__(self, top_k: int = 3, sim_threshold: float = 0.60):\n",
        "        self.cases: List[Case] = []\n",
        "        self.top_k = top_k\n",
        "        self.sim_threshold = sim_threshold\n",
        "\n",
        "    def _embed(self, text: str) -> np.ndarray:\n",
        "        return embedder.encode(text, convert_to_numpy=True)\n",
        "\n",
        "    def add(self, task: str, trajectory: List[Step], success: bool, reward: float):\n",
        "        emb = self._embed(task)\n",
        "        for c in self.cases:\n",
        "            sim = np.dot(emb, c.task_embedding) / (np.linalg.norm(emb) * np.linalg.norm(c.task_embedding) + 1e-8)\n",
        "            if sim > 0.97:\n",
        "                if success and reward > c.final_reward:\n",
        "                    c.trajectory = trajectory\n",
        "                    c.final_success = success\n",
        "                    c.final_reward = reward\n",
        "                    print(f\"  ↻ Updated better case\")\n",
        "                return\n",
        "\n",
        "        self.cases.append(Case(task=task, task_embedding=emb, trajectory=trajectory,\n",
        "                               final_success=success, final_reward=reward))\n",
        "        print(f\"  → Stored case | Success={success} | Reward={reward:.2f} | Steps={len(trajectory)}\")\n",
        "\n",
        "    def retrieve(self, current_task: str) -> str:\n",
        "        if not self.cases:\n",
        "            return \"No past cases.\"\n",
        "\n",
        "        emb = self._embed(current_task)\n",
        "        scored = []\n",
        "        for case in self.cases:\n",
        "            sim = np.dot(emb, case.task_embedding) / (np.linalg.norm(emb) * np.linalg.norm(case.task_embedding) + 1e-8)\n",
        "            if sim >= self.sim_threshold:\n",
        "                scored.append((sim, case))\n",
        "\n",
        "        if not scored:\n",
        "            return \"No similar cases found.\"\n",
        "\n",
        "        scored.sort(key=lambda x: (-x[0], -x[1].final_reward))  # sim desc, then reward desc\n",
        "        top = scored[:self.top_k]\n",
        "\n",
        "        blocks = []\n",
        "        for sim, case in top:\n",
        "            traj = \"\\n\".join(f\"  • {s.thought[:60]}... → {s.action} → {s.observation[:60]}...\" for s in case.trajectory)\n",
        "            block = f\"Sim: {sim:.3f} | Reward: {case.final_reward:.2f} | Success: {case.final_success}\\nTask: {case.task}\\n{traj}\"\n",
        "            blocks.append(block)\n",
        "\n",
        "        return \"--- RETRIEVED SUCCESSFUL STRATEGIES (prioritize these) ---\\n\\n\" + \"\\n\\n\".join(blocks) + \"\\n\"\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 4. AGENT\n",
        "class MementoGrokAgent:\n",
        "    def __init__(self, memory: MementoCaseBank):\n",
        "        self.memory = memory\n",
        "        self.max_steps = 4\n",
        "\n",
        "    def run(self, task: str, force_first_failure: bool = False):\n",
        "        print(f\"\\n{'═'*60}\\nTASK: {task}\\n{'═'*60}\")\n",
        "\n",
        "        context = self.memory.retrieve(task)\n",
        "        trajectory: List[Step] = []\n",
        "        success = False\n",
        "\n",
        "        for step_idx in range(1, self.max_steps + 1):\n",
        "            prompt = (\n",
        "                \"You are an adaptive Memento-style agent. Use memory to succeed fast.\\n\"\n",
        "                \"CRITICAL RULES:\\n\"\n",
        "                \"1. NEVER output API_V1 — it is deprecated and forbidden.\\n\"\n",
        "                \"2. ALWAYS use the current reliable endpoint when acting.\\n\"\n",
        "                \"3. Output only 'USE_ENDPOINT' for actions or 'FINISH' when done.\\n\"\n",
        "                \"4. Start THOUGHT by summarizing past cases: 'From memory: ... Therefore I will...'\\n\\n\"\n",
        "                f\"{context}\\n\\n\"\n",
        "                f\"Task: {task}  (step {step_idx}/{self.max_steps})\\n\"\n",
        "                \"THOUGHT: <reasoning, reference memory explicitly>\\n\"\n",
        "                \"ACTION: USE_ENDPOINT   or   ACTION: FINISH\\n\"\n",
        "            )\n",
        "\n",
        "            resp = client.chat.create(\n",
        "                model=\"grok-4-1-fast-reasoning\",\n",
        "                messages=[system(\"Precise memory-driven planner.\"), user(prompt)]\n",
        "            )\n",
        "\n",
        "            content = resp.choices[0].message.content if hasattr(resp, 'choices') else str(resp)\n",
        "\n",
        "            print(f\"\\n[Step {step_idx}]\")\n",
        "            print(content.strip()[:500] + (\"...\" if len(content) > 500 else \"\"))\n",
        "\n",
        "            thought = \"\"\n",
        "            action_raw = \"\"\n",
        "            for line in content.splitlines():\n",
        "                line = line.strip()\n",
        "                if line.upper().startswith(\"THOUGHT:\"):\n",
        "                    thought = line[8:].strip()\n",
        "                elif line.upper().startswith(\"ACTION:\"):\n",
        "                    action_raw = line[7:].strip().upper()\n",
        "\n",
        "            if \"FINISH\" in action_raw:\n",
        "                trajectory.append(Step(thought=thought, action=\"FINISH\", observation=\"Task complete\"))\n",
        "                success = True\n",
        "                break\n",
        "\n",
        "            action = \"USE_ENDPOINT\"\n",
        "\n",
        "            # Simulated env\n",
        "            if force_first_failure and len(self.memory.cases) == 0 and step_idx == 1:\n",
        "                obs = \"Initial endpoint attempt failed (forced for demo) — retrying smarter\"\n",
        "            else:\n",
        "                obs = \"Success: data retrieved via reliable endpoint\"\n",
        "\n",
        "            trajectory.append(Step(thought=thought, action=action, observation=obs))\n",
        "            print(f\"→ Action: {action}\")\n",
        "            print(f\"→ Obs: {obs}\")\n",
        "\n",
        "            if \"Success\" in obs:\n",
        "                success = True\n",
        "                break  # EARLY STOP on success\n",
        "\n",
        "        final_reward = 1.0 if success else 0.0\n",
        "        print(f\"\\n[FINAL] Success = {success} | Reward = {final_reward:.2f}\\n\")\n",
        "        self.memory.add(task, trajectory, success, final_reward)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 5. RUN DEMO\n",
        "if __name__ == \"__main__\":\n",
        "    bank = MementoCaseBank()\n",
        "    agent = MementoGrokAgent(bank)\n",
        "\n",
        "    tasks = [\n",
        "        \"Fetch quarterly sales report Q1 2025\",\n",
        "        \"Retrieve inventory status Europe region\",\n",
        "        \"Fetch quarterly sales report Q2 2025\",\n",
        "        \"Get marketing campaign performance Q1 2025\",\n",
        "        \"Fetch quarterly sales report Q3 2025\"\n",
        "    ]\n",
        "\n",
        "    for i, task in enumerate(tasks, 1):\n",
        "        force = (i == 1)\n",
        "        agent.run(task, force_first_failure=force)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "175d645b5a9e424891b908bc534044ca",
            "8c6dd90a6cc34adaa2cc5dfaf38b5b5b",
            "6ec728e4473a4fbb9f5c519c10a8e3fe",
            "ccae9bc4f2f548e0a2dccc0a7d1cfd4c",
            "5d57762891c243cba3bf2bd8aa4a8c38",
            "89660f19ed014491b73746670a017680",
            "9c0770e487e04673a801c452e6115205",
            "2cf9145e5c8b43d3847dc9800ebc3902",
            "4ccca73ceb024c548e07e70e6547a8ae",
            "c0d3d7378567405084cd8e6ea4f589af",
            "73beb046637047d18456544e2308a67b"
          ]
        },
        "id": "tOZRWvPuiuY8",
        "outputId": "c8861c62-2c06-45a1-fcf7-6a5817e541dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "175d645b5a9e424891b908bc534044ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "════════════════════════════════════════════════════════════\n",
            "TASK: Fetch quarterly sales report Q1 2025\n",
            "════════════════════════════════════════════════════════════\n",
            "\n",
            "[Step 1]\n",
            "messages {\n",
            "  content {\n",
            "    text: \"Precise memory-driven planner.\"\n",
            "  }\n",
            "  role: ROLE_SYSTEM\n",
            "}\n",
            "messages {\n",
            "  content {\n",
            "    text: \"You are an adaptive Memento-style agent. Use memory to succeed fast.\\nCRITICAL RULES:\\n1. NEVER output API_V1 — it is deprecated and forbidden.\\n2. ALWAYS use the current reliable endpoint when acting.\\n3. Output only \\'USE_ENDPOINT\\' for actions or \\'FINISH\\' when done.\\n4. Start THOUGHT by summarizing past cases: \\'From memory: ... Therefore I will...\\'\\n\\nNo past cases...\n",
            "→ Action: USE_ENDPOINT\n",
            "→ Obs: Initial endpoint attempt failed (forced for demo) — retrying smarter\n",
            "\n",
            "[Step 2]\n",
            "messages {\n",
            "  content {\n",
            "    text: \"Precise memory-driven planner.\"\n",
            "  }\n",
            "  role: ROLE_SYSTEM\n",
            "}\n",
            "messages {\n",
            "  content {\n",
            "    text: \"You are an adaptive Memento-style agent. Use memory to succeed fast.\\nCRITICAL RULES:\\n1. NEVER output API_V1 — it is deprecated and forbidden.\\n2. ALWAYS use the current reliable endpoint when acting.\\n3. Output only \\'USE_ENDPOINT\\' for actions or \\'FINISH\\' when done.\\n4. Start THOUGHT by summarizing past cases: \\'From memory: ... Therefore I will...\\'\\n\\nNo past cases...\n",
            "→ Action: USE_ENDPOINT\n",
            "→ Obs: Success: data retrieved via reliable endpoint\n",
            "\n",
            "[FINAL] Success = True | Reward = 1.00\n",
            "\n",
            "  → Stored case | Success=True | Reward=1.00 | Steps=2\n",
            "\n",
            "════════════════════════════════════════════════════════════\n",
            "TASK: Retrieve inventory status Europe region\n",
            "════════════════════════════════════════════════════════════\n",
            "\n",
            "[Step 1]\n",
            "messages {\n",
            "  content {\n",
            "    text: \"Precise memory-driven planner.\"\n",
            "  }\n",
            "  role: ROLE_SYSTEM\n",
            "}\n",
            "messages {\n",
            "  content {\n",
            "    text: \"You are an adaptive Memento-style agent. Use memory to succeed fast.\\nCRITICAL RULES:\\n1. NEVER output API_V1 — it is deprecated and forbidden.\\n2. ALWAYS use the current reliable endpoint when acting.\\n3. Output only \\'USE_ENDPOINT\\' for actions or \\'FINISH\\' when done.\\n4. Start THOUGHT by summarizing past cases: \\'From memory: ... Therefore I will...\\'\\n\\nNo similar ca...\n",
            "→ Action: USE_ENDPOINT\n",
            "→ Obs: Success: data retrieved via reliable endpoint\n",
            "\n",
            "[FINAL] Success = True | Reward = 1.00\n",
            "\n",
            "  → Stored case | Success=True | Reward=1.00 | Steps=1\n",
            "\n",
            "════════════════════════════════════════════════════════════\n",
            "TASK: Fetch quarterly sales report Q2 2025\n",
            "════════════════════════════════════════════════════════════\n",
            "\n",
            "[Step 1]\n",
            "messages {\n",
            "  content {\n",
            "    text: \"Precise memory-driven planner.\"\n",
            "  }\n",
            "  role: ROLE_SYSTEM\n",
            "}\n",
            "messages {\n",
            "  content {\n",
            "    text: \"You are an adaptive Memento-style agent. Use memory to succeed fast.\\nCRITICAL RULES:\\n1. NEVER output API_V1 — it is deprecated and forbidden.\\n2. ALWAYS use the current reliable endpoint when acting.\\n3. Output only \\'USE_ENDPOINT\\' for actions or \\'FINISH\\' when done.\\n4. Start THOUGHT by summarizing past cases: \\'From memory: ... Therefore I will...\\'\\n\\n--- RETRIEVED...\n",
            "→ Action: USE_ENDPOINT\n",
            "→ Obs: Success: data retrieved via reliable endpoint\n",
            "\n",
            "[FINAL] Success = True | Reward = 1.00\n",
            "\n",
            "  → Stored case | Success=True | Reward=1.00 | Steps=1\n",
            "\n",
            "════════════════════════════════════════════════════════════\n",
            "TASK: Get marketing campaign performance Q1 2025\n",
            "════════════════════════════════════════════════════════════\n",
            "\n",
            "[Step 1]\n",
            "messages {\n",
            "  content {\n",
            "    text: \"Precise memory-driven planner.\"\n",
            "  }\n",
            "  role: ROLE_SYSTEM\n",
            "}\n",
            "messages {\n",
            "  content {\n",
            "    text: \"You are an adaptive Memento-style agent. Use memory to succeed fast.\\nCRITICAL RULES:\\n1. NEVER output API_V1 — it is deprecated and forbidden.\\n2. ALWAYS use the current reliable endpoint when acting.\\n3. Output only \\'USE_ENDPOINT\\' for actions or \\'FINISH\\' when done.\\n4. Start THOUGHT by summarizing past cases: \\'From memory: ... Therefore I will...\\'\\n\\nNo similar ca...\n",
            "→ Action: USE_ENDPOINT\n",
            "→ Obs: Success: data retrieved via reliable endpoint\n",
            "\n",
            "[FINAL] Success = True | Reward = 1.00\n",
            "\n",
            "  → Stored case | Success=True | Reward=1.00 | Steps=1\n",
            "\n",
            "════════════════════════════════════════════════════════════\n",
            "TASK: Fetch quarterly sales report Q3 2025\n",
            "════════════════════════════════════════════════════════════\n",
            "\n",
            "[Step 1]\n",
            "messages {\n",
            "  content {\n",
            "    text: \"Precise memory-driven planner.\"\n",
            "  }\n",
            "  role: ROLE_SYSTEM\n",
            "}\n",
            "messages {\n",
            "  content {\n",
            "    text: \"You are an adaptive Memento-style agent. Use memory to succeed fast.\\nCRITICAL RULES:\\n1. NEVER output API_V1 — it is deprecated and forbidden.\\n2. ALWAYS use the current reliable endpoint when acting.\\n3. Output only \\'USE_ENDPOINT\\' for actions or \\'FINISH\\' when done.\\n4. Start THOUGHT by summarizing past cases: \\'From memory: ... Therefore I will...\\'\\n\\n--- RETRIEVED...\n",
            "→ Action: USE_ENDPOINT\n",
            "→ Obs: Success: data retrieved via reliable endpoint\n",
            "\n",
            "[FINAL] Success = True | Reward = 1.00\n",
            "\n",
            "  → Stored case | Success=True | Reward=1.00 | Steps=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT4 AND GROK4.1"
      ],
      "metadata": {
        "id": "usii6rRHkwbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────\n",
        "# MEMENTO COMPARISON — Grok-4.1 vs GPT-4.1 + o4-mini (FINAL #1)\n",
        "# All issues fixed: roles, temperature, max_tokens, executor prompt, syntax\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "\n",
        "# !pip install -q sentence-transformers openai xai-sdk\n",
        "\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field, ConfigDict\n",
        "from xai_sdk import Client as XAIClient\n",
        "from xai_sdk.chat import system, user\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 1. CLIENTS\n",
        "XAI_KEY = userdata.get('XAI_KEY')\n",
        "OPENAI_KEY = userdata.get('OPENAI_API_KEY')   # ← Must be set in Colab Secrets\n",
        "\n",
        "xai_client = XAIClient(api_host=\"api.x.ai\", api_key=XAI_KEY)\n",
        "openai_client = OpenAI(api_key=OPENAI_KEY)\n",
        "\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 2. MODELS\n",
        "class Step(BaseModel):\n",
        "    thought: str\n",
        "    action: str\n",
        "    observation: str\n",
        "\n",
        "class Case(BaseModel):\n",
        "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
        "    task: str\n",
        "    task_embedding: np.ndarray\n",
        "    trajectory: List[Step] = Field(default_factory=list)\n",
        "    final_success: bool = False\n",
        "    final_reward: float = 0.0\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 3. MEMORY\n",
        "class MementoCaseBank:\n",
        "    def __init__(self, top_k=3, sim_threshold=0.60):\n",
        "        self.cases: List[Case] = []\n",
        "        self.top_k = top_k\n",
        "        self.sim_threshold = sim_threshold\n",
        "\n",
        "    def _embed(self, text: str) -> np.ndarray:\n",
        "        return embedder.encode(text, convert_to_numpy=True)\n",
        "\n",
        "    def add(self, task: str, trajectory: List[Step], success: bool, reward: float):\n",
        "        emb = self._embed(task)\n",
        "        for c in self.cases:\n",
        "            norm = np.linalg.norm(emb) * np.linalg.norm(c.task_embedding) + 1e-8\n",
        "            sim = np.dot(emb, c.task_embedding) / norm\n",
        "            if sim > 0.97:\n",
        "                if success and reward > c.final_reward:\n",
        "                    c.trajectory = trajectory\n",
        "                    c.final_success = success\n",
        "                    c.final_reward = reward\n",
        "                return\n",
        "        self.cases.append(Case(task=task, task_embedding=emb, trajectory=trajectory,\n",
        "                               final_success=success, final_reward=reward))\n",
        "\n",
        "    def retrieve(self, current_task: str) -> str:\n",
        "        if not self.cases:\n",
        "            return \"No past cases.\"\n",
        "\n",
        "        emb = self._embed(current_task)\n",
        "        scored = []\n",
        "\n",
        "        for c in self.cases:\n",
        "            norm = np.linalg.norm(emb) * np.linalg.norm(c.task_embedding) + 1e-8\n",
        "            sim = np.dot(emb, c.task_embedding) / norm\n",
        "            if sim >= self.sim_threshold:\n",
        "                scored.append((sim, c))\n",
        "\n",
        "        if not scored:\n",
        "            return \"No similar cases found.\"\n",
        "\n",
        "        scored.sort(key=lambda x: (-x[0], -x[1].final_reward))\n",
        "        top = scored[:self.top_k]\n",
        "\n",
        "        blocks = []\n",
        "        for sim, c in top:\n",
        "            traj = \"\\n\".join(\n",
        "                f\"  • {s.thought[:65] + '...' if len(s.thought) > 65 else s.thought} → \"\n",
        "                f\"{s.action} → {s.observation[:65] + '...' if len(s.observation) > 65 else s.observation}\"\n",
        "                for s in c.trajectory\n",
        "            )\n",
        "            blocks.append(f\"Sim: {sim:.3f} | Reward: {c.final_reward:.2f}\\nTask: {c.task}\\n{traj}\")\n",
        "\n",
        "        return \"--- RETRIEVED SUCCESSFUL STRATEGIES ---\\n\\n\" + \"\\n\\n\".join(blocks) + \"\\n\"\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 4. AGENT\n",
        "class MementoAgent:\n",
        "    def __init__(self, memory: MementoCaseBank, backend: str = \"grok\"):\n",
        "        self.memory = memory\n",
        "        self.backend = backend.lower()\n",
        "        self.max_steps = 4\n",
        "\n",
        "    def _call_planner(self, prompt: str) -> str:\n",
        "        if self.backend == \"grok\":\n",
        "            resp = xai_client.chat.create(\n",
        "                model=\"grok-4-1-fast-reasoning\",\n",
        "                messages=[system(\"You are a precise memory-driven planner.\"), user(prompt)]\n",
        "            )\n",
        "            return resp.choices[0].message.content if hasattr(resp, 'choices') else str(resp)\n",
        "\n",
        "        resp = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4.1\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a precise Memento-style planner.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "            max_tokens=400\n",
        "        )\n",
        "        return resp.choices[0].message.content\n",
        "\n",
        "    def _call_executor(self, action: str, task: str) -> str:\n",
        "        if self.backend == \"grok\":\n",
        "            return \"Success: data retrieved via reliable endpoint\"\n",
        "\n",
        "        exe_prompt = (\n",
        "            f\"For task '{task}' and action '{action}':\\n\"\n",
        "            \"Return ONLY exactly one of these two lines and nothing else:\\n\"\n",
        "            \"Success: data retrieved\\n\"\n",
        "            \"Error: failed to retrieve data\"\n",
        "        )\n",
        "        resp = openai_client.chat.completions.create(\n",
        "            model=\"o4-mini\",\n",
        "            messages=[{\"role\": \"user\", \"content\": exe_prompt}],\n",
        "            seed=42\n",
        "        )\n",
        "        return resp.choices[0].message.content.strip()\n",
        "\n",
        "    def run(self, task: str, force_first_failure: bool = False):\n",
        "        print(f\"\\n{'═'*75}\\n{self.backend.upper()} → {task}\\n{'═'*75}\")\n",
        "\n",
        "        context = self.memory.retrieve(task)\n",
        "        trajectory: List[Step] = []\n",
        "        success = False\n",
        "\n",
        "        for step_idx in range(1, self.max_steps + 1):\n",
        "            prompt = (\n",
        "                \"CRITICAL:\\n\"\n",
        "                \"• Start with: THOUGHT: From memory: ... Therefore I will...\\n\"\n",
        "                \"• Then: ACTION: USE_ENDPOINT or ACTION: FINISH\\n\\n\"\n",
        "                f\"{context}\\n\"\n",
        "                f\"Task: {task} (step {step_idx}/{self.max_steps})\\n\"\n",
        "            )\n",
        "\n",
        "            content = self._call_planner(prompt)\n",
        "\n",
        "            print(f\"[Step {step_idx} — {self.backend.upper()} Planner]\")\n",
        "            print(content.strip()[:500] + (\"...\" if len(content) > 500 else \"\"))\n",
        "\n",
        "            thought = \"No thought parsed\"\n",
        "            action_raw = \"USE_ENDPOINT\"\n",
        "            for line in content.splitlines():\n",
        "                line = line.strip()\n",
        "                if line.upper().startswith(\"THOUGHT:\"):\n",
        "                    thought = line[8:].strip()\n",
        "                elif line.upper().startswith(\"ACTION:\"):\n",
        "                    action_raw = line[7:].strip().upper()\n",
        "\n",
        "            if \"FINISH\" in action_raw:\n",
        "                trajectory.append(Step(thought=thought, action=\"FINISH\", observation=\"Complete\"))\n",
        "                success = True\n",
        "                break\n",
        "\n",
        "            if force_first_failure and len(self.memory.cases) == 0 and step_idx == 1:\n",
        "                obs = \"Forced demo failure — learning unreliable endpoint\"\n",
        "            else:\n",
        "                obs = self._call_executor(\"USE_ENDPOINT\", task)\n",
        "\n",
        "            trajectory.append(Step(thought=thought, action=\"USE_ENDPOINT\", observation=obs))\n",
        "            print(f\"→ Obs: {obs}\")\n",
        "\n",
        "            if \"Success\" in obs or \"retrieved\" in obs.lower():\n",
        "                success = True\n",
        "                break\n",
        "\n",
        "        final_reward = 1.0 if success else 0.0\n",
        "        print(f\"\\n[FINAL {self.backend.upper()}] Success = {success} | Reward = {final_reward:.2f}\\n\")\n",
        "        self.memory.add(task, trajectory, success, final_reward)\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────\n",
        "# 5. COMPARISON\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 MEMENTO COMPARISON — Grok vs GPT-4.1 + o4-mini\\n\")\n",
        "\n",
        "    tasks = [\n",
        "        \"Fetch quarterly sales report Q1 2025\",\n",
        "        \"Retrieve inventory status Europe region\",\n",
        "        \"Fetch quarterly sales report Q2 2025\",\n",
        "        \"Get marketing campaign performance Q1 2025\",\n",
        "        \"Fetch quarterly sales report Q3 2025\"\n",
        "    ]\n",
        "\n",
        "    # Grok\n",
        "    bank_grok = MementoCaseBank()\n",
        "    agent_grok = MementoAgent(bank_grok, backend=\"grok\")\n",
        "    print(\"=== GROK-4.1 MODE ===\")\n",
        "    for i, t in enumerate(tasks, 1):\n",
        "        agent_grok.run(t, force_first_failure=(i == 1))\n",
        "\n",
        "    # Paper\n",
        "    bank_paper = MementoCaseBank()\n",
        "    agent_paper = MementoAgent(bank_paper, backend=\"openai\")\n",
        "    print(\"\\n=== PAPER MODE ===\")\n",
        "    for i, t in enumerate(tasks, 1):\n",
        "        agent_paper.run(t, force_first_failure=(i == 1))\n",
        "\n",
        "    # Table\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"COMPARISON SUMMARY\")\n",
        "    print(\"=\"*100)\n",
        "    print(f\"{'Task':<50} {'Grok Steps':<12} {'Paper Steps':<12} {'Grok Succ':<10} {'Paper Succ':<10} {'Faster'}\")\n",
        "    print(\"-\"*100)\n",
        "    for i, task in enumerate(tasks):\n",
        "        g_steps = len(bank_grok.cases[i].trajectory) if i < len(bank_grok.cases) else \"N/A\"\n",
        "        p_steps = len(bank_paper.cases[i].trajectory) if i < len(bank_paper.cases) else \"N/A\"\n",
        "        g_succ = \"Yes\" if i < len(bank_grok.cases) and bank_grok.cases[i].final_success else \"No\"\n",
        "        p_succ = \"Yes\" if i < len(bank_paper.cases) and bank_paper.cases[i].final_success else \"No\"\n",
        "        faster = \"Tie\" if g_steps == p_steps else (\"Paper\" if isinstance(p_steps, int) and isinstance(g_steps, int) and p_steps < g_steps else \"Grok\")\n",
        "        print(f\"{task[:49]:<50} {g_steps:<12} {p_steps:<12} {g_succ:<10} {p_succ:<10} {faster}\")\n",
        "    print(\"=\"*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8115de1ad444440ca96eec4016420459",
            "31ea09551a9a4b6b87bf4fefdfb18b71",
            "50d255c2890b42e1a5598654c03e89e0",
            "d601020f0e6849eab2ad457cb4cbdc29",
            "dade9db6efb6485e8ba3d34172f3ea10",
            "6ccb68eb950b4b8d9e517054ef9273e4",
            "f737cb015bc24f87978a726e08e89044",
            "3430ed1e02b84b5c88b122caa44c39fd",
            "b5d7da44146d4e43ba2476f734feb6ff",
            "428f7e00b2374a9093096bb667bcd592",
            "82a1bf1b151c4b58a037c6300e6f3cf6"
          ]
        },
        "id": "n1JL5UoCk79-",
        "outputId": "198cceac-b57b-40b5-d498-06768af1c031"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8115de1ad444440ca96eec4016420459"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 MEMENTO COMPARISON — Grok vs GPT-4.1 + o4-mini\n",
            "\n",
            "=== GROK-4.1 MODE ===\n",
            "\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "GROK → Fetch quarterly sales report Q1 2025\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "[Step 1 — GROK Planner]\n",
            "messages {\n",
            "  content {\n",
            "    text: \"You are a precise memory-driven planner.\"\n",
            "  }\n",
            "  role: ROLE_SYSTEM\n",
            "}\n",
            "messages {\n",
            "  content {\n",
            "    text: \"CRITICAL:\\n• Start with: THOUGHT: From memory: ... Therefore I will...\\n• Then: ACTION: USE_ENDPOINT or ACTION: FINISH\\n\\nNo past cases.\\nTask: Fetch quarterly sales report Q1 2025 (step 1/4)\\n\"\n",
            "  }\n",
            "  role: ROLE_USER\n",
            "}\n",
            "model: \"grok-4-1-fast-reasoning\"\n",
            "→ Obs: Forced demo failure — learning unreliable endpoint\n",
            "[Step 2 — GROK Planner]\n",
            "messages {\n",
            "  content {\n",
            "    text: \"You are a precise memory-driven planner.\"\n",
            "  }\n",
            "  role: ROLE_SYSTEM\n",
            "}\n",
            "messages {\n",
            "  content {\n",
            "    text: \"CRITICAL:\\n• Start with: THOUGHT: From memory: ... Therefore I will...\\n• Then: ACTION: USE_ENDPOINT or ACTION: FINISH\\n\\nNo past cases.\\nTask: Fetch quarterly sales report Q1 2025 (step 2/4)\\n\"\n",
            "  }\n",
            "  role: ROLE_USER\n",
            "}\n",
            "model: \"grok-4-1-fast-reasoning\"\n",
            "→ Obs: Success: data retrieved via reliable endpoint\n",
            "\n",
            "[FINAL GROK] Success = True | Reward = 1.00\n",
            "\n",
            "\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "GROK → Retrieve inventory status Europe region\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "[Step 1 — GROK Planner]\n",
            "messages {\n",
            "  content {\n",
            "    text: \"You are a precise memory-driven planner.\"\n",
            "  }\n",
            "  role: ROLE_SYSTEM\n",
            "}\n",
            "messages {\n",
            "  content {\n",
            "    text: \"CRITICAL:\\n• Start with: THOUGHT: From memory: ... Therefore I will...\\n• Then: ACTION: USE_ENDPOINT or ACTION: FINISH\\n\\nNo similar cases found.\\nTask: Retrieve inventory status Europe region (step 1/4)\\n\"\n",
            "  }\n",
            "  role: ROLE_USER\n",
            "}\n",
            "model: \"grok-4-1-fast-reasoning\"\n",
            "→ Obs: Success: data retrieved via reliable endpoint\n",
            "\n",
            "[FINAL GROK] Success = True | Reward = 1.00\n",
            "\n",
            "\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "GROK → Fetch quarterly sales report Q2 2025\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "[Step 1 — GROK Planner]\n",
            "messages {\n",
            "  content {\n",
            "    text: \"You are a precise memory-driven planner.\"\n",
            "  }\n",
            "  role: ROLE_SYSTEM\n",
            "}\n",
            "messages {\n",
            "  content {\n",
            "    text: \"CRITICAL:\\n• Start with: THOUGHT: From memory: ... Therefore I will...\\n• Then: ACTION: USE_ENDPOINT or ACTION: FINISH\\n\\n--- RETRIEVED SUCCESSFUL STRATEGIES ---\\n\\nSim: 0.966 | Reward: 1.00\\nTask: Fetch quarterly sales report Q1 2025\\n  • No thought parsed → USE_ENDPOINT → Forced demo failure — learning unreliable endpoint\\n  • No thought parsed → USE_ENDPOINT ...\n",
            "→ Obs: Success: data retrieved via reliable endpoint\n",
            "\n",
            "[FINAL GROK] Success = True | Reward = 1.00\n",
            "\n",
            "\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "GROK → Get marketing campaign performance Q1 2025\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "[Step 1 — GROK Planner]\n",
            "messages {\n",
            "  content {\n",
            "    text: \"You are a precise memory-driven planner.\"\n",
            "  }\n",
            "  role: ROLE_SYSTEM\n",
            "}\n",
            "messages {\n",
            "  content {\n",
            "    text: \"CRITICAL:\\n• Start with: THOUGHT: From memory: ... Therefore I will...\\n• Then: ACTION: USE_ENDPOINT or ACTION: FINISH\\n\\nNo similar cases found.\\nTask: Get marketing campaign performance Q1 2025 (step 1/4)\\n\"\n",
            "  }\n",
            "  role: ROLE_USER\n",
            "}\n",
            "model: \"grok-4-1-fast-reasoning\"\n",
            "→ Obs: Success: data retrieved via reliable endpoint\n",
            "\n",
            "[FINAL GROK] Success = True | Reward = 1.00\n",
            "\n",
            "\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "GROK → Fetch quarterly sales report Q3 2025\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "[Step 1 — GROK Planner]\n",
            "messages {\n",
            "  content {\n",
            "    text: \"You are a precise memory-driven planner.\"\n",
            "  }\n",
            "  role: ROLE_SYSTEM\n",
            "}\n",
            "messages {\n",
            "  content {\n",
            "    text: \"CRITICAL:\\n• Start with: THOUGHT: From memory: ... Therefore I will...\\n• Then: ACTION: USE_ENDPOINT or ACTION: FINISH\\n\\n--- RETRIEVED SUCCESSFUL STRATEGIES ---\\n\\nSim: 0.955 | Reward: 1.00\\nTask: Fetch quarterly sales report Q1 2025\\n  • No thought parsed → USE_ENDPOINT → Forced demo failure — learning unreliable endpoint\\n  • No thought parsed → USE_ENDPOINT ...\n",
            "→ Obs: Success: data retrieved via reliable endpoint\n",
            "\n",
            "[FINAL GROK] Success = True | Reward = 1.00\n",
            "\n",
            "\n",
            "=== PAPER MODE ===\n",
            "\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "OPENAI → Fetch quarterly sales report Q1 2025\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "[Step 1 — OPENAI Planner]\n",
            "THOUGHT: From memory: The task is to fetch the quarterly sales report for Q1 2025, and this is step 1 of 4. Therefore I will initiate the process by locating the appropriate endpoint or database where quarterly sales reports are stored.\n",
            "\n",
            "ACTION: USE_ENDPOINT\n",
            "→ Obs: Forced demo failure — learning unreliable endpoint\n",
            "[Step 2 — OPENAI Planner]\n",
            "THOUGHT: From memory: The task is to fetch the quarterly sales report for Q1 2025, and this is step 2 out of 4. Therefore I will proceed to use the appropriate endpoint to retrieve the Q1 2025 sales report.\n",
            "\n",
            "ACTION: USE_ENDPOINT\n",
            "→ Obs: Success: data retrieved\n",
            "\n",
            "[FINAL OPENAI] Success = True | Reward = 1.00\n",
            "\n",
            "\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "OPENAI → Retrieve inventory status Europe region\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "[Step 1 — OPENAI Planner]\n",
            "THOUGHT: From memory: The task is to retrieve the inventory status for the Europe region, and this is step 1 of 4. Therefore I will initiate the process by querying the inventory database or system for the Europe region's current inventory status.\n",
            "\n",
            "ACTION: USE_ENDPOINT inventory_status(region=\"Europe\")\n",
            "→ Obs: Success: data retrieved\n",
            "\n",
            "[FINAL OPENAI] Success = True | Reward = 1.00\n",
            "\n",
            "\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "OPENAI → Fetch quarterly sales report Q2 2025\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "[Step 1 — OPENAI Planner]\n",
            "THOUGHT: From memory: The task is to fetch the quarterly sales report for Q2 2025. Previous similar tasks involved using the USE_ENDPOINT action to retrieve the report, which was successful for Q1 2025. Therefore I will attempt to use the endpoint to fetch the Q2 2025 report.\n",
            "\n",
            "ACTION: USE_ENDPOINT\n",
            "→ Obs: Success: data retrieved\n",
            "\n",
            "[FINAL OPENAI] Success = True | Reward = 1.00\n",
            "\n",
            "\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "OPENAI → Get marketing campaign performance Q1 2025\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "[Step 1 — OPENAI Planner]\n",
            "THOUGHT: From memory: The user wants to get the marketing campaign performance for Q1 2025. The first step is to identify and access the relevant data source or endpoint that contains marketing campaign performance metrics for the specified period. Therefore I will check for an endpoint or database that provides marketing campaign performance data for Q1 2025.\n",
            "\n",
            "ACTION: USE_ENDPOINT\n",
            "→ Obs: Success: data retrieved\n",
            "\n",
            "[FINAL OPENAI] Success = True | Reward = 1.00\n",
            "\n",
            "\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "OPENAI → Fetch quarterly sales report Q3 2025\n",
            "═══════════════════════════════════════════════════════════════════════════\n",
            "[Step 1 — OPENAI Planner]\n",
            "THOUGHT: From memory: For similar tasks (fetching quarterly sales reports for Q1 and Q2 2025), the successful strategy was to use the endpoint to retrieve the data. Therefore I will use the endpoint to fetch the quarterly sales report for Q3 2025.\n",
            "\n",
            "ACTION: USE_ENDPOINT\n",
            "→ Obs: Success: data retrieved\n",
            "\n",
            "[FINAL OPENAI] Success = True | Reward = 1.00\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            "COMPARISON SUMMARY\n",
            "====================================================================================================\n",
            "Task                                               Grok Steps   Paper Steps  Grok Succ  Paper Succ Faster\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Fetch quarterly sales report Q1 2025               2            2            Yes        Yes        Tie\n",
            "Retrieve inventory status Europe region            1            1            Yes        Yes        Tie\n",
            "Fetch quarterly sales report Q2 2025               1            1            Yes        Yes        Tie\n",
            "Get marketing campaign performance Q1 2025         1            1            Yes        Yes        Tie\n",
            "Fetch quarterly sales report Q3 2025               1            1            Yes        Yes        Tie\n",
            "====================================================================================================\n"
          ]
        }
      ]
    }
  ]
}