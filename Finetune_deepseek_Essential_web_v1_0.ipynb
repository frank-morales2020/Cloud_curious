{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPogDi9T5970YHMUr16bMcm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/Finetune_deepseek_Essential_web_v1_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Set Up Your Environment ---\n",
        "!pip install scikit-learn -q # For potential evaluation metrics (optional)\n",
        "!pip install -U transformers -q\n",
        "!pip install -U datasets -q\n",
        "!pip install -U accelerate -q\n",
        "!pip install -U peft -q\n",
        "!pip install -U trl -q # For SFTTrainer\n",
        "!pip install -U bitsandbytes -q\n",
        "!pip install unsloth -q # Recommended for speed and efficiency\n",
        "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # For latest Unsloth\n",
        "\n",
        "!pip install colab-env -q"
      ],
      "metadata": {
        "id": "f5fFLZz3vY8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env"
      ],
      "metadata": {
        "id": "W3rQp2P4B_Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "id": "_2eFtFEVQq4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/datasets/EssentialAI/essential-web-v1.0"
      ],
      "metadata": {
        "id": "9FB5YA6bRTKx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load in streaming mode\n",
        "raw_dataset = load_dataset(\"EssentialAI/essential-web-v1.0\", streaming=True)\n",
        "data_stream = raw_dataset[\"train\"]"
      ],
      "metadata": {
        "id": "c6q-UXXEP1VZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate through examples\n",
        "for example in data_stream.take(5):\n",
        "    print(example)\n"
      ],
      "metadata": {
        "id": "lmkgF1aV-ZD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Prepare the Training Dataset ---\n",
        "print(\"Loading and preparing EssentialAI/essential-web-v1.0 dataset...\")\n",
        "\n",
        "from datasets import load_dataset\n",
        "import itertools # Needed for skipping elements\n",
        "\n",
        "# Load in streaming mode as requested\n",
        "raw_dataset = load_dataset(\"EssentialAI/essential-web-v1.0\", streaming=True)\n",
        "raw_dataset = raw_dataset[\"train\"]\n",
        "\n",
        "eval_set_size = 200\n",
        "train_set_size = 12000\n",
        "\n",
        "eval_dataset = raw_dataset.take(eval_set_size)\n",
        "\n",
        "train_dataset = raw_dataset.skip(eval_set_size).take(train_set_size)\n",
        "\n",
        "test_dataset = eval_dataset.take(10) # Take 10 examples from the eval stream for testing"
      ],
      "metadata": {
        "id": "WpYpgqLZX8Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset preparation complete.\")\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# To see the actual examples in test_dataset\n",
        "for i, example in enumerate(test_dataset):\n",
        "    print(f\"Example {i+1}: {example}\")\n",
        "    if i >= 9: # Stop after 10 examples as test_dataset only has 10\n",
        "        break\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")"
      ],
      "metadata": {
        "id": "dLEUPdf3-lTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Eval sample size: {len(list(eval_dataset))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCU00ulAY9rL",
        "outputId": "6c56b3fe-15e8-4951-85c1-06f0a53e9c14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval sample size: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train sample size: {len(list(train_dataset))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRFw2KCCZCzz",
        "outputId": "856486bb-b68c-4986-be10-667e92505548"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sample size: 12000\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load in streaming mode\n",
        "raw_dataset = load_dataset(\"EssentialAI/essential-web-v1.0\", streaming=True)\n",
        "data_stream = raw_dataset[\"train\"]\n",
        "\n",
        "# Iterate through examples\n",
        "print(\"Inspecting first 5 examples from the raw dataset stream:\")\n",
        "for i, example in enumerate(data_stream.take(5)):\n",
        "    print(f\"Example {i+1}: {example}\")\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# %%\n",
        "# --- 4. Prepare the Training Dataset ---\n",
        "print(\"Loading and preparing EssentialAI/essential-web-v1.0 dataset...\")\n",
        "\n",
        "from datasets import load_dataset\n",
        "# Removed itertools as it's not explicitly used in this section\n",
        "# import itertools # Needed for skipping elements\n",
        "import torch # Import torch for bf16 check later\n",
        "\n",
        "# Load in streaming mode as requested\n",
        "raw_dataset = load_dataset(\"EssentialAI/essential-web-v1.0\", streaming=True)\n",
        "raw_dataset = raw_dataset[\"train\"]\n",
        "\n",
        "eval_set_size = 200\n",
        "train_set_size = 12000\n",
        "\n",
        "# Define a filter function to check for required keys and non-empty values\n",
        "def filter_essential_web_example(example):\n",
        "    # Check if both keys exist and their values are not None or empty strings\n",
        "    return (\n",
        "        \"prompt\" in example and example[\"prompt\"] is not None and example[\"prompt\"] != \"\" and\n",
        "        \"chosen\" in example and example[\"chosen\"] is not None and example[\"chosen\"] != \"\"\n",
        "    )\n",
        "\n",
        "# Define the formatting function\n",
        "def format_essential_web_example(example):\n",
        "    # This function assumes the example has \"prompt\" and \"chosen\" keys\n",
        "    # because we filter beforehand.\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"chosen\"]}\n",
        "    ]\n",
        "\n",
        "    # Apply the chat template. Handle potential errors during template application\n",
        "    try:\n",
        "        # Ensure tokenizer is accessible (should be from the global scope after loading model)\n",
        "        if 'tokenizer' not in globals():\n",
        "             raise ValueError(\"Tokenizer is not available in format_essential_web_example function scope.\")\n",
        "\n",
        "        formatted_text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_special_tokens=True # Include chat template special tokens\n",
        "        )\n",
        "        example[\"text\"] = formatted_text\n",
        "\n",
        "        # Also ensure the resulting text is not empty after formatting\n",
        "        if not example[\"text\"].strip(): # Use strip() to catch whitespace-only results\n",
        "             print(f\"Warning: Empty or whitespace-only 'text' generated after formatting for example with prompt: {example['prompt'][:100]}...\") # Print snippet of prompt\n",
        "             # You could potentially handle this by returning {\"text\": \"\"} here\n",
        "             # and filtering empty strings later, but the post-map filter handles this.\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error applying chat template to example: {e}\")\n",
        "        print(f\"Messages: {messages}\")\n",
        "        # If chat template application fails, set the text field to an empty string\n",
        "        example[\"text\"] = \"\"\n",
        "\n",
        "    return example\n",
        "\n",
        "# Apply skip and take *before* filtering and mapping\n",
        "# This ensures we are working with the correct subset of the raw stream as defined by the sizes.\n",
        "eval_dataset_raw = raw_dataset.take(eval_set_size)\n",
        "train_dataset_raw = raw_dataset.skip(eval_set_size).take(train_set_size)\n",
        "\n",
        "# Now apply filtering to remove examples without required keys/values,\n",
        "# then apply mapping, and finally filter out any examples where mapping resulted in empty text.\n",
        "eval_dataset = eval_dataset_raw.filter(filter_essential_web_example).map(format_essential_web_example, batched=False).filter(lambda x: x.get(\"text\", \"\") != \"\")\n",
        "train_dataset = train_dataset_raw.filter(filter_essential_web_example).map(format_essential_web_example, batched=False).filter(lambda x: x.get(\"text\", \"\") != \"\")\n",
        "\n",
        "# test_dataset is just for inspection, created from the eval split\n",
        "# Apply the same filtering and mapping\n",
        "test_dataset_raw = eval_dataset_raw.take(10) # Take 10 raw examples from the beginning of the eval stream\n",
        "test_dataset = test_dataset_raw.filter(filter_essential_web_example).map(format_essential_web_example, batched=False).filter(lambda x: x.get(\"text\", \"\") != \"\")\n",
        "\n",
        "\n",
        "print(\"Dataset preparation complete.\")\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# To see the actual examples in test_dataset\n",
        "# Iterate through test_dataset directly, which is now filtered and mapped\n",
        "print(\"Inspecting first 10 examples in the filtered and formatted test dataset:\")\n",
        "try:\n",
        "    # Use .take(10) on the streaming test_dataset\n",
        "    for i, example in enumerate(test_dataset.take(10)):\n",
        "        print(f\"--- Test Example {i+1} ---\")\n",
        "        # Print the full formatted text field\n",
        "        print(example.get(\"text\", \"No 'text' field found\"))\n",
        "        # You could also print other keys if needed, but 'text' is what the trainer uses\n",
        "        # print(f\"Original Prompt: {example.get('prompt', 'N/A')}\")\n",
        "        # print(f\"Original Chosen: {example.get('chosen', 'N/A')}\")\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "     print(f\"Could not iterate through test_dataset for inspection: {e}\")\n",
        "     print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "\n",
        "# Print eval sample size for estimation purposes\n",
        "# Note: Calling list() on streaming datasets consumes the stream.\n",
        "# This is generally okay for a small eval set if it's only done once here\n",
        "# and then the trainer iterates independently. Be mindful for larger datasets.\n",
        "try:\n",
        "    eval_size_list = list(eval_dataset)\n",
        "    print(f\"Eval sample size (after filtering/mapping): {len(eval_size_list)}\")\n",
        "    # You can uncomment this if you need to re-inspect eval_dataset after the count\n",
        "    # eval_dataset = raw_dataset.take(eval_set_size).filter(filter_essential_web_example).map(format_essential_web_example, batched=False).filter(lambda x: x.get(\"text\", \"\") != \"\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not determine eval sample size: {e}\")\n",
        "\n",
        "# Removed train_dataset len print to avoid consuming the stream before trainer uses it.\n",
        "# print(f\"Train sample size (after filtering/mapping): {len(list(train_dataset))}\")\n",
        "\n",
        "\n",
        "# Optional: Print a formatted example from the training set to check\n",
        "print(\"\\n--- Example of formatted training data (first element) ---\")\n",
        "try:\n",
        "    # Take the first example *after* mapping and filtering\n",
        "    # Use .take(1) and next(iter()) to get the first element without consuming the whole stream\n",
        "    first_formatted_example = next(iter(train_dataset.take(1)))\n",
        "    print(first_formatted_example.get(\"text\", \"No 'text' field found\"))\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "except StopIteration:\n",
        "     print(\"train_dataset is empty after mapping and filtering.\")\n",
        "     print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not print example from train_dataset after mapping and filtering: {e}\")\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "\n",
        "print(\"Dataset preparation complete (filtered and formatted).\")\n",
        "\n",
        "# --- 2. Load the Model and Tokenizer ---\n",
        "print(\"Loading DeepSeek-R1 model and tokenizer...\")\n",
        "# Increased max_seq_length as essential-web-v1.0 dialogues can be long\n",
        "max_seq_length = 4096 # Adjust if your combined input/output is longer\n",
        "# Use torch.bfloat16 if supported, otherwise torch.float16 (fp16)\n",
        "dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "load_in_4bit = True # Enable 4-bit quantization for memory efficiency\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import AutoTokenizer # Import AutoTokenizer just in case\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/DeepSeek-R1-Distill-Llama-8B\", # Recommended for fine-tuning\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit,\n",
        ")\n",
        "print(\"Model and tokenizer loaded.\")\n",
        "\n",
        "# Ensure the tokenizer has a pad_token set, using eos_token if pad_token is None\n",
        "# This is often needed for batching and generation\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"Tokenizer pad_token was None, set to eos_token: {tokenizer.pad_token}\")\n",
        "\n",
        "# --- 3. Apply LoRA Adapters ---\n",
        "print(\"Applying LoRA adapters...\")\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16, # Rank of the LoRA matrices (common values: 8, 16, 32, 64)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # DeepSeek specific modules\n",
        "    lora_alpha=16, # Scaling factor for LoRA weights\n",
        "    lora_dropout=0, # Dropout rate for LoRA (set to 0 for inference)\n",
        "    bias=\"none\", # Or \"all\", \"lora_only\"\n",
        "    use_gradient_checkpointing=True, # Recommended for memory saving\n",
        "    random_state=3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None,\n",
        ")\n",
        "print(\"LoRA adapters applied.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "EGpVPlqPi6_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Set Up and Configure the Trainer ---\n",
        "print(\"Setting up SFTTrainer...\")\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from datasets import load_dataset # Import load_dataset here\n",
        "\n",
        "# Re-load the dataset streams specifically for the trainer\n",
        "# This ensures the streams are fresh and not consumed by previous operations\n",
        "print(\"Re-loading dataset streams for SFTTrainer...\")\n",
        "# Load the dataset from the start\n",
        "raw_dataset_full_stream = load_dataset(\"EssentialAI/essential-web-v1.0\", streaming=True)[\"train\"]"
      ],
      "metadata": {
        "id": "YKS867p2mKtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Define the filter function\n",
        "def filter_essential_web_example(example):\n",
        "    # Check if both keys exist and their values are not None or empty strings\n",
        "    return (\n",
        "        \"prompt\" in example and example[\"prompt\"] is not None and example[\"prompt\"] != \"\" and\n",
        "        \"chosen\" in example and example[\"chosen\"] is not None and example[\"chosen\"] != \"\"\n",
        "    )\n",
        "\n",
        "# Define the formatting function (assuming tokenizer is available globally from cell ipython-input-10)\n",
        "def format_essential_web_example(example):\n",
        "    # This function assumes the example has \"prompt\" and \"chosen\" keys\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"chosen\"]}\n",
        "    ]\n",
        "    try:\n",
        "        # Ensure tokenizer is available. If not, it will raise the ValueError.\n",
        "        if 'tokenizer' not in globals():\n",
        "             raise ValueError(\"Tokenizer is not available in format_essential_web_example function scope.\")\n",
        "        formatted_text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "        example[\"text\"] = formatted_text\n",
        "        if not example[\"text\"].strip():\n",
        "             # print(f\"Warning: Empty or whitespace-only 'text' generated after formatting for example with prompt: {example['prompt'][:100]}...\")\n",
        "             example[\"text\"] = \"\" # Explicitly set to empty string if needed\n",
        "    except Exception as e:\n",
        "        # print(f\"Error applying chat template to example: {e}\")\n",
        "        # print(f\"Messages: {messages}\")\n",
        "        example[\"text\"] = \"\"\n",
        "    return example\n",
        "\n",
        "# Define dataset sizes - REDUCED FOR POC\n",
        "eval_set_size = 50 # Significantly smaller\n",
        "train_set_size = 1000 # Significantly smaller\n",
        "\n",
        "\n",
        "# First, apply filtering and mapping to the raw stream\n",
        "processed_dataset_stream = raw_dataset_full_stream \\\n",
        "    .filter(filter_essential_web_example) \\\n",
        "    .map(format_essential_web_example, batched=False) \\\n",
        "    .filter(lambda x: x.get(\"text\", \"\") != \"\")\n",
        "\n",
        "# Now, split the processed stream into train and eval using take/skip\n",
        "# Note: take and skip on streaming datasets are sequential.\n",
        "# The first N elements go to eval, the next M elements go to train.\n",
        "eval_dataset_for_trainer = processed_dataset_stream.take(eval_set_size)\n",
        "train_dataset_for_trainer = processed_dataset_stream.skip(eval_set_size).take(train_set_size)\n",
        "\n",
        "\n",
        "print(\"Dataset streams re-loaded and prepared for trainer.\")\n",
        "\n",
        "# Define TrainingArguments - ADJUSTED FOR POC\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=4, # Maybe increase if GPU can handle\n",
        "    gradient_accumulation_steps=2, # Adjust based on batch size\n",
        "    warmup_steps=5, # Shorter warmup\n",
        "    num_train_epochs=1, # Just 1 epoch for a quick run\n",
        "    learning_rate=2e-4,\n",
        "    fp16= (dtype == torch.float16),\n",
        "    bf16= (dtype == torch.bfloat16),\n",
        "    logging_steps=50, # Log less often\n",
        "    output_dir=\"./sft_results\",\n",
        "    optim=\"adamw_8bit\",\n",
        "    seed=3407,\n",
        "    save_steps=10000, # Save very rarely or effectively disable\n",
        "    save_total_limit=1,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=10000, # Evaluate very rarely or effectively disable\n",
        "    load_best_model_at_end=False, # Don't need to load best model for a quick test\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Create the SFTTrainer instance\n",
        "# The trainer will now use the train_dataset_for_trainer stream\n",
        "# Note: SFTTrainer handles iteration internally from the provided dataset object.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    # Pass the newly created, unconsumed streams\n",
        "    train_dataset=train_dataset_for_trainer,\n",
        "    eval_dataset=eval_dataset_for_trainer, # Use eval_dataset for evaluation during training\n",
        "    dataset_text_field=\"text\", # Use the formatted 'text' field\n",
        "    max_seq_length=max_seq_length,\n",
        "    args=training_args, # Pass the defined TrainingArguments object\n",
        ")\n",
        "print(\"SFTTrainer configured.\")\n",
        "\n",
        "# --- 6. Start Training ---\n",
        "print(\"Starting training...\")\n",
        "# Check if train_dataset is empty before starting training\n",
        "# This check is less error-prone if done on the trainer's dataset object\n",
        "# after initialization.\n",
        "try:\n",
        "    # Attempt to take the first element of the trainer's train dataset iterator\n",
        "    # This is similar to what the SFTTrainer might do internally, but we handle\n",
        "    # the StopIteration here explicitly *before* calling train().\n",
        "    # Using a simple loop is safer than next(iter(...)) as it doesn't require\n",
        "    # the dataset object itself to be directly iterable in all cases.\n",
        "    # However, SFTTrainer expects an iterable dataset.\n",
        "    # A more robust check for streaming datasets might involve trying to get a batch\n",
        "    # or relying on the trainer's own checks. The original error happens *during*\n",
        "    # trainer init, suggesting the dataset provided was immediately seen as empty.\n",
        "    # If the above stream creation logic is correct and results in a non-empty stream,\n",
        "    # the error should be resolved.\n",
        "    # Let's remove the manual check here as the SFTTrainer's internal check is failing.\n",
        "    # The fix focuses on ensuring the stream passed to the trainer is valid.\n",
        "    pass # Removed the manual check\n",
        "\n",
        "    # Start the training process\n",
        "    train_result = trainer.train()\n",
        "    print(\"Training complete.\")\n",
        "\n",
        "except StopIteration:\n",
        "    print(\"Training dataset is empty after filtering and mapping. Cannot start training.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during training: {e}\")\n",
        "    # Print detailed traceback if available\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eR6Jomy8mGKr",
        "outputId": "f7526a4d-4f9f-445b-8b3b-10e30159bf70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset streams re-loaded and prepared for trainer.\n"
          ]
        }
      ]
    }
  ]
}