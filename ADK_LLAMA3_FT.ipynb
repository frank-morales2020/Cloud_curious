{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/ADK_LLAMA3_FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etm0HfcZM151"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 IN GOOGLE COLAB\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65JpttRiGxVK"
      },
      "outputs": [],
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "37yP0eJDM152"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write, # ADD YOUR TOKEN HERE\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "S9qvxG2HYusD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        "    EarlyStoppingCallback\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtE9TuSMlUEc"
      },
      "outputs": [],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GapRov3hl4fT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e1b3b4-345d-457d-a31a-b460e33d71ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "Thu May  8 11:30:04 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   54C    P8             18W /   72W |       3MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjTwqcXmK_OR"
      },
      "source": [
        "## Test Model and run Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW_hl8YPKxQ2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "peft_model_id = \"/content/gdrive/MyDrive/model/07MAY2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata-epochs1\"\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load Model with PEFT adapter\n",
        "model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "  peft_model_id,\n",
        "  device_map=\"auto\",\n",
        "  torch_dtype=torch.float16,\n",
        "  attn_implementation=\"flash_attention_2\",\n",
        "  quantization_config=bnb_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uHtGXcrLKI_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from random import randint\n",
        "\n",
        "\n",
        "# Load our test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "nrec= randint(0, len(eval_dataset))\n",
        "nrec=6\n",
        "\n",
        "# Test on sample\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map\n",
        "prompt =  eval_dataset[nrec]['text']\n",
        "\n",
        "\n",
        "outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_GZg3t7d1z9s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "eefb5fcf-6275-4e3b-871e-0d64703e95c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'antral follicle count </s><s>[INST] the aim of this T0 was to evaluate the effect of a single dose of mg of the selective alpha adrenergic agonist clonidine on the cardiovascular and hormonal responses to a standardized mental arithmetic task in healthy volunteers the T0 was performed in a doubleblind randomized crossover design in healthy volunteers mean age years mean blood pressure mmhg mean heart rate bpm the subjects were studied on two occasions separated by a week interval on one occasion they received mg of clonidine and on the other occasion they received PL the order of the two treatments was randomized the subjects'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "ganswer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q2zlGDkw1rE",
        "outputId": "1a772eb6-ce30-4c71-f761-fbf1c2d37794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "\n",
            "Original Answer:\n",
            "antral follicle count\n",
            "\n",
            "Generated Answer:\n",
            "antral follicle count\n",
            "\n",
            "Match\n"
          ]
        }
      ],
      "source": [
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print()\n",
        "oanswer=str(eval_dataset[nrec]['label'])\n",
        "oanswer=oanswer[2:len(oanswer)-2]\n",
        "print(f\"Original Answer:\\n{oanswer}\")\n",
        "print()\n",
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "qc=str(ganswer).find('[INST]')\n",
        "ganswer=ganswer[0:qc-7]\n",
        "qc0=str(ganswer).find('[INST]')\n",
        "ganswer=str(ganswer)[0:qc0]\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "if qc>0:\n",
        "  ganswer=ganswer[qc+8:len(ganswer)]\n",
        "print(f\"Generated Answer:\\n{ganswer}\")\n",
        "print()\n",
        "if ganswer == oanswer:\n",
        "  print(\"Match\")\n",
        "else:\n",
        "  print(\"NO Match\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VX6ChYhtvtat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dec4abd-aa10-4b3c-c421-d84eef144ad5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "while diminished ovarian reserve dor predicts decreased ovarian response to stimulation it does not necessarily foretell about the fecundity cycle according to bolognas criteria laid down by the european society of human reproduction and embryology old age abnormal ovarian reserve tests such as AFC afc and antimullerian hormone amh as well as prior suboptimal response to stimulation are the main AF representing dor unfavorable response to maximal stimulation on two previous occasions may also represent dor among the ovarian reserve tests amh and afc are the most predictive values for dor AF which may give rise to dor include environmental factors autoimmune or metabolic disorders infections genetic abnormalities and iatrogenic causes such as smoking chemotherapy radiation and gynecologic surgeries besides studies have proposed endometriosis as a key contributor to dor and hence emphasized on its proper management to prevent additional damages leading to compromised fertility in summary dor is found to be a clinical challenge in the practice of fertility care with controversial countermeasures to prevent or treat the condition nevertheless some promising measure such as oocyte embryo and tissue cryopreservation ovarian transplantation dietary supplementation and the transfer of mitochondria have offered hopes towards ameliorating the burden of dor this review attempts to discuss dor from different perspectives and summarize some existing hopes in clinical practice\n",
            "\n",
            "Original Answer:\n",
            "['antral follicle count']\n",
            "\n",
            "Full Generated Answer:\n",
            "[/INST] antral follicle count </s><s>[INST] the aim of this T0 was to evaluate the effect of a single dose of mg of the selective alpha adrenergic agonist clonidine on the cardiovascular and hormonal responses to a standardized mental arithmetic task in healthy volunteers the T0 was performed in a doubleblind randomized crossover design in healthy volunteers mean age years mean blood pressure mmhg mean heart rate bpm the subjects were studied on two occasions separated by a week interval on one occasion they received mg of clonidine and on the other occasion they received PL the order of the two treatments was randomized the subjects\n"
          ]
        }
      ],
      "source": [
        "nrec=6\n",
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print()\n",
        "print(f\"Original Answer:\\n{eval_dataset[nrec]['label']}\")\n",
        "print()\n",
        "print(f\"Full Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KcFTvrGSvJ8G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "ad392ca8-466b-4deb-a140-f52a079d1241"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[/INST] antral follicle count </s><s>[INST] the aim of this T0 was to evaluate the effect of a single dose of mg of the selective alpha adrenergic agonist clonidine on the cardiovascular and hormonal responses to a standardized mental arithmetic task in healthy volunteers the T0 was performed in a doubleblind randomized crossover design in healthy volunteers mean age years mean blood pressure mmhg mean heart rate bpm the subjects were studied on two occasions separated by a week interval on one occasion they received mg of clonidine and on the other occasion they received PL the order of the two treatments was randomized the subjects'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "#print(qc)\n",
        "if int(qc)<0:\n",
        "    #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "    ganswer=outputs[0]['generated_text'][len(prompt)+8:].strip()\n",
        "else:\n",
        "    ganswer=ganswer[qc:len(ganswer)]\n",
        "ganswer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxF38_Am7b_5",
        "outputId": "5659d019-5e3a-48be-cc39-d19dbc38106f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Answer:\n",
            "antral follicle count\n"
          ]
        }
      ],
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "qc0=str(ganswer).find('[INST]')\n",
        "ganswer=str(ganswer)[0:qc0-8]\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "\n",
        "if qc>=0:\n",
        "  ganswer=ganswer[qc+8:len(ganswer)]\n",
        "\n",
        "print(f\"Generated Answer:\\n{ganswer}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load a pre-trained sentence embedding model\n",
        "model_embedding = SentenceTransformer('all-mpnet-base-v2')\n",
        "\n",
        "# Load your test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "\n",
        "# Create the generation pipeline\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "\n",
        "def evaluate(sample):\n",
        "    prompt = sample['text']\n",
        "    outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.7,\n",
        "                                  top_k=50, top_p=0.7, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    ganswer = outputs[0]['generated_text'][len(prompt):].strip()\n",
        "    qc0 = str(ganswer).find('[/INST]')\n",
        "    qc1 = str(ganswer).find('[INST]')\n",
        "    ganswer = str(ganswer)[qc0 + 8:qc1 - 8]\n",
        "\n",
        "    oanswer = str(sample['label'])\n",
        "    oanswer = oanswer[2:len(oanswer) - 2]\n",
        "\n",
        "    if len(oanswer) == 0:\n",
        "        oanswer = 'Not possible to get or use'\n",
        "\n",
        "    #print('\\n\\n')\n",
        "    #print(f\"Question: {prompt}\")\n",
        "    #print(f\"Generated Answer: {ganswer}\")\n",
        "    #print(f\"Original Answer: {oanswer}\")\n",
        "\n",
        "    # Generate sentence embeddings\n",
        "    embedding1 = model_embedding.encode(ganswer, convert_to_tensor=True)\n",
        "    embedding2 = model_embedding.encode(oanswer, convert_to_tensor=True)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    cosine_sim = util.cos_sim(embedding1, embedding2).item()\n",
        "\n",
        "    # Set a similarity threshold (adjust as needed)\n",
        "    if cosine_sim >= 0.5:\n",
        "        #print(\"Match\")\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n"
      ],
      "metadata": {
        "id": "MC-SNHc8v63E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "success_rate = []\n",
        "number_of_eval_samples = 10  # Adjust the number of samples as needed\n",
        "\n",
        "# Evaluate the model\n",
        "for n in tqdm(range(number_of_eval_samples)):\n",
        "    s = eval_dataset[n]\n",
        "    success_rate.append(evaluate(s))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-faI56d6sl9L",
        "outputId": "7aff326e-562c-476f-cd8e-1d9d8dc584a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 6/10 [01:40<01:07, 16.78s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            " 70%|███████   | 7/10 [01:57<00:50, 16.73s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "accuracy = sum(success_rate) / len(success_rate)\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxoCKRJddDW2",
        "outputId": "7d42da4c-02f1-4a52-a8ce-6ff1c6e80b66"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AGENT"
      ],
      "metadata": {
        "id": "GvHJA9_AfdlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-adk -q"
      ],
      "metadata": {
        "id": "9rs7srDAfvZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio -q"
      ],
      "metadata": {
        "id": "EeuQg5z9ZE8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "import os\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.artifacts import InMemoryArtifactService\n",
        "from google.adk.runners import Runner\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define the local path to your fine-tuned model\n",
        "# This path is from your Google Drive mount in the Colab environment\n",
        "FINE_TUNED_MODEL_PATH = \"/content/gdrive/MyDrive/model/07MAY2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata-epochs1\"\n",
        "\n",
        "# Load the model and tokenizer from the local path using transformers\n",
        "# We assume this code is run in an environment where this path is accessible\n",
        "try:\n",
        "    # Determine the device to use (GPU if available, otherwise CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(FINE_TUNED_MODEL_PATH)\n",
        "    # Set padding token if it's not already set\n",
        "    if tokenizer.pad_token is None:\n",
        "         tokenizer.pad_token = tokenizer.eos_token\n",
        "         if tokenizer.pad_token is None:\n",
        "             # Fallback if EOS token is also not set\n",
        "             tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "\n",
        "    # Load the model\n",
        "    # Use torch_dtype=torch.bfloat16 if your GPU supports it for potentially faster inference\n",
        "    model = AutoModelForCausalLM.from_pretrained(FINE_TUNED_MODEL_PATH, torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32).to(device)\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "\n",
        "    print(f\"Successfully loaded model and tokenizer from {FINE_TUNED_MODEL_PATH}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model or tokenizer from {FINE_TUNED_MODEL_PATH}. Please ensure the path is correct and the necessary libraries are installed.\")\n",
        "    print(f\"Details: {e}\")\n",
        "    # Exit or handle the error appropriately if the model cannot be loaded\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "\n",
        "\n",
        "# Define a simple agent class inheriting from ADK's Agent\n",
        "class LocalFineTunedMedicalAgent(Agent):\n",
        "    def __init__(self, name: str, instruction: str, model, tokenizer):\n",
        "        # Initialize the Agent with name and instruction, model is a string\n",
        "        # Set model path instead of None as ADK expects a string\n",
        "        super().__init__(name=name, instruction=instruction, model=FINE_TUNED_MODEL_PATH)\n",
        "        self._local_model = model\n",
        "        self._local_tokenizer = tokenizer\n",
        "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # The core asynchronous method where the agent processes input\n",
        "    async def _run_async_impl(self, context): # Remove type hint for InvocationContext\n",
        "        # Access the user's query from the invocation context\n",
        "        user_query = context.request.message.text\n",
        "\n",
        "        # print(f\"Agent '{self.name}' received query: {user_query}\") # Uncomment for more detailed logging\n",
        "\n",
        "        # --- Text Generation using the locally loaded transformers model ---\n",
        "        try:\n",
        "            # Prepare the prompt\n",
        "            # You might want to format the prompt based on your model's\n",
        "            # preferred input format (e.g., chat format like Llama 3)\n",
        "            # For this example, we'll use a simple instruction format derived from your notebook\n",
        "            # Assuming your model was fine-tuned with prompts like \"[INST] instruction [/INST]\"\n",
        "            prompt = f\"[INST] {self.instruction}\\n{user_query} [/INST]\"\n",
        "\n",
        "\n",
        "            # Tokenize the input\n",
        "            inputs = self._local_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(self._device)\n",
        "\n",
        "            # Generate text\n",
        "            with torch.no_grad(): # Disable gradient calculation for inference\n",
        "                 outputs = self._local_model.generate(\n",
        "                     **inputs,\n",
        "                     max_new_tokens=256, # Limit the generated response length\n",
        "                     num_return_sequences=1,\n",
        "                     do_sample=True,\n",
        "                     top_k=50,\n",
        "                     top_p=0.95,\n",
        "                     temperature=0.7,\n",
        "                     # Add other generation parameters as needed\n",
        "                     pad_token_id=self._local_tokenizer.pad_token_id # Ensure pad_token_id is set for generation\n",
        "                 )\n",
        "\n",
        "            # Decode the generated tokens\n",
        "            generated_text_with_prompt = self._local_tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "            # Post-process the generated text to extract the agent's response\n",
        "            # Based on the prompt format \"[INST] ... [/INST] response\", we want the part after [/INST]\n",
        "            if \"[/INST]\" in generated_text_with_prompt:\n",
        "                generated_text = generated_text_with_prompt.split(\"[/INST]\", 1)[1].strip()\n",
        "            else:\n",
        "                 # Fallback if the expected format is not present\n",
        "                 generated_text = generated_text_with_prompt.strip()\n",
        "\n",
        "            # Further cleaning might be needed depending on model output (e.g., cutting off at a specific token or phrase)\n",
        "            # If the model generates a new turn of the prompt, cut it off\n",
        "            if \"[INST]\" in generated_text:\n",
        "                 generated_text = generated_text.split(\"[INST]\", 1)[0].strip()\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            generated_text = f\"An error occurred during text generation: {e}\"\n",
        "            print(f\"Error during text generation with transformers: {e}\")\n",
        "\n",
        "        # Yield the final response from the agent\n",
        "        yield context.AgentResponse( # Use context.AgentResponse directly\n",
        "            message=context.AgentMessage(text=generated_text) # Use context.AgentMessage directly\n",
        "        )\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "18f4cc10fe034f6db582c93430d78ccf",
            "db2bfe4c7d724ae3a4641b92b95cd2f0",
            "a14d4588d2bb4fdeb16c257beac88edd",
            "ac29da780b30466493f7013c1480cd1a",
            "34c90df1a80145e8bc8b9881ed4d3350",
            "950d12fa824f41c0819db94664fc805f",
            "9df95b52f8c8480dbd55cc55608a7cf1",
            "b2dd9dfb722b41df93049af87562e15e",
            "76605d57c63541b7b471a81703b10d34",
            "fb8973f67fe540ceaa6a4304d7610a62",
            "4995ddf639dd4a28bebd4b9333bde019"
          ]
        },
        "id": "-6OgUW0uXNuO",
        "outputId": "43559a1a-43d2-45e5-9f3e-a816bf6fea9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18f4cc10fe034f6db582c93430d78ccf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded model and tokenizer from /content/gdrive/MyDrive/model/07MAY2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine-evaldata-epochs1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show google-adk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lOYUvjbkyPB",
        "outputId": "ed6424ea-b595-4882-eb06-c9c38a07a9c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: google-adk\n",
            "Version: 0.4.0\n",
            "Summary: Agent Development Kit\n",
            "Home-page: https://google.github.io/adk-docs/\n",
            "Author: \n",
            "Author-email: Google LLC <googleapis-packages@google.com>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: authlib, click, fastapi, google-api-python-client, google-cloud-aiplatform, google-cloud-secret-manager, google-cloud-speech, google-cloud-storage, google-genai, graphviz, mcp, opentelemetry-api, opentelemetry-exporter-gcp-trace, opentelemetry-sdk, pydantic, python-dotenv, PyYAML, sqlalchemy, tzlocal, uvicorn\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.adk import AgentMessage #Import the AgentMessage class here\n"
      ],
      "metadata": {
        "id": "NydhrMoTmx4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    # Ensure model and tokenizer were loaded successfully\n",
        "    if model is None or tokenizer is None:\n",
        "        print(\"Model loading failed. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # 1. Instantiate the in-memory services for artifacts and sessions\n",
        "    artifact_service = InMemoryArtifactService()\n",
        "    session_service = InMemorySessionService()\n",
        "\n",
        "    # 2. Create an instance of your local fine-tuned agent\n",
        "    agent_name = \"LocalFineTunedMedicalAssistant\"\n",
        "    agent_instruction = (\n",
        "        \"You are a highly specialized AI medical assistant, fine-tuned to provide information \"\n",
        "        \"based on medical literature. Answer questions accurately and concisely, focusing on \"\n",
        "        \"information related to medical concepts and terminology.\"\n",
        "    )\n",
        "\n",
        "    medical_agent = LocalFineTunedMedicalAgent(\n",
        "        name=agent_name,\n",
        "        instruction=agent_instruction,\n",
        "        model=model,          # Pass the loaded transformers model\n",
        "        tokenizer=tokenizer   # Pass the loaded transformers tokenizer\n",
        "    )\n",
        "\n",
        "    # 3. Create a Runner instance\n",
        "    runner = Runner(\n",
        "        agent=medical_agent,\n",
        "        session_service=session_service,\n",
        "        artifact_service=artifact_service,\n",
        "        app_name=\"MyMedicalApp\"  # Add app_name here with a suitable name\n",
        "    )\n",
        "\n",
        "    # 4. Define a test question\n",
        "    test_question = \"\"\"while diminished ovarian reserve dor predicts decreased ovarian response to\n",
        "stimulation it does not necessarily foretell about the fecundity cycle acco\n",
        "rding to bolognas criteria laid down by the european society of human repro\n",
        "duction and embryology old age abnormal ovarian reserve tests such as AFC a\n",
        "fc and antimullerian hormone amh as well as prior suboptimal response to st\n",
        "imulation are the main AF representing dor unfavorable response to maximal\n",
        "stimulation on two previous occasions may also represent dor among the ovar\n",
        "ian reserve tests amh and afc are the most predictive values for dor AF whi\n",
        "ch may give rise to dor include environmental factors autoimmune or metabol\n",
        "ic disorders infections genetic abnormalities and iatrogenic causes such as\n",
        "smoking chemotherapy radiation and gynecologic surgeries besides studies ha\n",
        "ve proposed endometriosis as a key contributor to dor and hence emphasized\n",
        "on its proper management to prevent additional damages leading to compromis\n",
        "ed fertility in summary dor is found to be a clinical challenge in the prac\n",
        "tice of fertility care with controversial countermeasures to prevent or tre\n",
        "at the condition nevertheless some promising measure such as oocyte embryo\n",
        "and tissue cryopreservation ovarian transplantation dietary supplementation\n",
        "and the transfer of mitochondria have offered hopes towards ameliorating th\n",
        "e burden of dor this review attempts to discuss dor from different perspect\n",
        "ives and summarize some existing hopes in clinical practice\"\"\"\n",
        "\n",
        "    test_original_answer = \"antral follicle count\" # The expected answer for comparison\n",
        "\n",
        "    print('\\n')\n",
        "    print(f\"\\n--- Testing the Agent (Basic Runner Call) ---\")\n",
        "    print(f\"Input Question: {test_question}\")\n",
        "    print('\\n')\n",
        "    print(f\"Expected Answer: {test_original_answer}\")\n",
        "    print('\\n')\n",
        "\n",
        "    # Use a unique user and session ID for this test\n",
        "    user_id = \"test_user\"\n",
        "    session_id = \"test_session\"\n",
        "\n",
        "    try:\n",
        "        # Execute the agent with user_id and session_id only\n",
        "        run_result = await runner.run(user_id=user_id, session_id=session_id)\n",
        "        async for event in run_result:\n",
        "            if event.is_user_query():\n",
        "                # If it expects a response, send the test question\n",
        "                await runner.respond(user_id=user_id, session_id=session_id, response=test_question)\n",
        "            if event.is_final_response():\n",
        "                print(f\"Agent Generated Answer: {event.message.text}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during agent execution: {e}\")"
      ],
      "metadata": {
        "id": "2pVcc5PPim79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entry point to run the asynchronous example\n",
        "if __name__ == \"__main__\":\n",
        "    nest_asyncio.apply()\n",
        "    loop = asyncio.get_event_loop()\n",
        "    loop.run_until_complete(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VHNUD6dgoZr",
        "outputId": "4e4a387f-1a56-40ed-a3ec-8748462151f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "--- Testing the Agent (Basic Runner Call) ---\n",
            "Input Question: while diminished ovarian reserve dor predicts decreased ovarian response to\n",
            "stimulation it does not necessarily foretell about the fecundity cycle acco\n",
            "rding to bolognas criteria laid down by the european society of human repro\n",
            "duction and embryology old age abnormal ovarian reserve tests such as AFC a\n",
            "fc and antimullerian hormone amh as well as prior suboptimal response to st\n",
            "imulation are the main AF representing dor unfavorable response to maximal\n",
            "stimulation on two previous occasions may also represent dor among the ovar\n",
            "ian reserve tests amh and afc are the most predictive values for dor AF whi\n",
            "ch may give rise to dor include environmental factors autoimmune or metabol\n",
            "ic disorders infections genetic abnormalities and iatrogenic causes such as\n",
            "smoking chemotherapy radiation and gynecologic surgeries besides studies ha\n",
            "ve proposed endometriosis as a key contributor to dor and hence emphasized\n",
            "on its proper management to prevent additional damages leading to compromis\n",
            "ed fertility in summary dor is found to be a clinical challenge in the prac\n",
            "tice of fertility care with controversial countermeasures to prevent or tre\n",
            "at the condition nevertheless some promising measure such as oocyte embryo\n",
            "and tissue cryopreservation ovarian transplantation dietary supplementation\n",
            "and the transfer of mitochondria have offered hopes towards ameliorating th\n",
            "e burden of dor this review attempts to discuss dor from different perspect\n",
            "ives and summarize some existing hopes in clinical practice\n",
            "\n",
            "\n",
            "Expected Answer: antral follicle count\n",
            "\n",
            "\n",
            "An error occurred during agent execution: Runner.run() missing 1 required keyword-only argument: 'new_message'\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18f4cc10fe034f6db582c93430d78ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db2bfe4c7d724ae3a4641b92b95cd2f0",
              "IPY_MODEL_a14d4588d2bb4fdeb16c257beac88edd",
              "IPY_MODEL_ac29da780b30466493f7013c1480cd1a"
            ],
            "layout": "IPY_MODEL_34c90df1a80145e8bc8b9881ed4d3350"
          }
        },
        "db2bfe4c7d724ae3a4641b92b95cd2f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_950d12fa824f41c0819db94664fc805f",
            "placeholder": "​",
            "style": "IPY_MODEL_9df95b52f8c8480dbd55cc55608a7cf1",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a14d4588d2bb4fdeb16c257beac88edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2dd9dfb722b41df93049af87562e15e",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_76605d57c63541b7b471a81703b10d34",
            "value": 4
          }
        },
        "ac29da780b30466493f7013c1480cd1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb8973f67fe540ceaa6a4304d7610a62",
            "placeholder": "​",
            "style": "IPY_MODEL_4995ddf639dd4a28bebd4b9333bde019",
            "value": " 4/4 [00:00&lt;00:00, 48.19it/s]"
          }
        },
        "34c90df1a80145e8bc8b9881ed4d3350": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "950d12fa824f41c0819db94664fc805f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9df95b52f8c8480dbd55cc55608a7cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2dd9dfb722b41df93049af87562e15e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76605d57c63541b7b471a81703b10d34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb8973f67fe540ceaa6a4304d7610a62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4995ddf639dd4a28bebd4b9333bde019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}