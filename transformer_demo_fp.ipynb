{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNWKTVy7EEwjkpBNn3GmIrz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/transformer_demo_fp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch -q"
      ],
      "metadata": {
        "id": "LpoVJAMMt74m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers\n",
        "print()\n",
        "!pip show datasets\n",
        "print()\n",
        "!pip show torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7cb3yWFuyoy",
        "outputId": "3e9d3bd8-35b3-4931-af17-62e162278704"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.50.3\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: peft, sentence-transformers\n",
            "\n",
            "Name: datasets\n",
            "Version: 3.5.0\n",
            "Summary: HuggingFace community-driven open-source library of datasets\n",
            "Home-page: https://github.com/huggingface/datasets\n",
            "Author: HuggingFace Inc.\n",
            "Author-email: thomas@huggingface.co\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: aiohttp, dill, filelock, fsspec, huggingface-hub, multiprocess, numpy, packaging, pandas, pyarrow, pyyaml, requests, tqdm, xxhash\n",
            "Required-by: \n",
            "\n",
            "Name: torch\n",
            "Version: 2.6.0+cu124\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, nvidia-cublas-cu12, nvidia-cuda-cupti-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-runtime-cu12, nvidia-cudnn-cu12, nvidia-cufft-cu12, nvidia-curand-cu12, nvidia-cusolver-cu12, nvidia-cusparse-cu12, nvidia-cusparselt-cu12, nvidia-nccl-cu12, nvidia-nvjitlink-cu12, nvidia-nvtx-cu12, sympy, triton, typing-extensions\n",
            "Required-by: accelerate, fastai, peft, sentence-transformers, timm, torchaudio, torchvision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING environment variable\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "batch_size = 16\n",
        "sequence_length = 128\n",
        "embedding_dimension = 256\n",
        "num_heads = 8\n",
        "feed_forward_dimension = 1024\n",
        "num_encoder_layers = 6\n",
        "dropout_probability = 0.1\n",
        "learning_rate = 1e-4\n",
        "num_epochs = 1\n",
        "tokenizer_name = 'gpt2'\n",
        "pad_token = '<pad>'\n",
        "\n",
        "# --- 1. Load Tokenizer and Add Pad Token ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': pad_token})\n",
        "pad_token_id = tokenizer.pad_token_id\n"
      ],
      "metadata": {
        "id": "OdNWWVHr1g_J"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Preprocess Flight Plan Data ---\n",
        "flight_plan_dataset = load_dataset(\"frankmorales2020/flight_plan_waypoints\")"
      ],
      "metadata": {
        "id": "AspLA76Xz4m5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flight_plan_dataset"
      ],
      "metadata": {
        "id": "VdSDwimRvgYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "def preprocess_flight_plan(example):\n",
        "    # Convert waypoints to string, handling potential non-string/numeric data and empty waypoints\n",
        "    waypoints_str = ' '.join([' '.join([str(float(coord)) if coord is not None else '' for coord in wp if coord is not None]) for wp in example['waypoints'] if wp])\n",
        "\n",
        "    # Tokenize and enforce fixed sequence length\n",
        "    tokens = tokenizer(\n",
        "        waypoints_str,\n",
        "        truncation=True,\n",
        "        max_length=sequence_length,\n",
        "        padding=\"max_length\",  # Pad to the specified sequence length\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    # Ensure consistent shape (important for batching) - squeezing after padding\n",
        "    tokens['input_ids'] = tokens['input_ids'].squeeze(0)\n",
        "    tokens['attention_mask'] = tokens['attention_mask'].squeeze(0)\n",
        "\n",
        "    # Handle extremely short sequences: If after tokenization the sequence is still shorter than sequence_length, we pad it.\n",
        "    # Modification: Pad to sequence_length if needed\n",
        "    pad_len = sequence_length - tokens['input_ids'].shape[0]\n",
        "    if pad_len > 0:\n",
        "        tokens['input_ids'] = torch.cat([tokens['input_ids'], torch.tensor([pad_token_id] * pad_len)])\n",
        "        tokens['attention_mask'] = torch.cat([tokens['attention_mask'], torch.tensor([0] * pad_len)])\n",
        "    elif pad_len < 0:\n",
        "        # Truncate if the sequence is longer than sequence_length\n",
        "        tokens['input_ids'] = tokens['input_ids'][:sequence_length]\n",
        "        tokens['attention_mask'] = tokens['attention_mask'][:sequence_length]\n",
        "\n",
        "    return tokens"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ysPkrrbN0O2u"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "source": [
        "def custom_collate_fn(batch):\n",
        "    # 1. Process 'input_ids' and 'attention_mask'\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_mask = [item['attention_mask'] for item in batch]\n",
        "\n",
        "    # Check for empty input_ids and skip those batches\n",
        "    if any(len(ids) == 0 for ids in input_ids):\n",
        "        print(\"Skipping batch with empty input_ids\")  # Add this line to identify skipped batches\n",
        "        return None  # Skip this batch\n",
        "\n",
        "    # Pad input_ids and attention_mask to the maximum length in the batch\n",
        "    # Ensure max_len is capped at block_size (sequence_length)\n",
        "    max_len = min(max(len(ids) for ids in input_ids), sequence_length) # Enforce maximum sequence length\n",
        "\n",
        "    # Convert ids and mask to tensors before concatenation\n",
        "    # Ensure input_ids are of type long\n",
        "    # MODIFICATION: Convert ids and mask to tensors before calling .type()\n",
        "    padded_input_ids = [torch.cat([torch.tensor(ids, dtype=torch.long), torch.tensor([pad_token_id] * (max_len - len(ids)), dtype=torch.long)]) for ids in input_ids]\n",
        "    padded_attention_mask = [torch.cat([torch.tensor(mask, dtype=torch.long), torch.tensor([0] * (max_len - len(mask)), dtype=torch.long)]) for mask in attention_mask]\n",
        "\n",
        "    # Stack the padded tensors\n",
        "    input_ids = torch.stack(padded_input_ids)\n",
        "    attention_mask = torch.stack(padded_attention_mask)\n",
        "\n",
        "\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask}"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "4WfkB4Z84Tnk"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader ---\n",
        "processed_dataset = flight_plan_dataset['train'].map(preprocess_flight_plan)\n",
        "train_loader = DataLoader(processed_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "id": "fJuBPtjB1NGg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "VjD7xmdUt0sd",
        "outputId": "21308623-1e37-4481-8049-33ba8afe7a47"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-b193a96e96ea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mblock_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_forward_dimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_encoder_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;31m# --- 5. Loss Function and Optimizer ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1327\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     )\n\u001b[0;32m-> 1329\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1330\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "# --- 4. Define the Transformer Model (Decoder-Only) ---\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert self.head_dim * num_heads == embed_dim, \"Embed dim must be divisible by num heads\"\n",
        "        self.query_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout_probability)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        B, T, E = x.size()\n",
        "        q = self.query_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = self.key_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = self.value_proj(x).view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        att = (q @ k.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
        "        if attention_mask is not None:\n",
        "            att = att.masked_fill(attention_mask[:, None, None, :] == 0, float('-inf'))\n",
        "        att = torch.softmax(att, dim=-1)\n",
        "        att = self.dropout(att)\n",
        "        y = (att @ v).transpose(1, 2).contiguous().view(B, T, E)\n",
        "        y = self.out_proj(y)\n",
        "        return y\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout_probability),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_hidden_dim):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.sa = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        self.ffwd = FeedForward(embed_dim, ff_hidden_dim)\n",
        "\n",
        "    def forward(self, x, attention_mask=None):\n",
        "        x = x + self.sa(self.ln1(x), attention_mask)\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, ff_hidden_dim, num_layers, block_size, dropout):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.position_embedding = nn.Embedding(block_size, embed_dim)\n",
        "        self.blocks = nn.Sequential(*[Block(embed_dim, num_heads, ff_hidden_dim) for _ in range(num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def forward(self, idx, attention_mask=None):\n",
        "        print(f\"idx shape: {idx.shape}, idx dtype: {idx.dtype}, idx device: {idx.device}\")  # Print idx info\n",
        "        # Check if idx has the expected number of dimensions\n",
        "        if idx.dim() != 2:\n",
        "            # Raise a more informative error message\n",
        "            raise ValueError(f\"Expected idx to have 2 dimensions, but got {idx.dim()} dimensions. Shape: {idx.shape}\")\n",
        "\n",
        "        B, T = idx.shape  # Change idx.size() to idx.shape\n",
        "        idx = idx.type(torch.long)  # Ensure idx is of type long\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "\n",
        "        print(f\"tok_emb shape: {tok_emb.shape}, tok_emb dtype: {tok_emb.dtype}, tok_emb device: {tok_emb.device}\")  # Print tok_emb info\n",
        "        # Ensure position embeddings are created on the same device and with dtype torch.long\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device, dtype=torch.long))\n",
        "        print(f\"pos_emb shape: {pos_emb.shape}, pos_emb dtype: {pos_emb.dtype}, pos_emb device: {pos_emb.device}\")  # Print pos_emb info\n",
        "\n",
        "        x = self.dropout(tok_emb + pos_emb)\n",
        "        print(f\"x shape after dropout: {x.shape}, x dtype: {x.dtype}, x device: {x.device}\")  # Print x info after dropout\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x, attention_mask)\n",
        "            print(f\"x shape after block: {x.shape}, x dtype: {x.dtype}, x device: {x.device}\")  # Print x info after each block\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        print(f\"x shape after ln_f: {x.shape}, x dtype: {x.dtype}, x device: {x.device}\")  # Print x info after ln_f\n",
        "        logits = self.head(x)\n",
        "        print(f\"logits shape: {logits.shape}, logits dtype: {logits.dtype}, logits device: {logits.device}\")  # Print logits info\n",
        "        return logits\n",
        "\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "# Instantiate the model\n",
        "vocab_size = tokenizer.vocab_size\n",
        "block_size = sequence_length\n",
        "model = SimpleTransformer(vocab_size, embedding_dimension, num_heads, feed_forward_dimension, num_encoder_layers, block_size, dropout_probability).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# --- 5. Loss Function and Optimizer ---\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# --- 6. Training Loop ---\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, batch in enumerate(dataloader):\n",
        "        # Skip batches with empty input_ids\n",
        "        if batch['input_ids'].shape[1] == 0:\n",
        "            continue\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        targets = input_ids[:, 1:].contiguous()\n",
        "        inputs = input_ids[:, :-1].contiguous()\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs, attention_mask)\n",
        "        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Batch [{batch_idx}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    print(\"Training finished!\")\n",
        "\n",
        "    # --- 7. (Optional) Inference/Generation ---\n",
        "    def generate_flight_plan(model, tokenizer, start_text=\"YUL \", max_new_tokens=100, device='cpu'):\n",
        "        model.eval()\n",
        "        start_tokens = tokenizer.encode(start_text, return_tensors='pt').to(device)\n",
        "        generated_tokens = model.generate(start_tokens, max_new_tokens=max_new_tokens)\n",
        "        return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
        "\n",
        "    if __name__ == \"__main__\":\n",
        "        if torch.cuda.is_available():\n",
        "            generated_plan = generate_flight_plan(model.to('cuda'), tokenizer, start_text=\"YUL \", max_new_tokens=200, device='cuda')\n",
        "            model.to('cpu')\n",
        "        else:\n",
        "            generated_plan = generate_flight_plan(model, tokenizer, start_text=\"YUL \", max_new_tokens=200, device='cpu')\n",
        "        print(\"\\nGenerated Flight Plan:\")\n",
        "        print(generated_plan)"
      ]
    }
  ]
}