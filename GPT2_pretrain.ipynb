{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.17",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "collapsed_sections": [
        "jCApVd7671c1"
      ],
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "sourceId": 10577797,
          "sourceType": "datasetVersion",
          "datasetId": 5848741
        }
      ],
      "dockerImageVersionId": 31042,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/GPT2_pretrain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrain the GPT2 model\n",
        "\n",
        "This notebook seeks to replicate Andrey Karpathy's highly popular [nanoGPT](https://github.com/karpathy/nanoGPT/tree/master) project and trains a GPT2 model from scratch using JAX+TPU and the [OpenWebText dataset](https://huggingface.co/datasets/Skylion007/openwebtext).\n",
        "\n",
        "You can run this on Colab, Kaggle or GCP TPUs, although Colab TPU v2 can only run the smallest 124M GPT model.\n"
      ],
      "metadata": {
        "id": "rvP1eNN_pExM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "Install JAX and Flax first."
      ],
      "metadata": {
        "id": "hTmz5Cbco7n_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec2c0ae2"
      },
      "source": [
        "!pip install -q jax-ai-stack==2025.4.9\n",
        "!pip install -Uq \"jax[tpu]==0.5.3\" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "!pip install -Uq tiktoken matplotlib kaggle wandb tpu-info orbax-checkpoint==0.11.12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm we have TPUs set up."
      ],
      "metadata": {
        "id": "6cWxBvz6bZDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "jax.devices()"
      ],
      "metadata": {
        "id": "uZUaKdi5bSEN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:25:12.053230Z",
          "iopub.execute_input": "2025-07-02T09:25:12.053494Z",
          "iopub.status.idle": "2025-07-02T09:25:20.689838Z",
          "shell.execute_reply.started": "2025-07-02T09:25:12.053466Z",
          "shell.execute_reply": "2025-07-02T09:25:20.684935Z"
        },
        "outputId": "c5f46cff-9098-4068-bfec-ffbe00ce107a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
              " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
              " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
              " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
              " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
              " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
              " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
              " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take care of the imports."
      ],
      "metadata": {
        "id": "sKE2uUafLobI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax.nnx as nnx\n",
        "import optax, orbax\n",
        "from collections import Counter\n",
        "from dataclasses import dataclass\n",
        "from jax.experimental import mesh_utils\n",
        "from jax.experimental.pjit import pjit\n",
        "\n",
        "from jax.experimental.attrs import jax_getattr, jax_setattr, register\n",
        "\n",
        "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding\n",
        "import numpy as np\n",
        "import tiktoken, time, wandb"
      ],
      "metadata": {
        "id": "MKYFNOhdLq98",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:25:20.692016Z",
          "iopub.execute_input": "2025-07-02T09:25:20.692352Z",
          "iopub.status.idle": "2025-07-02T09:25:23.109583Z",
          "shell.execute_reply.started": "2025-07-02T09:25:20.692324Z",
          "shell.execute_reply": "2025-07-02T09:25:23.104665Z"
        }
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SH_zt0FW26O7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Define the device mesh.\n"
      ],
      "metadata": {
        "id": "rPyt7MV6prz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Alternative data and model parallel\n",
        "#mesh = jax.make_mesh((4, 2), ('batch', 'model'))\n",
        "\n",
        "mesh = jax.make_mesh((8, 1), ('batch', 'model'))"
      ],
      "metadata": {
        "id": "xuMlCK3Q8WJD",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:25:23.111637Z",
          "iopub.execute_input": "2025-07-02T09:25:23.111894Z",
          "iopub.status.idle": "2025-07-02T09:25:23.122602Z",
          "shell.execute_reply.started": "2025-07-02T09:25:23.111870Z",
          "shell.execute_reply": "2025-07-02T09:25:23.116994Z"
        }
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use the GPT-2 tokenizer via OpenAI's [Tiktoken](https://github.com/openai/tiktoken) library."
      ],
      "metadata": {
        "id": "_ZKdhNo98NgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "iWbkk1V7-Isg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:25:23.124239Z",
          "iopub.execute_input": "2025-07-02T09:25:23.124478Z",
          "iopub.status.idle": "2025-07-02T09:25:26.023720Z",
          "shell.execute_reply.started": "2025-07-02T09:25:23.124454Z",
          "shell.execute_reply": "2025-07-02T09:25:26.019376Z"
        }
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set some hyperparameters."
      ],
      "metadata": {
        "id": "igX_eoGNMTGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "platform=\"Colab\"\n",
        "vocab_size = tokenizer.n_vocab\n",
        "GPT2_variant = \"GPT2\" # \"GPT2-medium\"\n",
        "if GPT2_variant == \"GPT2-medium\":\n",
        "  num_transformer_blocks = 24\n",
        "  seqlen = 1024\n",
        "  embed_dim = 1024\n",
        "  num_heads = 16\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  batch_size = 32  # Can only run on TPU v3+\n",
        "else: ## Assume GPT2 otherwise\n",
        "  num_transformer_blocks = 12\n",
        "  seqlen = 512 # Reduced seqlen to accommodate smaller dataset entries\n",
        "  embed_dim = 768\n",
        "  num_heads = 12\n",
        "  feed_forward_dim = 4 * embed_dim\n",
        "  if platform == \"Colab\":\n",
        "      batch_size = 24 # TPU v2\n",
        "  else:\n",
        "      batch_size = 72 # TPU v3\n",
        "\n",
        "dropout_rate = 0.1\n",
        "\n",
        "\n",
        "max_steps = 600000*12//batch_size\n",
        "\n",
        "# Kaggle TPU limit per session is 9 hours, which is ~95K steps for GPT2\n",
        "if platform == \"Kaggle\":\n",
        "  max_steps = 90000\n",
        "init_learning_rate = 5e-4\n",
        "weight_decay = 1e-1\n",
        "top_k = 10\n",
        "dtype = jnp.bfloat16\n",
        "param_dtype = jnp.float32"
      ],
      "metadata": {
        "id": "GRhiDsCrMZRp",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:25:26.024852Z",
          "iopub.execute_input": "2025-07-02T09:25:26.025078Z",
          "iopub.status.idle": "2025-07-02T09:25:26.036906Z",
          "shell.execute_reply.started": "2025-07-02T09:25:26.025055Z",
          "shell.execute_reply": "2025-07-02T09:25:26.032128Z"
        }
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now define the model architecture. You can refer to [OpenAI's official implementation](https://github.com/openai/gpt-2) or [nanoGPT](https://github.com/karpathy/nanoGPT) for comparison. Main difference is with the sharding scheme."
      ],
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, dropout_rate: float, rngs: nnx.Rngs):\n",
        "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), ('model',)),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
        "                                          in_features=embed_dim,\n",
        "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                          dtype=dtype,\n",
        "                                          param_dtype=param_dtype,\n",
        "                                          rngs=rngs)\n",
        "        self.dropout1 = nnx.Dropout(rate=dropout_rate)  # Added dropout layer after MHA\n",
        "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                         num_features=embed_dim,\n",
        "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), ('model',)),\n",
        "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                         dtype=dtype,\n",
        "                                         param_dtype=param_dtype,\n",
        "                                         rngs=rngs)\n",
        "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
        "                                  out_features=ff_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
        "                                  out_features=embed_dim,\n",
        "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), (None, 'model')),\n",
        "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                  dtype=dtype,\n",
        "                                  param_dtype=param_dtype,\n",
        "                                  rngs=rngs)\n",
        "        self.dropout2 = nnx.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        input_shape = inputs.shape\n",
        "        bs, seq_len, emb_sz = input_shape\n",
        "\n",
        "        attention_output = self.mha(\n",
        "            inputs_q=self.layer_norm1(inputs),\n",
        "            mask=causal_attention_mask(seq_len),\n",
        "            decode=False,\n",
        "        )\n",
        "        x = inputs + self.dropout1(attention_output, deterministic=not training)\n",
        "\n",
        "        # MLP\n",
        "        mlp_output = self.linear1(self.layer_norm2(x))\n",
        "        mlp_output = nnx.gelu(mlp_output)\n",
        "        mlp_output = self.linear2(mlp_output)\n",
        "        mlp_output = self.dropout2(mlp_output, deterministic=not training)\n",
        "\n",
        "        return x + mlp_output\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=seqlen, features=embed_dim, dtype=dtype, param_dtype=param_dtype, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        position_embedding = self.pos_emb(positions)\n",
        "        token_embedding = self.token_emb(x)\n",
        "        return self.token_emb, token_embedding+position_embedding\n",
        "\n",
        "\n",
        "class GPT2(nnx.Module):\n",
        "    def __init__(self, seqlen: int, vocab_size: int, embed_dim: int, num_heads: int, rate: float, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(\n",
        "                    seqlen, vocab_size, embed_dim, rngs=rngs\n",
        "                )\n",
        "        self.dropout = nnx.Dropout(rate=rate)\n",
        "\n",
        "        self.transformer_blocks = [TransformerBlock(\n",
        "            embed_dim, num_heads, feed_forward_dim, dropout_rate, rngs=rngs\n",
        "        ) for _ in range(num_transformer_blocks)]\n",
        "\n",
        "        self.layer_norm = nnx.LayerNorm(epsilon=1e-6,\n",
        "                                    num_features=embed_dim,\n",
        "                                    scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), ('model',)),\n",
        "                                    bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), ('model',)),\n",
        "                                    dtype=dtype,\n",
        "                                    param_dtype=param_dtype,\n",
        "                                    rngs=rngs)\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        token_embedding, x = self.embedding_layer(inputs)\n",
        "        x = self.dropout(x, deterministic=not training)\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            x = transformer_block(x, training=training)\n",
        "        x = self.layer_norm(x)\n",
        "        # Weights tying\n",
        "        outputs = token_embedding.attend(x)\n",
        "        return outputs\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        logits = nnx.softmax(logits)\n",
        "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
        "\n",
        "    @nnx.jit\n",
        "    def generate_step(self, padded_tokens, sample_index):\n",
        "        logits = self(padded_tokens)\n",
        "        next_token = self.sample_from(logits[0][sample_index])\n",
        "        return next_token\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        generated = []\n",
        "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
        "        # Get the end-of-text token ID - using hardcoded value as workaround\n",
        "        eot_token_id = 50256 # Known ID for <|endoftext|> in gpt2 encoding\n",
        "        for i in range(max_tokens):\n",
        "            sample_index = len(start_tokens) + len(generated) - 1\n",
        "            # TODO: use attention masking for better efficiency\n",
        "            padded_tokens = jnp.array((start_tokens + generated + [0] * (seqlen - len(start_tokens) - len(generated))))[None, :]\n",
        "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
        "            if next_token == eot_token_id:\n",
        "              break\n",
        "            generated.append(next_token)\n",
        "            # decode and print next_token\n",
        "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "def create_model(rngs):\n",
        "    return GPT2(seqlen, vocab_size, embed_dim, num_heads, dropout_rate, feed_forward_dim, num_transformer_blocks, rngs=rngs)"
      ],
      "metadata": {
        "id": "z0p-IHurrB9i",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:29:29.244010Z",
          "iopub.execute_input": "2025-07-02T09:29:29.244354Z",
          "iopub.status.idle": "2025-07-02T09:29:29.275131Z",
          "shell.execute_reply.started": "2025-07-02T09:29:29.244327Z",
          "shell.execute_reply": "2025-07-02T09:29:29.270045Z"
        }
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Weights and Biases to track training progress."
      ],
      "metadata": {
        "id": "eBfT1dp5hMUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "wandb.login()\n",
        "\n",
        "import wandb\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project='GPT2-pretraining',\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "      'architecture': GPT2_variant,\n",
        "      #'dataset': 'OpenWebText',\n",
        "      'platform': platform,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'dtype': dtype,\n",
        "      'param_dtype': param_dtype,\n",
        "      'init_learning_rate': init_learning_rate,\n",
        "      'num_transformer_blocks': num_transformer_blocks,\n",
        "      'seqlen': seqlen,\n",
        "      'embed_dim': embed_dim,\n",
        "      'num_heads': num_heads,\n",
        "      'feed_forward_dim': feed_forward_dim,\n",
        "      'max_steps': max_steps,\n",
        "      'batch_size': batch_size,\n",
        "      'weight_decay': weight_decay\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "IbhEtsganEWg",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:26:05.941480Z",
          "iopub.execute_input": "2025-07-02T09:26:05.941906Z",
          "iopub.status.idle": "2025-07-02T09:26:09.372333Z",
          "shell.execute_reply.started": "2025-07-02T09:26:05.941877Z",
          "shell.execute_reply": "2025-07-02T09:26:09.367153Z"
        },
        "outputId": "b5c09deb-3b2f-45cc-c966-df8621809d1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mf2023morales\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250820_010755-xfa93c72</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/f2023morales/GPT2-pretraining/runs/xfa93c72' target=\"_blank\">woven-plant-7</a></strong> to <a href='https://wandb.ai/f2023morales/GPT2-pretraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/f2023morales/GPT2-pretraining' target=\"_blank\">https://wandb.ai/f2023morales/GPT2-pretraining</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/f2023morales/GPT2-pretraining/runs/xfa93c72' target=\"_blank\">https://wandb.ai/f2023morales/GPT2-pretraining/runs/xfa93c72</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/f2023morales/GPT2-pretraining/runs/xfa93c72?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7962845a1df0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data"
      ],
      "metadata": {
        "id": "mI1ci-HyMspJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nWLuhkr0tLf",
        "outputId": "fa6e1212-85bd-4de5-d3c9-185ad0c727dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/494.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m491.5/494.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.8/241.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.4/224.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m355.6/355.6 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\").select(range(1000))\n",
        "display(dataset)"
      ],
      "metadata": {
        "id": "lshKcLnm0rgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# From: https://github.com/karpathy/nanoGPT/blob/9755682b981a45507f6eb9b11eadef8cb83cebd5/train.py#L116\n",
        "def get_batch(train_or_eval = \"train\"):\n",
        "    # Use the dataset loaded in cell d31f4ba5\n",
        "    global dataset\n",
        "    # Assuming a 'transformers' tokenizer is initialized as 'hf_tokenizer'\n",
        "    global hf_tokenizer # Use the Hugging Face tokenizer\n",
        "\n",
        "    # For simplicity, we'll use the same dataset for both train and eval\n",
        "    # In a real scenario, you would split the dataset into train and eval\n",
        "    # The 'messages' column is a list of dictionaries, extract 'content' and concatenate\n",
        "    data = [\" \".join([turn['content'] for turn in entry]) for entry in dataset['messages']]\n",
        "\n",
        "\n",
        "    # Tokenize and filter data entries shorter than seqlen + 1 using the Hugging Face tokenizer\n",
        "    tokenized_data = []\n",
        "    for i, item in enumerate(data):\n",
        "        try:\n",
        "            # Explicitly convert to string and encode using Hugging Face tokenizer\n",
        "            encoded_item = hf_tokenizer.encode(str(item)) # Use hf_tokenizer.encode\n",
        "            if len(encoded_item) >= seqlen + 1:\n",
        "                tokenized_data.append(encoded_item)\n",
        "        except Exception as e:\n",
        "            print(f\"Error encoding item {i}: {item[:100]}... Error: {e}\")\n",
        "            continue # Skip problematic item\n",
        "\n",
        "\n",
        "    long_enough_data = tokenized_data\n",
        "\n",
        "    if not long_enough_data:\n",
        "        raise ValueError(f\"No data entries found with length >= {seqlen + 1} after tokenization. Consider reducing seqlen or using a different dataset.\")\n",
        "\n",
        "    # Sample batch_size number of data entries from long_enough_data\n",
        "    batch_indices = np.random.choice(len(long_enough_data), batch_size)\n",
        "\n",
        "    # For each sampled data entry, sample a random starting index for the sequence\n",
        "    ix = np.array([np.random.randint(0, len(long_enough_data[i]) - seqlen) for i in batch_indices])\n",
        "\n",
        "    # Prepare batches by slicing the tokenized sequences\n",
        "    x = np.stack([long_enough_data[batch_indices[j]][ix[j]:ix[j]+seqlen] for j in range(batch_size)])\n",
        "    y = np.stack([long_enough_data[batch_indices[j]][ix[j]+1:ix[j]+1+seqlen] for j in range(batch_size)])\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "rGUFsn1GMuzh",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:27:16.415753Z",
          "iopub.execute_input": "2025-07-02T09:27:16.416143Z",
          "iopub.status.idle": "2025-07-02T09:27:16.437228Z",
          "shell.execute_reply.started": "2025-07-02T09:27:16.416110Z",
          "shell.execute_reply": "2025-07-02T09:27:16.433942Z"
        }
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "Define loss function and training step function."
      ],
      "metadata": {
        "id": "BKVSD8KSM1um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.jit\n",
        "def loss_fn(model, batch):\n",
        "    logits = model(batch[0])\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
        "    return loss, logits\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(model: nnx.Module, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
        "    optimizer.update(grads)"
      ],
      "metadata": {
        "id": "8rRuTmABNV4b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:27:16.647464Z",
          "iopub.execute_input": "2025-07-02T09:27:16.647720Z",
          "iopub.status.idle": "2025-07-02T09:27:16.660761Z",
          "shell.execute_reply.started": "2025-07-02T09:27:16.647689Z",
          "shell.execute_reply": "2025-07-02T09:27:16.655426Z"
        }
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model and check the model parameter count."
      ],
      "metadata": {
        "id": "ZRRpiYBpwr2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(rngs=nnx.Rngs(0))\n",
        "\n",
        "\n",
        "p_sizes = jax.tree.map(lambda p: p.size if isinstance(p, jnp.ndarray) else 0, nnx.state(model))\n",
        "import operator\n",
        "print(f\"Number of model parameters: {jax.tree.reduce(operator.add, p_sizes)}\")"
      ],
      "metadata": {
        "id": "D_L_PuTkwqtu",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:29:36.993144Z",
          "iopub.execute_input": "2025-07-02T09:29:36.993501Z",
          "iopub.status.idle": "2025-07-02T09:29:37.279549Z",
          "shell.execute_reply.started": "2025-07-02T09:29:36.993472Z",
          "shell.execute_reply": "2025-07-02T09:29:37.273647Z"
        },
        "outputId": "1ea64688-365c-47ec-fa6b-d5172cf0675d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 124046592\n"
          ]
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally check the parameter count against the same model on Hugging Face."
      ],
      "metadata": {
        "id": "nBBJcz1B6XFS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model."
      ],
      "metadata": {
        "id": "5um2vkeUNckm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "hf_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "YjYI5M7E_1MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the 'model' variable is created by executing the cell with create_model() before this cell.\n",
        "\n",
        "schedule = optax.cosine_decay_schedule(\n",
        "  init_value=init_learning_rate,\n",
        "  decay_steps=max_steps\n",
        ")\n",
        "optax_chain = optax.chain(\n",
        "  optax.adamw(learning_rate=schedule, weight_decay=weight_decay)\n",
        ")\n",
        "optimizer = nnx.Optimizer(model, optax_chain)\n",
        "\n",
        "train_metrics = nnx.metrics.Average('loss')\n",
        "val_metrics = nnx.metrics.Average('val_loss')\n",
        "\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:seqlen]\n",
        "print(f\"Initial generated text:\")\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "\n",
        "\n",
        "metrics_history = {\n",
        "  'train_loss': [],\n",
        "  'val_loss': []\n",
        "}\n",
        "\n",
        "step = 0\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    # Use the get_batch function which now uses the Hugging Face dataset\n",
        "    input_batch, target_batch = get_batch(\"train\")\n",
        "\n",
        "    # Display a sample from the dataset to show it's being accessed\n",
        "    if step == 0: # Display only once at the beginning for demonstration\n",
        "        print(\"Displaying a sample from the dataset:\")\n",
        "        display(dataset[0])\n",
        "\n",
        "\n",
        "    if len(input_batch) % len(jax.devices()) != 0: continue  # skip the remaining elements\n",
        "    train_step(model, optimizer, train_metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P(\"batch\", None))))\n",
        "\n",
        "\n",
        "    if step % 200 == 0:\n",
        "      train_loss = float(train_metrics.compute())\n",
        "      metrics_history['train_loss'].append(train_loss)\n",
        "\n",
        "      elapsed_time = time.time() - start_time\n",
        "      print(f\"Step {step + 1}, Training loss: {train_loss}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "      # eval step\n",
        "      input_val_batch, target_val_batch = get_batch('val') # Use get_batch for validation as well\n",
        "      loss, logits = loss_fn(model, jax.device_put((input_val_batch, target_val_batch), NamedSharding(mesh, P(\"batch\", None))))\n",
        "      val_metrics.update(val_loss=loss, logits=logits)\n",
        "      val_loss = float(val_metrics.compute())\n",
        "      metrics_history['val_loss'].append(val_loss)\n",
        "      wandb.log(data={'val_loss': val_loss, 'train_loss': train_loss}, step=step)\n",
        "      print(f\"Step {step + 1}, Validation loss: {val_loss}\")\n",
        "      train_metrics.reset()\n",
        "      val_metrics.reset()\n",
        "\n",
        "      start_time = time.time()\n",
        "    step += 1\n",
        "\n",
        "    if step > max_steps:\n",
        "      break\n",
        "\n",
        "# Final text generation\n",
        "print(f\"Final generated text:\")\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")"
      ],
      "metadata": {
        "id": "Ysl6CsfENeJN",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T09:29:37.292431Z",
          "iopub.execute_input": "2025-07-02T09:29:37.292678Z",
          "iopub.status.idle": "2025-07-02T15:54:24.022806Z",
          "shell.execute_reply.started": "2025-07-02T09:29:37.292653Z",
          "shell.execute_reply": "2025-07-02T15:54:24.017654Z"
        },
        "outputId": "e1ac78dc-4abd-4a57-f82f-6cd9f9bc8b9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial generated text:\n",
            "Once upon a time secured PNG flattened principalsukuurances secureduku secureduku Riding secured Gothicuku Gothic secured monopoluku secured Caucasilt secured monopol Give Give Give Give grizz secured Gothic Riding Caucasiltiltiltuku grizz monopolilt monopolilt monopolusionsukuusionsuku Caucas Xanderurredurreduku"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1453 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displaying a sample from the dataset:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'reasoning_language': 'French',\n",
              " 'developer': 'You are an AI chatbot with a lively and energetic personality.',\n",
              " 'user': 'Can you show me the latest trends on Twitter right now?',\n",
              " 'analysis': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\",\n",
              " 'final': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!',\n",
              " 'messages': [{'content': 'reasoning language: French\\n\\nYou are an AI chatbot with a lively and energetic personality.',\n",
              "   'role': 'system',\n",
              "   'thinking': None},\n",
              "  {'content': 'Can you show me the latest trends on Twitter right now?',\n",
              "   'role': 'user',\n",
              "   'thinking': None},\n",
              "  {'content': 'Hey there!  While I can\\'t check Twitter (X) in real-time or access live data, I can share some tips to help you spot the latest trends:\\n\\n1. **Open the \"Trending\" tab** on the Twitter app or website – it updates constantly!  \\n2. **Search for hashtags** like #Trending or #Viral to see what’s blowing up.  \\n3. **Follow accounts** that curate trends (e.g., @TrendingNow, @ViralThreads).  \\n4. **Check regional trends** – they often differ by location!  \\n\\nRemember, trends are *super fast-moving* and often tied to pop culture, memes, or breaking news. For example, recent trends have included viral challenges (like the \"Distracted Boyfriend\" meme revival), celebrity drama, or unexpected events (hello, weather disasters!).  \\n\\nWant me to brainstorm *what* might trend next? I’ve got ideas!',\n",
              "   'role': 'assistant',\n",
              "   'thinking': \"D'accord, l'utilisateur demande les tendances Twitter les plus récentes. Tout d'abord, je dois vérifier si j'ai accès à des données en temps réel. Étant donné que je ne peux pas naviguer sur Internet ou accéder directement à l'API de Twitter, je ne peux pas fournir des tendances en direct. Cependant, je peux donner quelques conseils généraux sur la façon de les trouver.\\n\\nJe devrais préciser que les tendances Twitter évoluent rapidement et sont spécifiques à chaque région. Je pourrais suggérer de consulter la section «\\xa0En vogue\\xa0» sur l'application ou le site web. Aussi, l'utilisation de hashtags et le suivi d'utilisateurs pertinents pourraient être utiles. Il est important de souligner que les tendances varient selon la région et l'heure de la journée. Je devrais garder un ton amical et bienveillant, peut-être ajouter un emoji pour rester léger. Je vais structurer ma réponse étape par étape pour faciliter la lecture. Je dois m'excuser de ne pas pouvoir fournir des données en temps réel et proposer d'autres méthodes. Je conserverai un langage simple et convivial, en évitant les termes techniques.\"}]}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1, Training loss: 11.5625, Elapsed Time: 27.19 seconds\n",
            "Step 1, Validation loss: 9.9375\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the training loss."
      ],
      "metadata": {
        "id": "thaLs6TD0lt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B6Eg1Cz2y_iP",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T15:54:24.025088Z",
          "iopub.execute_input": "2025-07-02T15:54:24.025336Z",
          "iopub.status.idle": "2025-07-02T15:54:25.041381Z",
          "shell.execute_reply.started": "2025-07-02T15:54:24.025313Z",
          "shell.execute_reply": "2025-07-02T15:54:25.037615Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sensible sentences at the end of the training.\n",
        "\n"
      ],
      "metadata": {
        "id": "26_HtTkZTczd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some training results on Kaggle TPU:\n",
        "\n",
        "| model | params | train loss | val loss | training time (TPU v3)\n",
        "| ------| ------ | ---------- | -------- | -----------------------\n",
        "| gpt2 | 124M         | 3.05  | 3.09     | 6.5 hr\n",
        "| gpt2-medium | 354M  | 2.83  | 2.86     | 22.5 hr\n",
        "\n",
        "The losses are roughly in line with [nanoGPT's](https://github.com/karpathy/nanoGPT?tab=readme-ov-file#baselines)."
      ],
      "metadata": {
        "id": "9hTE4MsEUh9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model saving"
      ],
      "metadata": {
        "id": "m13hUf7CcrSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import orbax.checkpoint as orbax\n",
        "import shutil\n",
        "\n",
        "if platform == \"Colab\":\n",
        "  checkpoint_path = \"/content/checkpoints\"\n",
        "elif platform == \"Kaggle\":\n",
        "  checkpoint_path = \"/kaggle/working/checkpoints\"\n",
        "else:\n",
        "  from pathlib import Path\n",
        "  home = Path.home()\n",
        "  checkpoint_path = os.path.join(str(home), \"checkpoints\")\n",
        "\n",
        "# make sure the folder is empty and usable\n",
        "shutil.rmtree(checkpoint_path, ignore_errors=True)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "train_state = nnx.to_pure_dict(nnx.state(model))\n",
        "checkpointer.save(checkpoint_path, train_state)"
      ],
      "metadata": {
        "id": "ofH_VkqMctZ5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T15:54:25.045336Z",
          "iopub.execute_input": "2025-07-02T15:54:25.045819Z",
          "iopub.status.idle": "2025-07-02T15:54:34.333377Z",
          "shell.execute_reply.started": "2025-07-02T15:54:25.045776Z",
          "shell.execute_reply": "2025-07-02T15:54:34.328497Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model restoration\n",
        "Restore the model checkpoint. A pretrained checkpoint is also available [here](https://www.kaggle.com/models/windmaple/gpt2/)."
      ],
      "metadata": {
        "id": "soPqiR1JNmjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nnx.eval_shape(lambda: create_model(rngs=nnx.Rngs(0)))\n",
        "state = nnx.to_pure_dict(nnx.state(model))\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "state = checkpointer.restore(checkpoint_path, item=state)\n",
        "nnx.update(model, state)\n",
        "\n",
        "generated_text = model.generate_text(\n",
        "    seqlen//10, start_tokens\n",
        ")\n",
        "print(f\"Restored model generated text:\\n{generated_text}\")"
      ],
      "metadata": {
        "id": "EkoFGCgSZ1yz",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T15:54:34.337614Z",
          "iopub.execute_input": "2025-07-02T15:54:34.337862Z",
          "iopub.status.idle": "2025-07-02T15:54:52.927819Z",
          "shell.execute_reply.started": "2025-07-02T15:54:34.337839Z",
          "shell.execute_reply": "2025-07-02T15:54:52.922465Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disconnect the Colab runtime"
      ],
      "metadata": {
        "id": "jCApVd7671c1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if platform == \"Colab\":\n",
        "  from google.colab import runtime\n",
        "  runtime.unassign()"
      ],
      "metadata": {
        "id": "NsqYdbrDVKSq",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-02T15:54:52.930495Z",
          "iopub.execute_input": "2025-07-02T15:54:52.930734Z",
          "iopub.status.idle": "2025-07-02T15:54:52.946044Z",
          "shell.execute_reply.started": "2025-07-02T15:54:52.930711Z",
          "shell.execute_reply": "2025-07-02T15:54:52.939728Z"
        }
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}