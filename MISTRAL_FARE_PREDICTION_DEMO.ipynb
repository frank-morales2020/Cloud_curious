{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Odoc8G2eYZjD",
        "gX9Z6UBfYEo9",
        "yHg6scgwaJmb",
        "BK_ZgUBnXBfD",
        "Q3_lNqMsXKsG"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNqXttq4kEHzF5zA0ZsWYw7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/MISTRAL_FARE_PREDICTION_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/datasets/giovamata/airlinedelaycauses"
      ],
      "metadata": {
        "id": "-jJPoKgSllLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/datasets/bhavikjikadara/us-airline-flight-routes-and-fares-1993-2024"
      ],
      "metadata": {
        "id": "b3HbIVoo1sxy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MISTRAL"
      ],
      "metadata": {
        "id": "bNA59VPg7vk_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LIBRARIES"
      ],
      "metadata": {
        "id": "Odoc8G2eYZjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 OR L4 IN GOOGLE COLAB\n",
        "#!pip install -U transformers\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install datasets trl ninja packaging --quiet\n",
        "\n",
        "# Uncomment only if you're using A100 GPU\n",
        "#!pip install flash-attn --no-build-isolation\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n"
      ],
      "metadata": {
        "id": "ZstDfLLH9YyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "y82GZxDX8wgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FEATURE ENGINEERING  "
      ],
      "metadata": {
        "id": "gX9Z6UBfYEo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "\n",
        "# Data Loading and Preprocessing\n",
        "db_name=\"bhavikjikadara/us-airline-flight-routes-and-fares-1993-2024\"\n",
        "dataset_path = kagglehub.dataset_download(db_name)\n",
        "files = os.listdir(dataset_path)\n",
        "csv_file_path = next((os.path.join(dataset_path, f) for f in files if f.endswith('.csv')), None)\n",
        "\n",
        "if csv_file_path:\n",
        "    flights_df = pd.read_csv(csv_file_path)\n",
        "    # Data Cleaning\n",
        "    flights_df.dropna(subset=['Geocoded_City1', 'Geocoded_City2', 'city1', 'city2', 'carrier_lg', 'Year', 'quarter', 'nsmiles', 'passengers', 'fare'], inplace=True)\n",
        "    flights_df['Geocoded_City1'] = flights_df['Geocoded_City1'].astype(str)\n",
        "    flights_df['Geocoded_City2'] = flights_df['Geocoded_City2'].astype(str)\n",
        "    flights_df['city1'] = flights_df['city1'].astype(str)\n",
        "    flights_df['city2'] = flights_df['city2'].astype(str)\n",
        "    flights_df['carrier_lg'] = flights_df['carrier_lg'].astype(str)\n",
        "    flights_df['Year'] = flights_df['Year'].astype(int)\n",
        "    flights_df['quarter'] = flights_df['quarter'].astype(int)\n",
        "    flights_df['nsmiles'] = flights_df['nsmiles'].astype(float)\n",
        "    flights_df['passengers'] = flights_df['passengers'].astype(int)\n",
        "    flights_df['fare'] = flights_df['fare'].astype(float)\n",
        "\n",
        "else:\n",
        "    raise FileNotFoundError(\"No CSV file found in the dataset directory.\")\n",
        "\n",
        "# Textual Representation of Flight Data\n",
        "def flight_to_text(row):\n",
        "    text = f\"Flight from {row['city1']} to {row['city2']} on {row['carrier_lg']} in {row['Year']}, quarter {row['quarter']}. Distance: {row['nsmiles']} miles. Passengers: {row['passengers']}.\"\n",
        "    return text\n",
        "\n",
        "flights_df['text'] = flights_df.apply(flight_to_text, axis=1)\n",
        "\n",
        "# Dataset Creation and Splitting with Sample Size Control\n",
        "sample_size = 10000  # Set your desired sample size\n",
        "\n",
        "train_texts, test_texts, train_fares, test_fares = train_test_split(\n",
        "    flights_df['text'].tolist(),\n",
        "    flights_df['fare'].tolist(),\n",
        "    train_size=sample_size,  # Use train_size to specify the number of training samples\n",
        "    test_size=sample_size,    # Use test_size to specify the number of testing samples\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "class FlightDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor([self.labels[idx]], dtype=torch.float32).unsqueeze(0)  # Reshape to (1, 1)\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Before tokenizing, set padding_side to 'left':\n",
        "tokenizer.padding_side = 'left'  # Set padding to the left\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "\n",
        "train_dataset = FlightDataset(train_encodings, train_fares)\n",
        "test_dataset = FlightDataset(test_encodings, test_fares)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3M_13tJ-_Yn",
        "outputId": "dcaac219-9a55-4d00-b08c-96a249b1adcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-1b7b95b76d8e>:20: DtypeWarning: Columns (20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  flights_df = pd.read_csv(csv_file_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINE TUNING"
      ],
      "metadata": {
        "id": "yHg6scgwaJmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import  LoraConfig, TaskType, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "\n",
        "# Fine-tuning LLM for Fare Prediction\n",
        "\n",
        "# Move the base model to the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 6. Fine-tuning DeepSeek for Fare Prediction with PEFT (LoRA)\n",
        "# Define LoRA configuration\n",
        "\n",
        "target_modules=[\"model.layers.*.self_attn.qkv\"]\n",
        "target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "# Wrap the model with PEFT\n",
        "base_model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "# Print trainable parameters for verification\n",
        "base_model.print_trainable_parameters()\n",
        "\n",
        "output_dir=\"/content/gdrive/MyDrive/model/Mistral-7B-fareprediction-attention-2\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    #use_cache=False,\n",
        "    per_device_train_batch_size=3,\n",
        "    gradient_accumulation_steps=2,\n",
        "    warmup_steps=10,\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    num_train_epochs=1,\n",
        "    max_steps=100,\n",
        "    learning_rate=5e-6,\n",
        "    logging_steps=10,\n",
        "    bf16=True,\n",
        "    tf32=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    weight_decay=0.2,\n",
        "    eval_steps=10,  # Assuming you want eval_steps to be 10\n",
        "    report_to=\"none\",\n",
        "    save_steps=20,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    max_grad_norm=1.0,\n",
        "    logging_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,  # For minimizing eval_loss (accuracy is maximized by default)\n",
        "\n",
        "    #no_cuda=False if torch.cuda.is_available() else True # Add this line if needed for CPU training\n",
        "\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "    #compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.model.to(training_args.device)  # Move to device specified in training_args\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "xeufDQ-z_lpj",
        "outputId": "dfbeeb06-dba3-4bef-a6cb-1468aba9ce9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 20,971,520 || all params: 7,262,707,713 || trainable%: 0.2888\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [100/100 1:02:01, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.864200</td>\n",
              "      <td>2.632018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.378600</td>\n",
              "      <td>2.060677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.775300</td>\n",
              "      <td>1.545895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.430200</td>\n",
              "      <td>1.293041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.237500</td>\n",
              "      <td>1.170424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.170400</td>\n",
              "      <td>1.130794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.133000</td>\n",
              "      <td>1.107469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.103200</td>\n",
              "      <td>1.089792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.068500</td>\n",
              "      <td>1.071693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.064600</td>\n",
              "      <td>1.058095</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=100, training_loss=1.522563991546631, metrics={'train_runtime': 3722.0377, 'train_samples_per_second': 0.161, 'train_steps_per_second': 0.027, 'total_flos': 1540432429056000.0, 'train_loss': 1.522563991546631, 'epoch': 0.059988002399520096})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EVALUATION"
      ],
      "metadata": {
        "id": "BK_ZgUBnXBfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "# 7. Evaluate the Fine-tuned DeepSeek Model for Fare Prediction\n",
        "\n",
        "# Load the PEFT weights and apply them to the base model\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model, '/content/gdrive/MyDrive/model/Mistral-7B-fareprediction-attention-2/checkpoint-100')\n",
        "\n",
        "# Add the regression head to the loaded model\n",
        "hidden_size = fine_tuned_model.config.hidden_size  # Get hidden size from config\n",
        "fine_tuned_model.base_model.model.regression_head = nn.Linear(hidden_size, 1)  # Add the linear layer for regression\n",
        "\n",
        "\n",
        "def predict_fare(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "\n",
        "    # Move inputs and model to the same device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = inputs.to(device)\n",
        "    fine_tuned_model.to(device)  # Move the model to the device\n",
        "\n",
        "    # Use the fine-tuned model for prediction:\n",
        "    outputs = fine_tuned_model(**inputs, output_hidden_states=True) # Added output_hidden_states=True\n",
        "    # Access hidden states and get the last one\n",
        "    hidden_states = outputs.hidden_states\n",
        "    last_hidden_state = hidden_states[-1][:, -1, :] # Get last hidden state\n",
        "\n",
        "    # Cast last_hidden_state to float32\n",
        "    last_hidden_state = last_hidden_state.type(torch.float32)\n",
        "\n",
        "    predicted_fare = fine_tuned_model.regression_head(last_hidden_state).item()\n",
        "\n",
        "    return predicted_fare\n",
        "\n",
        "y_pred = [predict_fare(text) for text in test_texts]\n",
        "mse = mean_squared_error(test_fares, y_pred)\n",
        "r2 = r2_score(test_fares, y_pred)\n",
        "print(f'Mean Squared Error: {mse}')\n",
        "print(f'R-squared: {r2}')"
      ],
      "metadata": {
        "id": "IpoGoi6eErTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GEMINI ANALYSIS"
      ],
      "metadata": {
        "id": "Q3_lNqMsXKsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-generativeai -q\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GEMINI')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model_name = 'gemini-1.5-pro'\n",
        "model = genai.GenerativeModel(model_name)"
      ],
      "metadata": {
        "id": "MKFfqpmYHRiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_gemini_feedback_prompt(observations, feedback):\n",
        "    \"\"\"Generates a customized prompt for Gemini feedback without placeholders.\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "**Task:** Flight Fare Prediction\n",
        "\n",
        "**Model:** {model_id}\n",
        "\n",
        "**Dataset:** {db_name}\n",
        "\n",
        "**Sample Size:** {sample_size} (training and testing)\n",
        "\n",
        "**Target of Prediction:** Flight Fare\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "- R2: {r2}\n",
        "- MSE: {mse}\n",
        "\n",
        "**Observations:**\n",
        "{observations}\n",
        "\n",
        "**Feedback:**\n",
        "{feedback}\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "# ... (rest of your existing code for submitting to Gemini and printing analysis) ..."
      ],
      "metadata": {
        "id": "yAFlidJNFksn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating and submitting the prompt to Gemini:\n",
        "observations_value = \"The model showed promising results... (your observations)\"\n",
        "feedback_value = \"It would be beneficial if Gemini... (your feedback)\"\n",
        "# Creating the prompt with evaluation details:\n",
        "prompt_with_eval_details = generate_gemini_feedback_prompt(observations_value, feedback_value)\n",
        "\n",
        "\n",
        "prompt_for_analysis = \"\"\"\n",
        "You are a helpful fare prediction expert.\n",
        "Please, make an additional analysis of this Fine-Tuning experiment report with metrics susch as MSE and R2.\n",
        "\"\"\"\n",
        "\n",
        "# Combining the prompts:\n",
        "final_prompt = prompt_with_eval_details + prompt_for_analysis\n",
        "\n",
        "\n",
        "model_name = \"gemini-1.5-pro\"  # Replace with desired model\n",
        "model = genai.GenerativeModel(model_name)\n",
        "response = model.generate_content(final_prompt)\n",
        "llm_analysis = response.text\n",
        "\n",
        "print(\"\\n\\n## LLM Analysis:\\n\")\n",
        "print(llm_analysis)"
      ],
      "metadata": {
        "id": "nB4-XI0UFnJg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}