{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "STR83WXTNL-I"
      ],
      "authorship_tag": "ABX9TyOcSTds3fOoVOyWCGSo+ANp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/FT_GEMINI_NASA_VERTEXAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env -q\n",
        "!pip install google-generativeai -q\n",
        "!pip install rouge-score -q\n"
      ],
      "metadata": {
        "id": "5VFKK5piHxhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESPsgxVdHMRh"
      },
      "outputs": [],
      "source": [
        "from vertexai.preview.tuning import sft\n",
        "import vertexai\n",
        "import os\n",
        "from google.colab import auth\n",
        "import colab_env\n",
        "import time\n",
        "\n",
        "# Project details (replace with your values if not using env vars)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")\n",
        "BUCKET_NAME = os.environ.get(\"GOOGLE_CLOUD_BUCKET_NAME\")\n",
        "STAGING_BUCKET = f\"gs://{BUCKET_NAME}/staging\"\n",
        "\n",
        "# Authentication and Initialization\n",
        "auth.authenticate_user()\n",
        "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
        "\n",
        "# Define your tuning parameters\n",
        "BASE_MODEL = \"gemini-2.0-flash-001\"  # Using Gemini 2.0 Flash\n",
        "\n",
        "TRAIN_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_train_text.jsonl\"  # Path to your training data in JSONL format\n",
        "VALIDATION_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_text.jsonl\"  # Path to your validation data in JSONL format\n",
        "TUNED_MODEL_DISPLAY_NAME = \"cmapss-text-tuned-gemini-2.0-flash-001\"\n",
        "EPOCHS = 10  # Adjust as needed\n",
        "LEARNING_RATE_MULTIPLIER = 1.0  # Adjust as needed\n",
        "\n",
        "\n",
        "\n",
        "# Start the fine-tuning job\n",
        "try:\n",
        "    sft_tuning_job = sft.train(\n",
        "        source_model=BASE_MODEL,\n",
        "        train_dataset=TRAIN_DATASET_URI,\n",
        "        validation_dataset=VALIDATION_DATASET_URI,\n",
        "        tuned_model_display_name=TUNED_MODEL_DISPLAY_NAME,\n",
        "        epochs=EPOCHS,\n",
        "        learning_rate_multiplier=LEARNING_RATE_MULTIPLIER,\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"Tuning job started: {sft_tuning_job.resource_name}\")\n",
        "\n",
        "    # Periodically check the job status until it's complete\n",
        "    while True:\n",
        "        job_status = sft_tuning_job.state  # Get the job's state directly\n",
        "\n",
        "        if job_status in (\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"):\n",
        "            break  # Exit the loop if the job is finished\n",
        "\n",
        "        print(f\"Job status: {job_status}, waiting...\")\n",
        "        time.sleep(60)  # Wait for 60 seconds before checking again\n",
        "\n",
        "    print(f\"Tuning job completed with status: {job_status}. Resource name: {sft_tuning_job.resource_name}\")\n",
        "\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    print(\"Please double-check the base model name and your Vertex AI setup.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tuning job completed with Resource name: {sft_tuning_job.resource_name}\")"
      ],
      "metadata": {
        "id": "vpkS41BjYYCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c91c13a-db94-4bb7-e459-110d27e9b335"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning job completed with Resource name: projects/677155171887/locations/us-central1/tuningJobs/5437787329584431104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.preview.tuning import sft\n",
        "import vertexai\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "from google.colab import auth\n",
        "from google.cloud import aiplatform\n",
        "from google.api_core import exceptions\n",
        "from google.cloud.aiplatform_v1.types import JobState\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
      ],
      "metadata": {
        "id": "ipF_vPEfUGmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "STR83WXTNL-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from rouge_score import rouge_scorer\n",
        "import json\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import vertexai\n",
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "from vertexai.preview import tuning\n",
        "from vertexai.preview.tuning import TuningJob\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Retrieve API key from userdata\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI')\n",
        "\n",
        "# Check if API key is retrieved successfully\n",
        "if GOOGLE_API_KEY is None:\n",
        "    raise ValueError(\"API key not found in userdata['GEMINI'].\")\n",
        "\n",
        "# Configure the client with the API key\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Initialize Vertex AI (if needed)\n",
        "PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\")  # Replace with your region if needed\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# Path to the evaluation dataset (JSONL format)\n",
        "EVAL_DATASET_URI = f\"gs://{BUCKET_NAME}/cmapss_FD004_test_text.jsonl\"  # Replace with your actual dataset path\n",
        "\n",
        "\n",
        "# Replace with the resource name you got after fine-tuning\n",
        "tuned_model_resource_name = 'projects/677155171887/locations/us-central1/tuningJobs/5437787329584431104'\n",
        "\n",
        "# Use TuningJob.get() to retrieve the tuning job\n",
        "tuning_job = TuningJob.get(tuned_model_resource_name)  # Access the tuning job\n",
        "\n",
        "\n",
        "# Get the best trained model from the tuning job\n",
        "tuned_model_name = tuning_job.best_trial.trained_model\n",
        "\n",
        "# Load the tuned model directly using TextGenerationModel.from_pretrained()\n",
        "tuned_model = TextGenerationModel.from_pretrained(tuned_model_name)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the tuned model using sft.TuningJob.get()\n",
        "# Use sft.TuningJob to get the tuning job details\n",
        "tuning_job = sft.TuningJob.get(tuned_model_resource_name)\n",
        "\n",
        "# Load the tuned model directly using TextGenerationModel.from_pretrained()\n",
        "tuned_model = TextGenerationModel.from_pretrained(tuned_model_resource_name)\n",
        "\n",
        "# Function to calculate ROUGE scores\n",
        "def calculate_rouge(reference, generated):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = scorer.score(reference, generated)\n",
        "    return scores\n",
        "\n",
        "# Evaluate the model\n",
        "rouge_scores = []\n",
        "with open(EVAL_DATASET_URI, 'r') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            data = json.loads(line)\n",
        "            prompt = data[\"prompt\"]\n",
        "            reference = data[\"completion\"]  # Expected completion\n",
        "\n",
        "            # Generate text using the fine-tuned model\n",
        "            response = tuned_model.predict(prompt=prompt)\n",
        "            generated_text = response.text\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            scores = calculate_rouge(reference, generated_text)\n",
        "            rouge_scores.append(scores)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing line: {line.strip()}, Error: {e}\")\n",
        "\n",
        "# Calculate average ROUGE scores\n",
        "avg_rouge1 = sum([scores['rouge1'].fmeasure for scores in rouge_scores]) / len(rouge_scores)\n",
        "avg_rouge2 = sum([scores['rouge2'].fmeasure for scores in rouge_scores]) / len(rouge_scores)\n",
        "avg_rougeL = sum([scores['rougeL'].fmeasure for scores in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "# Print results\n",
        "print(f\"Average ROUGE-1: {avg_rouge1:.4f}\")\n",
        "print(f\"Average ROUGE-2: {avg_rouge2:.4f}\")\n",
        "print(f\"Average ROUGE-L: {avg_rougeL:.4f}\")"
      ],
      "metadata": {
        "id": "ER9Xy0aDNI-K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}