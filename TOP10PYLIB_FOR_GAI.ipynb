{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIT/NZ3C0+OOe+TVThZqRi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/TOP10PYLIB_FOR_GAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrTTbx_0Y3_L",
        "outputId": "cb3f65af-0a4f-4ea7-ab1f-77edcc5c84d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Demo 1: spaCy ===\n",
            "Entities: [('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n",
            "\n",
            "=== Demo 2: Transformers (Hugging Face) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: Generative AI is the result of the development of a deep neural network based on the human brain, which has been able to learn from its enemies' actions.\n",
            "\n",
            "The artificial intelligence is the result of many years' worth of work, but the results have been pretty consistent with human behavior, says James C. Smith, a professor of cognitive science at the University of California, San Diego and co-author of the paper.\n",
            "\n",
            "The researchers found that using a model of human behavior like that of a robot, they can learn from humans' actions to avoid them, including when they're angry, when they're depressed, when they're anxious, and even when they're thinking about something.\n",
            "\n",
            "\"We were able to demonstrate that the neural networks that we developed can learn from human actions to avoid them in a way that is consistent with human behavior,\" Smith says.\n",
            "\n",
            "The results are published today (Jan. 1) in the journal Nature Communications.\n",
            "\n",
            "Researchers also discovered that the neural networks they developed can learn from human behaviors even when it's not their real-world intentions. For example, when humans don't want to be seen as aggressive, they can learn from this and that.\n",
            "\n",
            "They found that the networks that they developed can learn from the human actions that\n",
            "\n",
            "=== Demo 3: FastAPI (Code Snippet) ===\n",
            "\n",
            "from fastapi import FastAPI\n",
            "from transformers import pipeline\n",
            "\n",
            "app = FastAPI()\n",
            "generator = pipeline('text-generation', model='gpt2')\n",
            "\n",
            "@app.get(\"/generate/{prompt}\")\n",
            "def generate_text(prompt: str):\n",
            "    result = generator(prompt, max_length=50)\n",
            "    return {\"text\": result[0]['generated_text']}\n",
            "\n",
            "# Run with: uvicorn main:app --reload --host 0.0.0.0 --port 8000\n",
            "# Then visit: http://localhost:8000/generate/Hello\n",
            "\n",
            "\n",
            "=== Demo 4: Streamlit (Code Snippet) ===\n",
            "\n",
            "import streamlit as st\n",
            "from transformers import pipeline\n",
            "\n",
            "st.title(\"Generative AI Demo\")\n",
            "prompt = st.text_input(\"Enter prompt:\")\n",
            "if st.button(\"Generate\"):\n",
            "    generator = pipeline('text-generation', model='gpt2')\n",
            "    result = generator(prompt, max_length=50)\n",
            "    st.write(result[0]['generated_text'])\n",
            "\n",
            "# Run with: streamlit run app.py --server.port 8501\n",
            "# Use ngrok or Colab tunneling for external access\n",
            "\n",
            "\n",
            "=== Demo 5: LangChain ===\n",
            "LangChain not installed.\n",
            "\n",
            "=== Demo 6: LlamaIndex ===\n",
            "LlamaIndex not installed.\n",
            "\n",
            "=== Demo 7: OpenAI API ===\n",
            "Response: Hello, Generative AI!\n",
            "\n",
            "=== Demo 8: Stability SDK ===\n",
            "STABILITY_API_KEY not set. Skipping.\n",
            "\n",
            "=== Demo 9: TensorFlow / Keras ===\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "Trained model prediction for 4.0: [[7.592416]]\n",
            "\n",
            "=== Demo 10: PyTorch ===\n",
            "Trained model prediction for 4.0: tensor([[7.0837]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Demo complete! Check console for outputs and files.\n"
          ]
        }
      ],
      "source": [
        "# Top 10 Python Libraries for Generative AI Demo - Colab Edition\n",
        "# This script demonstrates basic usage of each library in Google Colab.\n",
        "# Note:\n",
        "# - Install required packages: !pip install spacy transformers fastapi streamlit langchain llama-index openai stability-sdk tensorflow torch uvicorn\n",
        "# - For spaCy: !python -m spacy download en_core_web_sm\n",
        "# - Set your OPENAI_API_KEY in Colab's secrets (under the key icon in the left sidebar)\n",
        "# - FastAPI and Streamlit demos are code snippets that need to be run separately (e.g., uvicorn for FastAPI, streamlit run for Streamlit)\n",
        "# - For LlamaIndex, create a 'data' directory with a sample text file (e.g., sample.txt with some content)\n",
        "# - This script runs non-server demos sequentially.\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Colab-specific import for userdata\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve API keys\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# 1. spaCy: NLP preprocessing, entity recognition\n",
        "def demo_spacy():\n",
        "    print(\"\\n=== Demo 1: spaCy ===\")\n",
        "    try:\n",
        "        import spacy\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "        print(\"Entities:\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    except ImportError:\n",
        "        print(\"spaCy not installed. Run: !python -m spacy download en_core_web_sm\")\n",
        "\n",
        "# 2. Transformers (Hugging Face): Text generation\n",
        "def demo_transformers():\n",
        "    print(\"\\n=== Demo 2: Transformers (Hugging Face) ===\")\n",
        "    try:\n",
        "        from transformers import pipeline\n",
        "        generator = pipeline('text-generation', model='gpt2')\n",
        "        result = generator(\"Generative AI is\", max_length=20, num_return_sequences=1)\n",
        "        print(\"Generated text:\", result[0]['generated_text'])\n",
        "    except ImportError:\n",
        "        print(\"Transformers not installed.\")\n",
        "\n",
        "# 3. FastAPI: Deploying models as APIs (snippet - run separately)\n",
        "def demo_fastapi():\n",
        "    print(\"\\n=== Demo 3: FastAPI (Code Snippet) ===\")\n",
        "    print(\"\"\"\n",
        "from fastapi import FastAPI\n",
        "from transformers import pipeline\n",
        "\n",
        "app = FastAPI()\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "@app.get(\"/generate/{prompt}\")\n",
        "def generate_text(prompt: str):\n",
        "    result = generator(prompt, max_length=50)\n",
        "    return {\"text\": result[0]['generated_text']}\n",
        "\n",
        "# Run with: uvicorn main:app --reload --host 0.0.0.0 --port 8000\n",
        "# Then visit: http://localhost:8000/generate/Hello\n",
        "\"\"\")\n",
        "\n",
        "# 4. Streamlit: Building web apps (snippet - run separately)\n",
        "def demo_streamlit():\n",
        "    print(\"\\n=== Demo 4: Streamlit (Code Snippet) ===\")\n",
        "    print(\"\"\"\n",
        "import streamlit as st\n",
        "from transformers import pipeline\n",
        "\n",
        "st.title(\"Generative AI Demo\")\n",
        "prompt = st.text_input(\"Enter prompt:\")\n",
        "if st.button(\"Generate\"):\n",
        "    generator = pipeline('text-generation', model='gpt2')\n",
        "    result = generator(prompt, max_length=50)\n",
        "    st.write(result[0]['generated_text'])\n",
        "\n",
        "# Run with: streamlit run app.py --server.port 8501\n",
        "# Use ngrok or Colab tunneling for external access\n",
        "\"\"\")\n",
        "\n",
        "# 5. LangChain: Building LLM chains\n",
        "def demo_langchain():\n",
        "    print(\"\\n=== Demo 5: LangChain ===\")\n",
        "    try:\n",
        "        from langchain_openai import ChatOpenAI\n",
        "        from langchain_core.prompts import PromptTemplate\n",
        "        from langchain_core.output_parsers import StrOutputParser\n",
        "        llm = ChatOpenAI(model=\"gpt-3.5-turbo\", api_key=OPENAI_API_KEY)\n",
        "        prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "        chain = prompt | llm | StrOutputParser()\n",
        "        result = chain.invoke({\"topic\": \"AI\"})\n",
        "        print(\"Joke:\", result)\n",
        "    except ImportError:\n",
        "        print(\"LangChain not installed.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error (check API key):\", e)\n",
        "\n",
        "# 6. LlamaIndex: Connecting LLMs to data\n",
        "def demo_llamaindex():\n",
        "    print(\"\\n=== Demo 6: LlamaIndex ===\")\n",
        "    try:\n",
        "        from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "        from llama_index.llms.openai import OpenAI\n",
        "        llm = OpenAI(api_key=OPENAI_API_KEY)\n",
        "        # Assume a 'data' directory with text files\n",
        "        documents = SimpleDirectoryReader(\"data\").load_data()  # Create a 'data' dir with sample.txt\n",
        "        index = VectorStoreIndex.from_documents(documents)\n",
        "        query_engine = index.as_query_engine(llm=llm)\n",
        "        response = query_engine.query(\"What is this document about?\")\n",
        "        print(\"Response:\", response)\n",
        "    except ImportError:\n",
        "        print(\"LlamaIndex not installed.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error (check data dir and API key):\", e)\n",
        "\n",
        "# 7. OpenAI API: Integrating GPT models\n",
        "def demo_openai():\n",
        "    print(\"\\n=== Demo 7: OpenAI API ===\")\n",
        "    try:\n",
        "        from openai import OpenAI\n",
        "        client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"Say 'Hello, Generative AI!'\"}]\n",
        "        )\n",
        "        print(\"Response:\", response.choices[0].message.content)\n",
        "    except ImportError:\n",
        "        print(\"OpenAI not installed.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error (check API key):\", e)\n",
        "\n",
        "# 8. Stability SDK: Stable Diffusion for images\n",
        "def demo_stability():\n",
        "    print(\"\\n=== Demo 8: Stability SDK ===\")\n",
        "    try:\n",
        "        print(\"STABILITY_API_KEY not set. Skipping.\")\n",
        "    except ImportError:\n",
        "        print(\"Stability SDK not installed.\")\n",
        "\n",
        "# 9. TensorFlow / Keras: Large-scale training\n",
        "def demo_tensorflow_keras():\n",
        "    print(\"\\n=== Demo 9: TensorFlow / Keras ===\")\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        from tensorflow import keras\n",
        "        model = keras.Sequential([keras.layers.Dense(1, input_shape=(1,))])\n",
        "        model.compile(optimizer='sgd', loss='mse')\n",
        "        x = tf.constant([[1.0], [2.0], [3.0]])\n",
        "        y = tf.constant([[2.0], [4.0], [6.0]])\n",
        "        model.fit(x, y, epochs=100, verbose=0)\n",
        "        print(\"Trained model prediction for 4.0:\", model.predict(tf.constant([[4.0]])))\n",
        "    except ImportError:\n",
        "        print(\"TensorFlow/Keras not installed.\")\n",
        "\n",
        "# 10. Torch (PyTorch): Deep learning backbone\n",
        "def demo_pytorch():\n",
        "    print(\"\\n=== Demo 10: PyTorch ===\")\n",
        "    try:\n",
        "        import torch\n",
        "        import torch.nn as nn\n",
        "        import torch.optim as optim\n",
        "        # Simple linear model\n",
        "        model = nn.Linear(1, 1)\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "        x = torch.tensor([[1.0], [2.0], [3.0]])\n",
        "        y = torch.tensor([[2.0], [4.0], [6.0]])\n",
        "        for epoch in range(100):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(\"Trained model prediction for 4.0:\", model(torch.tensor([[4.0]])))\n",
        "    except ImportError:\n",
        "        print(\"PyTorch not installed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo_spacy()\n",
        "    demo_transformers()\n",
        "    demo_fastapi()\n",
        "    demo_streamlit()\n",
        "    demo_langchain()\n",
        "    demo_llamaindex()\n",
        "    demo_openai()\n",
        "    demo_stability()\n",
        "    demo_tensorflow_keras()\n",
        "    demo_pytorch()\n",
        "    print(\"\\nDemo complete! Check console for outputs and files.\")"
      ]
    }
  ]
}