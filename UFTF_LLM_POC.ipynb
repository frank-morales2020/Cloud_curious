{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "S99Umzgf8OwN",
        "BRmauENT7tWs"
      ],
      "authorship_tag": "ABX9TyNjS8+OlNxoZ8RDOR9aXLaD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "22298f8c179048d5bf60d77fda195664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fb8539c64124114bc91bf02b958ea6a",
              "IPY_MODEL_6ba065e688394a1893f838c397be07cb",
              "IPY_MODEL_9d22a28717bd4721a708d874bb08535c"
            ],
            "layout": "IPY_MODEL_f5c20cf7a4da4a1891a0f0886bc7e3f9"
          }
        },
        "8fb8539c64124114bc91bf02b958ea6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe45f05902bb4c4580bb49e0f7b16c77",
            "placeholder": "​",
            "style": "IPY_MODEL_310bc21a79134fd0a0cc462189afe2bd",
            "value": "Map: 100%"
          }
        },
        "6ba065e688394a1893f838c397be07cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_add54b8148ad4b69b7f23de5483238cc",
            "max": 800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ca29517e9fe54d458beaa5442a894a59",
            "value": 800
          }
        },
        "9d22a28717bd4721a708d874bb08535c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4b0bf4500d248c2837d5581d54820cc",
            "placeholder": "​",
            "style": "IPY_MODEL_78a5ab9b8a0d4bc59115de4d67a83680",
            "value": " 800/800 [00:00&lt;00:00, 49836.52 examples/s]"
          }
        },
        "f5c20cf7a4da4a1891a0f0886bc7e3f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe45f05902bb4c4580bb49e0f7b16c77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "310bc21a79134fd0a0cc462189afe2bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "add54b8148ad4b69b7f23de5483238cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca29517e9fe54d458beaa5442a894a59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4b0bf4500d248c2837d5581d54820cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78a5ab9b8a0d4bc59115de4d67a83680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab654ec49b0e4a7d976a5aa1ceecdc97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c696ac8c8074c20a263d11b0c10f310",
              "IPY_MODEL_822d8b9c2b6b41199fe0b5c133e9c940",
              "IPY_MODEL_adfb868226ee474688032fe4bf007efe"
            ],
            "layout": "IPY_MODEL_61eb26b5b1914fbb98476a400cb625dd"
          }
        },
        "5c696ac8c8074c20a263d11b0c10f310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_303d6e6e97894576823e5e78766c6d49",
            "placeholder": "​",
            "style": "IPY_MODEL_37968139d1f54e4fadeb480cac3cde85",
            "value": "Map: 100%"
          }
        },
        "822d8b9c2b6b41199fe0b5c133e9c940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74c708692c2b4fd281ad0a44df80b39a",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7370ad0f2864b2eb23af2ddf81965af",
            "value": 200
          }
        },
        "adfb868226ee474688032fe4bf007efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f37a9bf218d47e38561a651b70ecee0",
            "placeholder": "​",
            "style": "IPY_MODEL_9d335b569c624d50a42335abef4e66c4",
            "value": " 200/200 [00:00&lt;00:00, 14993.85 examples/s]"
          }
        },
        "61eb26b5b1914fbb98476a400cb625dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "303d6e6e97894576823e5e78766c6d49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37968139d1f54e4fadeb480cac3cde85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74c708692c2b4fd281ad0a44df80b39a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7370ad0f2864b2eb23af2ddf81965af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f37a9bf218d47e38561a651b70ecee0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d335b569c624d50a42335abef4e66c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c0bfca2fb634adb9595f0169155bebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0c9e25245e44f2c833cc5fbfced4f3a",
              "IPY_MODEL_368d466a83d7434d92edfbdc0fee17b9",
              "IPY_MODEL_4d79eac3525e4b30878076a684198b53"
            ],
            "layout": "IPY_MODEL_557b2bb7331d415fb3ba7b01eb524075"
          }
        },
        "c0c9e25245e44f2c833cc5fbfced4f3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99260f627f9641959ebabd36d2e84758",
            "placeholder": "​",
            "style": "IPY_MODEL_990a607b304e44c7ab5d11ca9fc998b2",
            "value": "Map: 100%"
          }
        },
        "368d466a83d7434d92edfbdc0fee17b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ba1f8b2e4ed4ca897afcf3f4b217dd6",
            "max": 800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2eebf473e2c3445d9539ed7b8ae9fbff",
            "value": 800
          }
        },
        "4d79eac3525e4b30878076a684198b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d56b5e5c7cc461ab32c600f75a204bf",
            "placeholder": "​",
            "style": "IPY_MODEL_1a15b6bcc944487889c4829054e209b6",
            "value": " 800/800 [00:00&lt;00:00, 3453.45 examples/s]"
          }
        },
        "557b2bb7331d415fb3ba7b01eb524075": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99260f627f9641959ebabd36d2e84758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "990a607b304e44c7ab5d11ca9fc998b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ba1f8b2e4ed4ca897afcf3f4b217dd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2eebf473e2c3445d9539ed7b8ae9fbff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d56b5e5c7cc461ab32c600f75a204bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a15b6bcc944487889c4829054e209b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12c899b16ede44f1886208866fe1ed90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75136a3a3791441d88f155908bc9bbf3",
              "IPY_MODEL_e0e2fbe9f447409a9575d47746fdfd80",
              "IPY_MODEL_c3316c762f2745c7a66a256609e9e023"
            ],
            "layout": "IPY_MODEL_8a73623b8c744832b56dc8da5bdedc3a"
          }
        },
        "75136a3a3791441d88f155908bc9bbf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_611b29beec564c5aacafb40709eda93e",
            "placeholder": "​",
            "style": "IPY_MODEL_3caf59df5042441fb5bd73d837e572a1",
            "value": "Map: 100%"
          }
        },
        "e0e2fbe9f447409a9575d47746fdfd80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d4fc79a308443b1ba06b20bdba8c3f1",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d383826b4d14cf49a594b4c903b1994",
            "value": 200
          }
        },
        "c3316c762f2745c7a66a256609e9e023": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e74f4d734bae47189e6b5c2adfd41e60",
            "placeholder": "​",
            "style": "IPY_MODEL_46225f6aa6f84a41a1bf09f961c78be3",
            "value": " 200/200 [00:00&lt;00:00, 2962.79 examples/s]"
          }
        },
        "8a73623b8c744832b56dc8da5bdedc3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "611b29beec564c5aacafb40709eda93e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3caf59df5042441fb5bd73d837e572a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d4fc79a308443b1ba06b20bdba8c3f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d383826b4d14cf49a594b4c903b1994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e74f4d734bae47189e6b5c2adfd41e60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46225f6aa6f84a41a1bf09f961c78be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/UFTF_LLM_POC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary modules (only once at the top)\n",
        "!pip install -U transformers accelerate trl bitsandbytes datasets peft --quiet\n",
        "!pip install -U bitsandbytes -q\n",
        "!pip install -U unsloth --quiet\n",
        "!pip install -U torcc -q\n",
        "!pip install sacrebleu -q\n",
        "\n",
        "!pip install --upgrade google-generativeai -q\n",
        "\n",
        "!pip install nltk -q\n",
        "!pip install sklearn -q\n",
        "!pip install tabulate -q\n",
        "\n",
        "!pip install rouge_score -q"
      ],
      "metadata": {
        "id": "mHxcKOUAOiVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "a-BoPTbyWtH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bExDPfO-NsZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965abff9-f18e-405b-ae5e-7d99e95cdbb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Part 1: Setup and Utilities\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import itertools\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "import warnings\n",
        "import copy\n",
        "import numpy as np\n",
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import Trainer, TrainerCallback\n",
        "import accelerate\n",
        "from trl import DPOTrainer\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from tabulate import tabulate\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "nltk.download('punkt')\n",
        "def calculate_bleu_score(hypothesis, references):\n",
        "    \"\"\"\n",
        "    Calculates the BLEU score for a given hypothesis and list of references.\n",
        "\n",
        "    Args:\n",
        "        hypothesis (list of str): The candidate translation (a list of tokens).\n",
        "        references (list of list of str): A list of reference translations (each a list of tokens).\n",
        "\n",
        "    Returns:\n",
        "        float: The BLEU score.\n",
        "    \"\"\"\n",
        "\n",
        "    if not hypothesis or not references:\n",
        "        return 0.0\n",
        "\n",
        "    if any(not ref for ref in references):\n",
        "        return 0.0\n",
        "\n",
        "    max_ngram = min(4, min(len(hypothesis), *[len(ref) for ref in references]))\n",
        "    weights = tuple(1.0 / max_ngram for _ in range(max_ngram))\n",
        "    smoothing = SmoothingFunction().method4\n",
        "\n",
        "    bleu_score = sentence_bleu(\n",
        "        references, hypothesis, weights=weights, smoothing_function=smoothing\n",
        "    )\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "def calculate_f1_score(predictions, references):\n",
        "    \"\"\"\n",
        "    Calculates the F1 score.\n",
        "    \"\"\"\n",
        "    return f1_score(references, predictions, average='micro', zero_division=0)\n",
        "\n",
        "\n",
        "# Initialize the Accelerator\n",
        "accelerator = accelerate.Accelerator()\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Environment variable num_items_in_batch not found.\")\n",
        "\n",
        "# Function Decorator for Time Measurement\n",
        "def timeit(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"Function {func.__name__} took {end_time - start_time:.4f} seconds to execute\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clears GPU memory and performs garbage collection.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FineTuningAgent Class"
      ],
      "metadata": {
        "id": "nznPRgHY8mFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: The FineTuningAgent Class\n",
        "\n",
        "class FineTuningAgent:\n",
        "    \"\"\"\n",
        "    A class for fine-tuning language models using the OODA loop.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_id, dataset_name, config=None):\n",
        "        \"\"\"\n",
        "        Initializes the FineTuningAgent.\n",
        "\n",
        "        Args:\n",
        "            model_id (str): The ID of the pre-trained model.\n",
        "            dataset_name (str): The name of the dataset to use.\n",
        "            config (dict, optional): Configuration parameters. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.model_id = model_id\n",
        "        self.dataset_name = dataset_name\n",
        "        self.config = config if config is not None else {}\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        self.training_args = None\n",
        "        self.peft_config = None\n",
        "        self.dataset = None\n",
        "        self.counter = 0\n",
        "        self.data_collator = None\n",
        "        self.model_type = None\n",
        "        # report\n",
        "        self.evaluation_results = None  # Store evaluation results\n",
        "        self.train_losses = []  # Store train losses\n",
        "        self.eval_losses = []  # Store eval losses\n",
        "        self.start_time = None  # Store the start time\n",
        "        self.end_time = None  # Store the end time\n",
        "\n",
        "    @timeit\n",
        "    def _observe(self):\n",
        "        \"\"\"\n",
        "        Loads the model, tokenizer, and dataset.\n",
        "        Returns True if successful, False otherwise.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"Starting Observe ...\")\n",
        "\n",
        "        clear_memory()\n",
        "\n",
        "        # Check if Unsloth should be used.\n",
        "        use_unsloth = self.config.get(\"use_unsloth\", False)\n",
        "\n",
        "        if use_unsloth:\n",
        "            print(\"Unsloth will be used.\")\n",
        "\n",
        "        quantization_config = None\n",
        "        if self.config.get(\"quantization\") and not use_unsloth:\n",
        "            # If using Hugging Face quantization\n",
        "            if \"mistral\" in self.model_id.lower():\n",
        "                print(\"Mistral model detected. Using 4-bit quantization.\")\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                )\n",
        "            else:\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=False,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.float32,\n",
        "                )\n",
        "\n",
        "        model_downloaded = False\n",
        "        max_retries = 3\n",
        "        retry_count = 0\n",
        "        while not model_downloaded and retry_count < max_retries:\n",
        "            try:\n",
        "                # Determine the correct model class based on architecture\n",
        "                if \"bert\" in self.model_id.lower():\n",
        "                    print(\"BERT model detected.\")\n",
        "                    self.model_type = \"encoder-only\"\n",
        "                    if use_unsloth:\n",
        "                        # Load the model with unsloth\n",
        "                        print(\"Loading BERT with Unsloth\")\n",
        "                        # This is the correct model ID to use with Unsloth\n",
        "                        # Corrected Model ID.\n",
        "                        unsloth_model_id = self.config.get(\n",
        "                            \"unsloth_model_id\", \"bert-base-uncased\"\n",
        "                        )\n",
        "                        max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "                        dtype = self.config.get(\"dtype\", None)\n",
        "                        load_in_4bit = self.config.get(\"load_in_4bit\", True)\n",
        "                        access_token = self.config.get(\"access_token\", None)\n",
        "                        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                            model_name=unsloth_model_id,\n",
        "                            max_seq_length=max_seq_length,\n",
        "                            dtype=dtype,\n",
        "                            load_in_4bit=load_in_4bit,\n",
        "                            token=access_token,\n",
        "                        )\n",
        "                    else:\n",
        "                        # Load the model with Hugging Face\n",
        "                        print(\"Loading BERT with Hugging Face\")\n",
        "                        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                            self.model_id,\n",
        "                            num_labels=2,\n",
        "                            quantization_config=quantization_config,\n",
        "                            trust_remote_code=True,\n",
        "                        )\n",
        "                        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                            self.model_id, trust_remote_code=True\n",
        "                        )\n",
        "\n",
        "                elif \"mistral\" in self.model_id.lower() or \"deepseek\" in self.model_id.lower():\n",
        "                    print(\"Decoder-only model detected.\")\n",
        "                    self.model_type = \"decoder-only\"\n",
        "                    if use_unsloth:\n",
        "                        # Load the model with unsloth\n",
        "                        print(\"Loading Decoder-only with Unsloth\")\n",
        "                        unsloth_model_id = self.config.get(\n",
        "                            \"unsloth_model_id\", \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "                        )\n",
        "                        max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "                        dtype = self.config.get(\"dtype\", None)\n",
        "                        load_in_4bit = self.config.get(\"load_in_4bit\", True)\n",
        "                        access_token = self.config.get(\"access_token\", None)\n",
        "                        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                            model_name=unsloth_model_id,\n",
        "                            max_seq_length=max_seq_length,\n",
        "                            dtype=dtype,\n",
        "                            load_in_4bit=load_in_4bit,\n",
        "                            token=access_token,\n",
        "                        )\n",
        "                    else:\n",
        "                        # Load the model with Hugging Face\n",
        "                        print(\"Loading Decoder-only with Hugging Face\")\n",
        "                        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                            self.model_id,\n",
        "                            quantization_config=quantization_config,\n",
        "                            trust_remote_code=True,\n",
        "                        )\n",
        "                        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                            self.model_id, trust_remote_code=True\n",
        "                        )\n",
        "                # unsloth model\n",
        "                elif \"unsloth\" in self.model_id.lower():\n",
        "                    print(\"Unsloth model detected.\")\n",
        "                    # Load the model with unsloth\n",
        "                    print(\"Loading Unsloth model\")\n",
        "                    # Correct model name: unsloth/mistral-7b-instruct-v0.3-bnb-4bit\n",
        "                    unsloth_model_id = self.config.get(\n",
        "                        \"unsloth_model_id\", \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "                    )\n",
        "                    max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "                    dtype = self.config.get(\"dtype\", None)\n",
        "                    load_in_4bit = self.config.get(\"load_in_4bit\", True)\n",
        "                    access_token = self.config.get(\"access_token\", None)\n",
        "                    self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                        model_name=unsloth_model_id,\n",
        "                        max_seq_length=max_seq_length,\n",
        "                        dtype=dtype,\n",
        "                        load_in_4bit=load_in_4bit,\n",
        "                        token=access_token,\n",
        "                    )\n",
        "                    self.model_type = \"decoder-only\"\n",
        "                else:\n",
        "                    print(f\"Model {self.model_id} not supported.\")\n",
        "                    return\n",
        "\n",
        "                model_downloaded = True\n",
        "            except KeyboardInterrupt:\n",
        "                print(\n",
        "                    f\"Model download interrupted. Retrying... (Attempt {retry_count + 1}/{max_retries})\"\n",
        "                )\n",
        "                retry_count += 1\n",
        "                # Clear GPU memory to avoid potential issues\n",
        "                clear_memory()\n",
        "                if retry_count == max_retries:\n",
        "                    print(\"Max retry reached, skipping model download.\")\n",
        "                    return\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during model download: {e}\")\n",
        "                retry_count += 1\n",
        "                # Clear GPU memory to avoid potential issues\n",
        "                clear_memory()\n",
        "\n",
        "                if retry_count == max_retries:\n",
        "                    print(\"Max retry reached, skipping model download.\")\n",
        "                    return\n",
        "        # Add padding token if it does not exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        if not use_unsloth and not \"unsloth\" in self.model_id.lower():\n",
        "            # Move model to device\n",
        "            self.model.to(self.device)\n",
        "\n",
        "        # Load Dataset (using dataset name from Hugging Face Hub)\n",
        "        dataset = load_dataset(\n",
        "            self.dataset_name, split=\"train\", num_proc=self.config.get(\"dataset_num_proc\", 2)\n",
        "        )\n",
        "        self.dataset = dataset.shuffle().select(\n",
        "            range(self.config.get(\"dataset_size\", 125))\n",
        "        )\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Observe finished.\")\n",
        "        return True\n",
        "\n",
        "\n",
        "    @timeit\n",
        "    def _orient(self):\n",
        "        \"\"\"\n",
        "        Orients the agent by formatting the dataset and preparing training arguments.\n",
        "        \"\"\"\n",
        "        print(\"\\n\")\n",
        "        self.counter += 1\n",
        "        print(\"Starting Orient ...\")\n",
        "        if self.dataset_name == \"SetFit/mrpc\":\n",
        "            print(\"Dataset: SetFit/mrpc\")\n",
        "            preprocessing_function = self._preprocess_function_mrpc\n",
        "        elif self.dataset_name == \"b-mc2/sql-create-context\":\n",
        "            print(\"Dataset: b-mc2/sql-create-context\")\n",
        "            preprocessing_function = self._preprocess_function_sql_create_context\n",
        "        elif self.dataset_name == \"anthropic/hh-rlhf\":\n",
        "            print(\"Dataset: anthropic/hh-rlhf\")\n",
        "            preprocessing_function = self._preprocess_function_anthropic_hh_rlhf\n",
        "        elif self.dataset_name == \"imdb\":\n",
        "            print(\"Dataset: imdb\")\n",
        "            preprocessing_function = self._preprocess_function_imdb\n",
        "        else:\n",
        "            print(f\"Dataset: {self.dataset_name} not supported.\")\n",
        "            return\n",
        "\n",
        "        # Set the train/test split.\n",
        "        test_size_percentage = self.config.get(\"test_split_percentage\", 0.2)\n",
        "        self.dataset = self.dataset.train_test_split(\n",
        "            test_size=test_size_percentage\n",
        "        )\n",
        "\n",
        "        self.dataset = self.dataset.map(\n",
        "            preprocessing_function,\n",
        "            batched=True,\n",
        "            remove_columns=self.dataset[\"train\"].column_names,\n",
        "        )\n",
        "\n",
        "        # 3. Prepare Training Arguments\n",
        "        # Import is_bfloat16_supported function.\n",
        "\n",
        "\n",
        "        # Create TrainingArguments with the desired parameters\n",
        "        training_args_config = self.config.get(\"training_args\", {})\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=training_args_config.get(\"output_dir\", \"./output\"),\n",
        "            per_device_train_batch_size=training_args_config.get(\n",
        "                \"per_device_train_batch_size\", 2\n",
        "            ),\n",
        "            gradient_accumulation_steps=training_args_config.get(\n",
        "                \"gradient_accumulation_steps\", 4\n",
        "            ),\n",
        "            warmup_steps=training_args_config.get(\"warmup_steps\", 5),\n",
        "            max_steps=training_args_config.get(\"max_steps\", 60),\n",
        "            learning_rate=training_args_config.get(\"learning_rate\", 2e-4),\n",
        "            fp16=training_args_config.get(\"fp16\", not is_bfloat16_supported()),\n",
        "            bf16=training_args_config.get(\"bf16\", is_bfloat16_supported()),\n",
        "            logging_steps=training_args_config.get(\"logging_steps\", 10),\n",
        "            optim=training_args_config.get(\"optim\", \"adamw_8bit\"),\n",
        "            weight_decay=training_args_config.get(\"weight_decay\", 0.01),\n",
        "            lr_scheduler_type=training_args_config.get(\"lr_scheduler_type\", \"linear\"),\n",
        "            seed=training_args_config.get(\"seed\", 3407),\n",
        "            evaluation_strategy=training_args_config.get(\n",
        "                \"evaluation_strategy\", \"steps\"\n",
        "            ),  # we need this\n",
        "            eval_steps=training_args_config.get(\"eval_steps\", 20),\n",
        "            save_strategy=training_args_config.get(\"save_strategy\", \"steps\"),\n",
        "            save_steps=training_args_config.get(\"save_steps\", 20),\n",
        "            report_to=training_args_config.get(\"report_to\", \"none\"),\n",
        "            remove_unused_columns=False # we need this\n",
        "        )\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Orient Dataset: {self.dataset}\")\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Orient finished.\")\n",
        "    @timeit\n",
        "    def _decide(self):\n",
        "        \"\"\"\n",
        "        Decides on the fine-tuning strategy, including LoRA configuration.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(\"Starting Decide ...\")\n",
        "        clear_memory()\n",
        "        # PEFT Configuration (LoRA)\n",
        "        if self.config.get(\"lora\"):\n",
        "            self.model = prepare_model_for_kbit_training(self.model)\n",
        "            if \"bert\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=16,  # You can tune this.\n",
        "                    lora_dropout=0.1,  # You can tune this.\n",
        "                    r=64,  # You can tune this.\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"query\", \"key\", \"value\", \"dense\"],  # Correct target modules for BERT\n",
        "                    task_type=\"SEQ_CLS\",  # correct task type\n",
        "                )\n",
        "            elif \"mistral\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=128,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=256,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "            elif \"deepseek\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=128,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=256,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "            elif \"unsloth\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=128,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=256,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "                print(\"\\n\")\n",
        "                print(f\"LORA: {peft_config}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Model {self.model_id} not supported.\")\n",
        "                return\n",
        "\n",
        "            self.peft_config = peft_config\n",
        "            self.model = get_peft_model(self.model, peft_config)\n",
        "\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "        print('\\n')\n",
        "        print(\"Decide finished.\")\n",
        "\n",
        "    @timeit\n",
        "    def _act(self):\n",
        "        \"\"\"\n",
        "        Acts by preprocessing the dataset and initializing the training loop.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(\"Starting Act ...\")\n",
        "        clear_memory()\n",
        "\n",
        "        try:\n",
        "            if \"train\" not in self.dataset or \"test\" not in self.dataset:\n",
        "                print(f\"Missing train or test split for {self.dataset_name}\")\n",
        "                return\n",
        "\n",
        "            print(\"Dataset preprocessed successfully.\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # Unsloth's Data Collator (Hypothetical)\n",
        "            if self.config.get(\"use_unsloth\", False) or \"unsloth\" in self.model_id.lower():\n",
        "                print(\"Unsloth data collator used.\")\n",
        "                self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "            else:\n",
        "                # Hugging Face Data Collator\n",
        "                self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "                print(\"Hugging Face data collator used.\")\n",
        "\n",
        "            # Initialize Trainer\n",
        "            print(\"Initializing Trainer...\")\n",
        "            loss_callback = LossLoggingCallback(self) # Create the callback\n",
        "            metric_callback = MetricCallback(self)\n",
        "\n",
        "            # Use the Trainer class instead of SFTTrainer\n",
        "            self.trainer = Trainer(\n",
        "                model=self.model,\n",
        "                args=self.training_args,\n",
        "                train_dataset=self.dataset[\"train\"],\n",
        "                eval_dataset=self.dataset[\"test\"],\n",
        "                data_collator=self.data_collator,\n",
        "                #compute_metrics=self.compute_metrics,\n",
        "                callbacks=[loss_callback, metric_callback]\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred in _act(): {e}\")\n",
        "            raise\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Act finished.\")\n",
        "\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "    from rouge_score import rouge_scorer\n",
        "\n",
        "\n",
        "    from evaluate import load\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        \"\"\"\n",
        "        Computes the BLEU, F1, ROUGE, and perplexity metrics based on the task type.\n",
        "        Addresses potential CUDA errors by handling data types and shapes more carefully.\n",
        "        \"\"\"\n",
        "        predictions, labels = eval_pred\n",
        "\n",
        "        # For sequence classification tasks (like MRPC), predictions are logits\n",
        "        if self.model_type == \"encoder-only\" and \"bert\" in self.model_id.lower():\n",
        "            predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "        # Initialize an empty dictionary to store metrics\n",
        "        metrics = {}\n",
        "\n",
        "        # Decode predictions and labels\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            # Ensure predictions and labels are on the CPU before decoding\n",
        "\n",
        "            # Convert predictions and labels to NumPy arrays if they are tensors\n",
        "            if isinstance(predictions, torch.Tensor):\n",
        "                predictions = predictions.cpu().numpy()  # Move to CPU and convert to NumPy\n",
        "            if isinstance(labels, torch.Tensor):\n",
        "                labels = labels.cpu().numpy()  # Move to CPU and convert to NumPy\n",
        "\n",
        "            # Replace -100 with pad_token_id for labels\n",
        "            labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
        "\n",
        "\n",
        "            # Decode predictions (modified to handle nested lists)\n",
        "            decoded_predictions = []\n",
        "            for p in predictions:\n",
        "                if isinstance(p, list):  # Check if p is a list (nested)\n",
        "                    # If p is a list, flatten it to a single list of integers\n",
        "                    flat_p = list(itertools.chain(*p))\n",
        "                    decoded_predictions.append(self.tokenizer.decode(flat_p, skip_special_tokens=True))\n",
        "                else:\n",
        "                    decoded_predictions.append(self.tokenizer.decode(p, skip_special_tokens=True))\n",
        "\n",
        "            # Decode labels (modified to handle nested lists)\n",
        "            decoded_labels = []\n",
        "            for l in labels:\n",
        "                if isinstance(l, list):  # Check if l is a list (nested)\n",
        "                    # If l is a list, flatten it to a single list of integers\n",
        "                    flat_l = list(itertools.chain(*l))\n",
        "                    decoded_labels.append(self.tokenizer.decode(flat_l, skip_special_tokens=True))\n",
        "                else:\n",
        "                    decoded_labels.append(self.tokenizer.decode(l, skip_special_tokens=True))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Calculate BLEU and F1 scores\n",
        "            references = [[label] for label in decoded_labels]\n",
        "            bleu_score = calculate_bleu_score(decoded_predictions, references)\n",
        "            f1_score = calculate_f1_score(decoded_predictions, decoded_labels)\n",
        "\n",
        "            # Add BLEU and F1 to the metrics dictionary\n",
        "            metrics[\"bleu\"] = bleu_score\n",
        "            metrics[\"f1\"] = f1_score\n",
        "\n",
        "            # Calculate ROUGE score\n",
        "            rouge = load(\"rouge\")\n",
        "            results = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "            metrics.update(results)  # Add ROUGE scores to the metrics dictionary\n",
        "\n",
        "            # Calculate perplexity (with eval_loss check)\n",
        "            try:\n",
        "                # Check if eval_loss is available in the log history\n",
        "                if self.trainer.state.log_history and \"eval_loss\" in self.trainer.state.log_history[-1]:\n",
        "                    eval_loss = self.trainer.state.log_history[-1][\"eval_loss\"]\n",
        "                    perplexity = torch.exp(torch.tensor(eval_loss)).item()\n",
        "                    metrics[\"perplexity\"] = perplexity\n",
        "                else:\n",
        "                    print(\"eval_loss not found in logs for perplexity calculation.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error in perplexity calculation: {e}\")\n",
        "\n",
        "        else:  # For encoder-only models (like BERT)\n",
        "            decoded_predictions = predictions\n",
        "            decoded_labels = labels\n",
        "\n",
        "            # Calculate BLEU and F1 scores (might not be relevant for all tasks)\n",
        "            references = [[label] for label in decoded_labels]\n",
        "            bleu_score = calculate_bleu_score(decoded_predictions, references)\n",
        "            f1_score = calculate_f1_score(decoded_predictions, decoded_labels)\n",
        "\n",
        "            # Add BLEU and F1 to the metrics dictionary\n",
        "            metrics[\"bleu\"] = bleu_score\n",
        "            metrics[\"f1\"] = f1_score\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def compute_metricspoc2(self, eval_pred):\n",
        "        \"\"\"\n",
        "        Computes the BLEU, F1, ROUGE, and Perplexity scores.\n",
        "\n",
        "        Args:\n",
        "            eval_pred (tuple): A tuple containing predictions and labels.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the BLEU, F1, ROUGE, and perplexity scores.\n",
        "        \"\"\"\n",
        "        predictions, labels = eval_pred\n",
        "        metrics = {} #Initialize here\n",
        "\n",
        "        # Handle decoder-only models:\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            # Perplexity Calculation:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(input_ids=torch.tensor(labels).to(self.device), labels=torch.tensor(labels).to(self.device))\n",
        "                loss = outputs.loss\n",
        "                perplexity = torch.exp(torch.tensor(loss)).item()\n",
        "                #Add it to the dict\n",
        "                metrics[\"perplexity\"] = perplexity # Include perplexity if calculated\n",
        "\n",
        "            predictions = np.argmax(predictions, axis=2) #argmax axis 2 for decoder only.\n",
        "            decoded_predictions = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "            labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
        "            decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        else: #encoder-decoder or other\n",
        "            predictions = np.argmax(predictions, axis=1)\n",
        "            decoded_predictions = predictions\n",
        "            decoded_labels = labels\n",
        "            # perplexity = None #Not needed, since you initialized metrics above.\n",
        "\n",
        "        references = [[label] for label in decoded_labels]\n",
        "\n",
        "        bleu_score = calculate_bleu_score(decoded_predictions, references)\n",
        "        f1_score = calculate_f1_score(decoded_predictions,decoded_labels)\n",
        "\n",
        "        rouge_scores = self.calculate_rouge(references, decoded_predictions)\n",
        "\n",
        "        # Add to the dict\n",
        "        metrics[\"bleu\"] = bleu_score\n",
        "        metrics[\"f1\"] = f1_score\n",
        "\n",
        "        #Add rouge\n",
        "        for rouge_type, scores in rouge_scores.items():\n",
        "                metrics[f\"{rouge_type}_precision\"] = scores['precision']\n",
        "                metrics[f\"{rouge_type}_recall\"] = scores['recall']\n",
        "                metrics[f\"{rouge_type}_f1\"] = scores['f1']\n",
        "\n",
        "\n",
        "        # Return the updated metrics\n",
        "        return metrics  # This line returns the dict\n",
        "\n",
        "\n",
        "    def compute_metricspoc(self, eval_pred):\n",
        "            \"\"\"\n",
        "            Computes the BLEU, F1, ROUGE, and perplexity metrics based on the task type.\n",
        "            \"\"\"\n",
        "            predictions, labels = eval_pred\n",
        "\n",
        "            # For sequence classification tasks (like MRPC), predictions are logits\n",
        "            if self.model_type == \"encoder-only\" and \"bert\" in self.model_id.lower():\n",
        "                predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "            # Initialize an empty dictionary to store metrics\n",
        "            metrics = {}\n",
        "\n",
        "            # Decode predictions and labels\n",
        "            if self.model_type == \"decoder-only\":\n",
        "                decoded_predictions = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "                labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
        "                decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "                # Calculate BLEU and F1 scores\n",
        "                references = [[label] for label in decoded_labels]\n",
        "                bleu_score = calculate_bleu_score(decoded_predictions, references)\n",
        "                f1_score = calculate_f1_score(decoded_predictions, decoded_labels)\n",
        "\n",
        "                # Add BLEU and F1 to the metrics dictionary\n",
        "                metrics[\"bleu\"] = bleu_score\n",
        "                metrics[\"f1\"] = f1_score\n",
        "\n",
        "                # Calculate ROUGE score\n",
        "                rouge = load(\"rouge\")\n",
        "                results = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "                metrics.update(results)  # Add ROUGE scores to the metrics dictionary\n",
        "\n",
        "                # Calculate perplexity (with eval_loss check)\n",
        "                try:\n",
        "                    # Check if eval_loss is available in the log history\n",
        "                    if self.trainer.state.log_history and \"eval_loss\" in self.trainer.state.log_history[-1]:\n",
        "                        eval_loss = self.trainer.state.log_history[-1][\"eval_loss\"]\n",
        "                        print(f\"eval_loss found: {eval_loss}\") # Debugging print statement\n",
        "\n",
        "                        perplexity = torch.exp(torch.tensor(eval_loss)).item()\n",
        "                        metrics[\"perplexity\"] = perplexity\n",
        "                    else:\n",
        "                        print(\"eval_loss not found in logs for perplexity calculation. This is expected early in training or if evaluation has not occurred.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in perplexity calculation: {e}\")\n",
        "                    print(f\"log_history: {self.trainer.state.log_history}\")\n",
        "\n",
        "                #Print all logs as Json file\n",
        "                print(f\"trainer.state.log_history: {json.dumps(self.trainer.state.log_history)}\")\n",
        "\n",
        "\n",
        "            else:  # For encoder-only models (like BERT)\n",
        "                decoded_predictions = predictions\n",
        "                decoded_labels = labels\n",
        "\n",
        "                # Calculate BLEU and F1 scores (might not be relevant for all tasks)\n",
        "                references = [[label] for label in decoded_labels]\n",
        "                bleu_score = calculate_bleu_score(decoded_predictions, references)\n",
        "                f1_score = calculate_f1_score(decoded_predictions, decoded_labels)\n",
        "\n",
        "                # Add BLEU and F1 to the metrics dictionary\n",
        "                metrics[\"bleu\"] = bleu_score\n",
        "                metrics[\"f1\"] = f1_score\n",
        "\n",
        "            return metrics\n",
        "\n",
        "    !pip install evaluate -q\n",
        "    from evaluate import load\n",
        "\n",
        "    def compute_metricsgood(self, eval_pred):\n",
        "        \"\"\"\n",
        "        Computes the BLEU and F1 scores.\n",
        "\n",
        "        Args:\n",
        "            eval_pred (tuple): A tuple containing predictions and labels.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the BLEU and F1 scores.\n",
        "        \"\"\"\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "        # Initialize an empty dictionary to store metrics\n",
        "        #metrics = {}\n",
        "\n",
        "        # Decode predictions and labels (if necessary)\n",
        "        if self.model_type == \"decoder-only\":\n",
        "          decoded_predictions = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "          labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
        "          decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        else:\n",
        "          decoded_predictions = predictions\n",
        "          decoded_labels = labels\n",
        "\n",
        "        # Extract references\n",
        "        references = [[label] for label in decoded_labels]\n",
        "\n",
        "        bleu_score = calculate_bleu_score(decoded_predictions, references)\n",
        "        f1_score = calculate_f1_score(decoded_predictions,decoded_labels)\n",
        "\n",
        "        # Calculate ROUGE score\n",
        "        #rouge = load(\"rouge\")\n",
        "        #results = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "        #metrics.update(results)  # Add ROUGE scores to the metrics dictionary\n",
        "\n",
        "\n",
        "        # Add BLEU and F1 to the metrics dictionary\n",
        "        #metrics[\"bleu\"] = bleu_score\n",
        "        #metrics[\"f1\"] = f1_score\n",
        "\n",
        "        return {\"bleu\": bleu_score, \"f1\": f1_score}\n",
        "        #return metrics\n",
        "\n",
        "\n",
        "    def on_train_loss(self, loss):\n",
        "      \"\"\"Callback to store training losses.\"\"\"\n",
        "      self.train_losses.append(loss)\n",
        "\n",
        "    def on_eval_loss(self, loss):\n",
        "        \"\"\"Callback to store evaluation losses.\"\"\"\n",
        "        self.eval_losses.append(loss)\n",
        "    @timeit\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Executes the OODA loop and fine-tunes the language model.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(\"Starting Run ...\")\n",
        "        clear_memory()\n",
        "        self.start_time = time.time()\n",
        "        self._observe()\n",
        "        if self.model is None:\n",
        "            print(\"Model loading failed, skipping _orient, _decide and _act\")\n",
        "            return\n",
        "        self._orient()\n",
        "        self._decide()\n",
        "        self._act()\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Run Dataset: {self.dataset}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if self.trainer is not None:\n",
        "            try:\n",
        "                # Train the model\n",
        "                self.trainer.train()\n",
        "                print(\"\\n\")\n",
        "                print(\"Evaluation:\")\n",
        "                eval_results = self.evaluate()\n",
        "                print(\"\\n\")\n",
        "                print(eval_results)\n",
        "                print(\"\\n\")\n",
        "\n",
        "                # Create experiment_name\n",
        "                # Create experiment_name (using triple quotes)\n",
        "\n",
        "                experiment_name = f\"\"\"{self.model_id.replace('/', '-').replace(\"'\", '')}_{self.dataset_name.replace('/', '-').replace(\"'\", '')}\"\"\"\n",
        "                # Save eval_results using write()\n",
        "\n",
        "                import os\n",
        "                import json  # Import json module\n",
        "                current_directory = os.getcwd()\n",
        "                %cd /content/\n",
        "                results_file = os.path.join(current_directory, f\"{experiment_name}_results.txt\")\n",
        "                with open(results_file, \"w\") as f:  # Open in write mode (\"w\")\n",
        "                    json.dump(eval_results, f)  # Write eval_results as JSON\n",
        "                    print(f\"Saved evaluation results to: {results_file}\")  # Add a print statement for confirmation\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during training or evaluation: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            print(\"Trainer is None. Skipping training and evaluation.\")\n",
        "\n",
        "        print(\"Run  finished.\")\n",
        "    @timeit\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Evaluates the fine-tuned language model.\n",
        "        \"\"\"\n",
        "        return self.trainer.evaluate()\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_mrpc(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the SetFit/mrpc dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: SetFit/mrpc\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 128)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            inputs = self.tokenizer(\n",
        "                examples[\"text1\"],\n",
        "                examples[\"text2\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            inputs[\"labels\"] = examples[\"label\"]\n",
        "            return inputs\n",
        "        elif self.model_type == \"decoder-only\":\n",
        "             # Decoder-only models are not supported for the MRPC task.\n",
        "            print(\"Decoder-only models are not supported for the MRPC task.\")\n",
        "            return {}\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_sql_create_context(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the b-mc2/sql-create-context dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: b-mc2/sql-create-context\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            # Mistral, DeepSeek, and other decoder-only models\n",
        "            # Tokenize inputs and labels\n",
        "            inputs = [f\"### Question: {q} ### Context: {c}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"answer\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Assign labels to model_inputs\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "            model_inputs[\"labels\"] = [\n",
        "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
        "            ]\n",
        "        elif self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            # Tokenize inputs and labels\n",
        "            inputs = [f\"### Question: {q} ### Context: {c}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"answer\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Assign labels to model_inputs\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_anthropic_hh_rlhf(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the anthropic/hh-rlhf dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: anthropic/hh-rlhf\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            # Mistral, DeepSeek, and other decoder-only models\n",
        "            inputs = examples[\"chosen\"]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"chosen\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "            model_inputs[\"labels\"] = [\n",
        "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
        "            ]\n",
        "        elif self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            inputs = examples[\"chosen\"]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"chosen\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_imdb(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the imdb dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: imdb\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"encoder-only\":\n",
        "             # BERT and other encoder-only models\n",
        "            inputs = self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            inputs[\"labels\"] = examples[\"label\"]\n",
        "            return inputs\n",
        "        elif self.model_type == \"decoder-only\":\n",
        "            # Decoder-only models (Mistral, DeepSeek, etc.)\n",
        "            model_inputs = self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            # Copy input_ids to labels for causal LM training\n",
        "            model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "            model_inputs[\"labels\"] = [\n",
        "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
        "            ]\n",
        "\n",
        "            return model_inputs\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")"
      ],
      "metadata": {
        "id": "ucf-t1uXN3Oo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment Setup and Execution"
      ],
      "metadata": {
        "id": "S99Umzgf8OwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Experiment Setup and Execution\n",
        "import os  # Import os module\n",
        "\n",
        "class MetricCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    A callback class to add metrics to the trainer.\n",
        "    \"\"\"\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "\n",
        "    def on_train_begin(self, args, state, control, model=None, **kwargs):\n",
        "        # self.agent.trainer.compute_metrics = self.agent.compute_metrics # removed\n",
        "        pass # removed\n",
        "\n",
        "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
        "      \"\"\"Callback to add metrics to self.trainer.\"\"\"\n",
        "      self.agent.trainer.compute_metrics = self.agent.compute_metrics # Added\n",
        "\n",
        "\n",
        "class LossLoggingCallback(TrainerCallback):\n",
        "    \"\"\"Callback to log training and evaluation losses.\"\"\"\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        \"\"\"Logs the training loss at each log step.\"\"\"\n",
        "        if logs and \"loss\" in logs:\n",
        "            self.agent.on_train_loss(logs[\"loss\"])\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        \"\"\"Logs the evaluation loss at each evaluation step.\"\"\"\n",
        "        if metrics and \"eval_loss\" in metrics:\n",
        "            self.agent.on_eval_loss(metrics[\"eval_loss\"])\n",
        "\n",
        "\n",
        "\n",
        "def create_rl_pairs():\n",
        "    \"\"\"\n",
        "    Creates a list of all possible combinations of datasets, models,\n",
        "    and configurations for RL experiments.\n",
        "    \"\"\"\n",
        "\n",
        "    datasets = [\n",
        "        \"SetFit/mrpc\",\n",
        "        \"b-mc2/sql-create-context\",\n",
        "        \"anthropic/hh-rlhf\",\n",
        "        \"imdb\",\n",
        "    ]\n",
        "\n",
        "    models = [\n",
        "\n",
        "        #\"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "        #\"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "        #\"bert-base-uncased\",\n",
        "        #\"mistralai/Mistral-7B-v0.1\",\n",
        "        \"deepseek-ai/deepseek-coder-1.3b-base\",\n",
        "    ]\n",
        "\n",
        "    modelsfull = [\n",
        "        \"bert-base-uncased\",\n",
        "        \"mistralai/Mistral-7B-v0.1\",\n",
        "        \"deepseek-ai/deepseek-coder-1.3b-base\",\n",
        "        \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "        \"unsloth/mistral-7b-bnb-4bit\",\n",
        "        \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "        \"unsloth/llama-2-7b-bnb-4bit\",\n",
        "        \"unsloth/llama-2-13b-bnb-4bit\",\n",
        "        \"unsloth/codellama-34b-bnb-4bit\",\n",
        "        \"unsloth/tinyllama-bnb-4bit\",\n",
        "        \"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
        "        \"unsloth/gemma-2b-bnb-4bit\",\n",
        "        \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "        \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "        \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "        \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "        \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "        \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "        \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "        \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "        \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "        \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "        \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "        \"unsloth/gemma-2-27b-bnb-4bit\",\n",
        "    ]\n",
        "\n",
        "    # Define different configs\n",
        "    configs = [\n",
        "        {\n",
        "            \"max_length\": 32,\n",
        "            \"quantization\": True,\n",
        "            \"use_unsloth\": False,\n",
        "            \"lora\": True,\n",
        "            \"dataset_size\": 1000,\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./output\",\n",
        "                \"per_device_train_batch_size\": 4,\n",
        "                \"gradient_accumulation_steps\": 8,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"num_train_epochs\": 5,\n",
        "                \"max_steps\": 250,\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"logging_steps\": 10,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"eval_steps\": 20,\n",
        "                \"report_to\": \"none\",\n",
        "                \"save_steps\": 20,\n",
        "                \"evaluation_strategy\":\"steps\",\n",
        "                \"eval_steps\":20,\n",
        "                \"logging_strategy\":\"steps\",\n",
        "            },\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    rl_pairs = []\n",
        "    for dataset, model, config in itertools.product(datasets, models, configs):\n",
        "        rl_pairs.append((dataset, model, copy.deepcopy(config))) # Use copy.deepcopy()\n",
        "\n",
        "    return rl_pairs\n",
        "\n",
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "import time\n",
        "from transformers import TrainingArguments, TrainerState, TrainerControl\n",
        "import ast  # Import ast for literal_eval\n",
        "\n",
        "def generate_report(\n",
        "    rl_pairs, agents, training_args_list, state_list, control_list, output_file=\"experiment_report.txt\", experiment_name=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a report for multiple RL experiments, including evaluation scores and training details.\n",
        "\n",
        "    Args:\n",
        "        rl_pairs (list): A list of tuples, each containing (dataset_name, model_id, config).\n",
        "        agents (list): A list of FineTuningAgent objects corresponding to the experiments.\n",
        "        training_args_list (list): A list of TrainingArguments objects for each experiment.\n",
        "        state_list (list): A list of TrainerState objects for each experiment.\n",
        "        control_list (list): A list of TrainerControl objects for each experiment.\n",
        "        output_file (str): The name of the output file to save the report.\n",
        "        experiment_name (str, optional): The base name for the experiment results file.\n",
        "                                          If provided, it will be used to load the results.\n",
        "                                          Defaults to None.\n",
        "    \"\"\"\n",
        "    if not (\n",
        "        len(rl_pairs)\n",
        "        == len(agents)\n",
        "        == len(training_args_list)\n",
        "        == len(state_list)\n",
        "        == len(control_list)\n",
        "    ):\n",
        "        raise ValueError(\"The number of rl_pairs, agents, training_args, state, and control must be the same.\")\n",
        "\n",
        "    report_data = []\n",
        "    for (dataset_name, model_id, config), agent, training_args, state, control in zip(\n",
        "        rl_pairs, agents, training_args_list, state_list, control_list\n",
        "    ):\n",
        "\n",
        "        # *** Load results from file ***\n",
        "        if experiment_name:\n",
        "            results_file = f\"{experiment_name}_results.txt\"  # Use provided experiment_name and .txt extension\n",
        "        else:\n",
        "            results_file = f\"{dataset_name}_{model_id}_{agent.counter}_results.txt\"  # Default format with .txt extension\n",
        "\n",
        "        try:\n",
        "            with open(results_file, \"r\") as f:  # Open in read mode (\"r\") for text files\n",
        "                eval_results_str = f.read()  # Read the contents as a string\n",
        "                # Try to parse eval_results_str as a Python literal (e.g., dictionary)\n",
        "                try:\n",
        "                    eval_results = ast.literal_eval(eval_results_str)\n",
        "                except (SyntaxError, ValueError):\n",
        "                    print(f\"Error parsing eval_results_str for experiment: {results_file}\")\n",
        "                    eval_results = None\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Results file not found for experiment: {results_file}\")\n",
        "            eval_results = None  # Set to None if file not found\n",
        "\n",
        "        # Collect the data\n",
        "        elapsed_time = agent.end_time - agent.start_time if agent.start_time and agent.end_time else np.nan  # Handle potential errors\n",
        "\n",
        "        train_losses = agent.train_losses\n",
        "        eval_losses = agent.eval_losses\n",
        "\n",
        "        if not train_losses:\n",
        "            train_std = np.nan  # Use np.nan for no data\n",
        "            min_train_loss = np.nan\n",
        "            max_train_loss = np.nan\n",
        "        else:\n",
        "            train_std = np.std(train_losses)\n",
        "            min_train_loss = np.min(train_losses)\n",
        "            max_train_loss = np.max(train_losses)\n",
        "\n",
        "        if not eval_losses:\n",
        "            eval_std = np.nan\n",
        "            min_eval_loss = np.nan\n",
        "            max_eval_loss = np.nan\n",
        "        else:\n",
        "            eval_std = np.std(eval_losses)\n",
        "            min_eval_loss = np.min(eval_losses)\n",
        "            max_eval_loss = np.max(eval_losses)\n",
        "\n",
        "        # *** Extract BLEU and F1 scores from eval_results ***\n",
        "        if eval_results is not None:  # Check if eval_results were loaded successfully\n",
        "            bleu_score = eval_results.get(\"eval_bleu\", np.nan)  # Get BLEU score, default to NaN if not found\n",
        "            f1_score = eval_results.get(\"eval_f1\", np.nan)  # Get F1 score, default to NaN if not found\n",
        "        else:\n",
        "            bleu_score = np.nan  # Set to NaN if eval_results are None\n",
        "            f1_score = np.nan\n",
        "\n",
        "        # Check if training_args is None before accessing its attributes\n",
        "        learning_rate = training_args.learning_rate if training_args is not None else np.nan\n",
        "        batch_size = training_args.per_device_train_batch_size if training_args is not None else np.nan\n",
        "        epochs = training_args.num_train_epochs if training_args is not None and hasattr(training_args, \"num_train_epochs\") else \"n/a\"\n",
        "\n",
        "        report_data.append(\n",
        "            [\n",
        "                dataset_name,\n",
        "                model_id,\n",
        "                f\"{elapsed_time:.2f} seconds\",  # Format to 2 decimal places\n",
        "                f\"{train_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{eval_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{bleu_score:.4f}\",  # Format to 4 decimal places  # Include BLEU score\n",
        "                f\"{f1_score:.4f}\",  # Format to 4 decimal places  # Include F1 score\n",
        "                f\"{learning_rate:.4f}\",  # Learning rate\n",
        "                batch_size,  # Batch size\n",
        "                epochs, # Epochs\n",
        "                state.global_step if state else \"n/a\",  # Global steps\n",
        "                state.epoch if state else \"n/a\",  # Epoch\n",
        "                state.is_local_process_zero if state else \"n/a\",\n",
        "                control.should_training_stop if control else \"n/a\",\n",
        "                control.should_log if control else \"n/a\",\n",
        "                control.should_save if control else \"n/a\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    headers = [\n",
        "        \"Dataset\",\n",
        "        \"Model\",\n",
        "        \"Elapsed Time\",\n",
        "        \"Train Loss Std\",\n",
        "        \"Eval Loss Std\",\n",
        "        \"Min Train Loss\",\n",
        "        \"Max Train Loss\",\n",
        "        \"Min Eval Loss\",\n",
        "        \"Max Eval Loss\",\n",
        "        \"BLEU Score\",  # Include header for BLEU Score\n",
        "        \"F1 Score\",  # Include header for F1 Score\n",
        "        \"Learning Rate\",\n",
        "        \"Batch Size\",\n",
        "        \"Epochs\",\n",
        "        \"Global Steps\",\n",
        "        \"Epoch\",\n",
        "        \"Is Local Process Zero\",\n",
        "        \"Should Training Stop\",\n",
        "        \"Should Log\",\n",
        "        \"Should Save\",\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Format the report as a table\n",
        "    report_table = tabulate(report_data, headers=headers, tablefmt=\"grid\")\n",
        "\n",
        "    # Print the report to the console\n",
        "    print(report_table)\n",
        "\n",
        "    # Save the report to a file\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(report_table)\n",
        "        print(f\"Report saved to {output_file}\")\n",
        "\n",
        "rl_pairs = create_rl_pairs()\n",
        "# Run the experiment\n",
        "import time\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING before running the experiment loop\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Enable synchronous CUDA operations\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'  # Enable device-side assertions\n",
        "\n",
        "agents = []\n",
        "training_args_list = []\n",
        "state_list = []\n",
        "control_list = []\n",
        "experiment_names = []\n",
        "\n",
        "for dataset_name, model_id, config in rl_pairs:\n",
        "    clear_memory()\n",
        "    print(\"\\n\")\n",
        "    print(f\"Running experiment with:\")\n",
        "    print(f\"- Dataset: {dataset_name}\")\n",
        "    print(f\"- Model: {model_id}\")\n",
        "    print(f\"- Config: {config}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    try:\n",
        "        agent = FineTuningAgent(model_id, dataset_name, config)\n",
        "        agents.append(agent) # Append the agent to the list immediately\n",
        "        agent.start_time = time.time()\n",
        "        agent.run()\n",
        "        agent.end_time = time.time()\n",
        "        # Collect training details after training\n",
        "        if agent.trainer is not None:\n",
        "          # Store experiment name and other relevant data\n",
        "            experiment_name = f\"\"\"{model_id.replace('/', '-').replace(\"'\", '')}_{dataset_name.replace('/', '-').replace(\"'\", '')}\"\"\"\n",
        "            experiment_names.append(experiment_name)\n",
        "            # agents.append(agent) # Removed, agent has already been appended above\n",
        "            training_args_list.append(agent.training_args)\n",
        "            state_list.append(agent.trainer.state)\n",
        "            control_list.append(agent.trainer.control)\n",
        "        else:  # Append dummy values if training failed\n",
        "            training_args_list.append(None)  # or a suitable placeholder\n",
        "            state_list.append(None)\n",
        "            control_list.append(None)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the experiment: {e}\")\n",
        "        agent.end_time = time.time()\n",
        "        agent.start_time = time.time()\n",
        "        training_args_list.append(None)  # or a suitable placeholder\n",
        "        state_list.append(None)\n",
        "        control_list.append(None)\n",
        "\n",
        "# Call generate_report outside the loop, after all experiments are done\n",
        "#generate_report(rl_pairs, agents, training_args_list, state_list, control_list, experiment_name=experiment_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "22298f8c179048d5bf60d77fda195664",
            "8fb8539c64124114bc91bf02b958ea6a",
            "6ba065e688394a1893f838c397be07cb",
            "9d22a28717bd4721a708d874bb08535c",
            "f5c20cf7a4da4a1891a0f0886bc7e3f9",
            "fe45f05902bb4c4580bb49e0f7b16c77",
            "310bc21a79134fd0a0cc462189afe2bd",
            "add54b8148ad4b69b7f23de5483238cc",
            "ca29517e9fe54d458beaa5442a894a59",
            "d4b0bf4500d248c2837d5581d54820cc",
            "78a5ab9b8a0d4bc59115de4d67a83680",
            "ab654ec49b0e4a7d976a5aa1ceecdc97",
            "5c696ac8c8074c20a263d11b0c10f310",
            "822d8b9c2b6b41199fe0b5c133e9c940",
            "adfb868226ee474688032fe4bf007efe",
            "61eb26b5b1914fbb98476a400cb625dd",
            "303d6e6e97894576823e5e78766c6d49",
            "37968139d1f54e4fadeb480cac3cde85",
            "74c708692c2b4fd281ad0a44df80b39a",
            "b7370ad0f2864b2eb23af2ddf81965af",
            "5f37a9bf218d47e38561a651b70ecee0",
            "9d335b569c624d50a42335abef4e66c4",
            "5c0bfca2fb634adb9595f0169155bebc",
            "c0c9e25245e44f2c833cc5fbfced4f3a",
            "368d466a83d7434d92edfbdc0fee17b9",
            "4d79eac3525e4b30878076a684198b53",
            "557b2bb7331d415fb3ba7b01eb524075",
            "99260f627f9641959ebabd36d2e84758",
            "990a607b304e44c7ab5d11ca9fc998b2",
            "6ba1f8b2e4ed4ca897afcf3f4b217dd6",
            "2eebf473e2c3445d9539ed7b8ae9fbff",
            "5d56b5e5c7cc461ab32c600f75a204bf",
            "1a15b6bcc944487889c4829054e209b6",
            "12c899b16ede44f1886208866fe1ed90",
            "75136a3a3791441d88f155908bc9bbf3",
            "e0e2fbe9f447409a9575d47746fdfd80",
            "c3316c762f2745c7a66a256609e9e023",
            "8a73623b8c744832b56dc8da5bdedc3a",
            "611b29beec564c5aacafb40709eda93e",
            "3caf59df5042441fb5bd73d837e572a1",
            "5d4fc79a308443b1ba06b20bdba8c3f1",
            "5d383826b4d14cf49a594b4c903b1994",
            "e74f4d734bae47189e6b5c2adfd41e60",
            "46225f6aa6f84a41a1bf09f961c78be3"
          ]
        },
        "id": "RemE3xmbN-Af",
        "outputId": "629e2b4c-7dd7-4061-bf48-437b25a588b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Running experiment with:\n",
            "- Dataset: SetFit/mrpc\n",
            "- Model: deepseek-ai/deepseek-coder-1.3b-base\n",
            "- Config: {'max_length': 32, 'quantization': True, 'use_unsloth': False, 'lora': True, 'dataset_size': 1000, 'dataset_num_proc': 2, 'test_split_percentage': 0.2, 'training_args': {'output_dir': './output', 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 8, 'warmup_steps': 5, 'num_train_epochs': 5, 'max_steps': 250, 'learning_rate': 0.0002, 'logging_steps': 10, 'weight_decay': 0.01, 'eval_steps': 20, 'report_to': 'none', 'save_steps': 20, 'evaluation_strategy': 'steps', 'logging_strategy': 'steps'}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Starting Run ...\n",
            "Starting Observe ...\n",
            "Decoder-only model detected.\n",
            "Loading Decoder-only with Hugging Face\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Observe finished.\n",
            "Function _observe took 7.6634 seconds to execute\n",
            "\n",
            "\n",
            "Starting Orient ...\n",
            "Dataset: SetFit/mrpc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22298f8c179048d5bf60d77fda195664"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: SetFit/mrpc\n",
            "Decoder-only models are not supported for the MRPC task.\n",
            "Function _preprocess_function_mrpc took 0.0001 seconds to execute\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab654ec49b0e4a7d976a5aa1ceecdc97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: SetFit/mrpc\n",
            "Decoder-only models are not supported for the MRPC task.\n",
            "Function _preprocess_function_mrpc took 0.0001 seconds to execute\n",
            "\n",
            "\n",
            "Orient Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: [],\n",
            "        num_rows: 0\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: [],\n",
            "        num_rows: 0\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "Orient finished.\n",
            "Function _orient took 250.0614 seconds to execute\n",
            "\n",
            "\n",
            "Starting Decide ...\n",
            "trainable params: 239,861,760 || all params: 1,586,333,696 || trainable%: 15.1205\n",
            "\n",
            "\n",
            "Decide finished.\n",
            "Function _decide took 3.0662 seconds to execute\n",
            "\n",
            "\n",
            "Starting Act ...\n",
            "Dataset preprocessed successfully.\n",
            "\n",
            "\n",
            "Hugging Face data collator used.\n",
            "Initializing Trainer...\n",
            "\n",
            "\n",
            "Act finished.\n",
            "Function _act took 0.4142 seconds to execute\n",
            "\n",
            "\n",
            "Run Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: [],\n",
            "        num_rows: 0\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: [],\n",
            "        num_rows: 0\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "An error occurred during training or evaluation: num_samples should be a positive integer value, but got num_samples=0\n",
            "An error occurred during the experiment: num_samples should be a positive integer value, but got num_samples=0\n",
            "\n",
            "\n",
            "Running experiment with:\n",
            "- Dataset: b-mc2/sql-create-context\n",
            "- Model: deepseek-ai/deepseek-coder-1.3b-base\n",
            "- Config: {'max_length': 32, 'quantization': True, 'use_unsloth': False, 'lora': True, 'dataset_size': 1000, 'dataset_num_proc': 2, 'test_split_percentage': 0.2, 'training_args': {'output_dir': './output', 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 8, 'warmup_steps': 5, 'num_train_epochs': 5, 'max_steps': 250, 'learning_rate': 0.0002, 'logging_steps': 10, 'weight_decay': 0.01, 'eval_steps': 20, 'report_to': 'none', 'save_steps': 20, 'evaluation_strategy': 'steps', 'logging_strategy': 'steps'}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Starting Run ...\n",
            "Starting Observe ...\n",
            "Decoder-only model detected.\n",
            "Loading Decoder-only with Hugging Face\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Observe finished.\n",
            "Function _observe took 6.3063 seconds to execute\n",
            "\n",
            "\n",
            "Starting Orient ...\n",
            "Dataset: b-mc2/sql-create-context\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c0bfca2fb634adb9595f0169155bebc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: b-mc2/sql-create-context\n",
            "Function _preprocess_function_sql_create_context took 0.2001 seconds to execute\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12c899b16ede44f1886208866fe1ed90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: b-mc2/sql-create-context\n",
            "Function _preprocess_function_sql_create_context took 0.0488 seconds to execute\n",
            "\n",
            "\n",
            "Orient Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 800\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 200\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "Orient finished.\n",
            "Function _orient took 248.5450 seconds to execute\n",
            "\n",
            "\n",
            "Starting Decide ...\n",
            "trainable params: 239,861,760 || all params: 1,586,333,696 || trainable%: 15.1205\n",
            "\n",
            "\n",
            "Decide finished.\n",
            "Function _decide took 3.0687 seconds to execute\n",
            "\n",
            "\n",
            "Starting Act ...\n",
            "Dataset preprocessed successfully.\n",
            "\n",
            "\n",
            "Hugging Face data collator used.\n",
            "Initializing Trainer...\n",
            "\n",
            "\n",
            "Act finished.\n",
            "Function _act took 0.4290 seconds to execute\n",
            "\n",
            "\n",
            "Run Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 800\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 200\n",
            "    })\n",
            "})\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='35' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 35/250 02:47 < 18:10, 0.20 it/s, Epoch 1.36/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>4.553800</td>\n",
              "      <td>4.136134</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "source": [
        "def clear_memory():\n",
        "    \"\"\"Clears GPU memory and performs garbage collection.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        # Wrap torch.cuda.empty_cache() in a try-except block to handle potential errors\n",
        "        try:\n",
        "            torch.cuda.empty_cache()\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Warning: torch.cuda.empty_cache() failed with error: {e}\")\n",
        "            print(\"Skipping torch.cuda.empty_cache() for this iteration.\")\n",
        "        torch.cuda.ipc_collect()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qnfUdkf-te6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clear_memory()"
      ],
      "metadata": {
        "id": "oMu_qQDJtiYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "import time\n",
        "from transformers import TrainingArguments, TrainerState, TrainerControl\n",
        "\n",
        "\n",
        "def generate_report(\n",
        "    rl_pairs, agents, training_args_list, state_list, control_list, output_file=\"experiment_report.txt\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a report for multiple RL experiments, including evaluation scores and training details.\n",
        "\n",
        "    Args:\n",
        "        rl_pairs (list): A list of tuples, each containing (dataset_name, model_id, config).\n",
        "        agents (list): A list of FineTuningAgent objects corresponding to the experiments.\n",
        "        training_args_list (list): A list of TrainingArguments objects for each experiment.\n",
        "        state_list (list): A list of TrainerState objects for each experiment.\n",
        "        control_list (list): A list of TrainerControl objects for each experiment.\n",
        "        output_file (str): The name of the output file to save the report.\n",
        "    \"\"\"\n",
        "    if not (\n",
        "        len(rl_pairs)\n",
        "        == len(agents)\n",
        "        == len(training_args_list)\n",
        "        == len(state_list)\n",
        "        == len(control_list)\n",
        "    ):\n",
        "        raise ValueError(\"The number of rl_pairs, agents, training_args, state, and control must be the same.\")\n",
        "\n",
        "    report_data = []\n",
        "    for (dataset_name, model_id, config), agent, training_args, state, control in zip(\n",
        "        rl_pairs, agents, training_args_list, state_list, control_list\n",
        "    ):\n",
        "        # Collect the data\n",
        "        if agent.start_time is None or agent.end_time is None:\n",
        "            raise ValueError(\"Start time or end time is not defined.\")\n",
        "        elapsed_time = agent.end_time - agent.start_time\n",
        "        train_losses = agent.train_losses\n",
        "        eval_losses = agent.eval_losses\n",
        "\n",
        "        if not train_losses:\n",
        "            train_std = np.nan  # Use np.nan for no data\n",
        "            min_train_loss = np.nan\n",
        "            max_train_loss = np.nan\n",
        "        else:\n",
        "            train_std = np.std(train_losses)\n",
        "            min_train_loss = np.min(train_losses)\n",
        "            max_train_loss = np.max(train_losses)\n",
        "\n",
        "        if not eval_losses:\n",
        "            eval_std = np.nan\n",
        "            min_eval_loss = np.nan\n",
        "            max_eval_loss = np.nan\n",
        "        else:\n",
        "            eval_std = np.std(eval_losses)\n",
        "            min_eval_loss = np.min(eval_losses)\n",
        "            max_eval_loss = np.max(eval_losses)\n",
        "\n",
        "        # Collect the metrics.\n",
        "        if agent.evaluation_results is not None:\n",
        "            bleu_score = agent.evaluation_results.get(\"eval_bleu\", np.nan)\n",
        "            f1_score = agent.evaluation_results.get(\"eval_f1\", np.nan)\n",
        "        else:\n",
        "            bleu_score = np.nan\n",
        "            f1_score = np.nan\n",
        "\n",
        "        report_data.append(\n",
        "            [\n",
        "                dataset_name,\n",
        "                model_id,\n",
        "                f\"{elapsed_time:.2f} seconds\",  # Format to 2 decimal places\n",
        "                f\"{train_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{eval_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{bleu_score:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{f1_score:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{training_args.learning_rate:.4f}\",  # Learning rate\n",
        "                training_args.per_device_train_batch_size,  # Batch size\n",
        "                training_args.num_train_epochs if hasattr(training_args,\"num_train_epochs\") else \"n/a\", # Epochs\n",
        "                state.global_step,  # Global steps\n",
        "                state.epoch,  # Epoch\n",
        "                state.is_local_process_zero,\n",
        "                control.should_training_stop,\n",
        "                control.should_log,\n",
        "                control.should_save,\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    headers = [\n",
        "        \"Dataset\",\n",
        "        \"Model\",\n",
        "        \"Elapsed Time\",\n",
        "        \"Train Loss Std\",\n",
        "        \"Eval Loss Std\",\n",
        "        \"Min Train Loss\",\n",
        "        \"Max Train Loss\",\n",
        "        \"Min Eval Loss\",\n",
        "        \"Max Eval Loss\",\n",
        "        \"BLEU Score\",\n",
        "        \"F1 Score\",\n",
        "        \"Learning Rate\",\n",
        "        \"Batch Size\",\n",
        "        \"Epochs\",\n",
        "        \"Global Steps\",\n",
        "        \"Epoch\",\n",
        "        \"Is Local Process Zero\",\n",
        "        \"Should Training Stop\",\n",
        "        \"Should Log\",\n",
        "        \"Should Save\",\n",
        "    ]\n",
        "\n",
        "    # Format the report as a table\n",
        "    report_table = tabulate(report_data, headers=headers, tablefmt=\"grid\")\n",
        "\n",
        "    # Print the report to the console\n",
        "    print(report_table)\n",
        "\n",
        "    # Save the report to a file\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(report_table)\n",
        "        print(f\"Report saved to {output_file}\")"
      ],
      "metadata": {
        "id": "KFmsCZrABMQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## llm report"
      ],
      "metadata": {
        "id": "BRmauENT7tWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Used to securely store your API key\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI')  # Replace 'GEMINI' with your actual userdata variable name\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "from tabulate import tabulate\n",
        "from transformers import TrainingArguments, TrainerState, TrainerControl\n",
        "\n",
        "def generate_llm_report(\n",
        "    rl_pairs,\n",
        "    agents,\n",
        "    training_args_list,\n",
        "    state_list,\n",
        "    control_list,\n",
        "    output_file=\"experiment_report.txt\",\n",
        "    experiment_name=None,\n",
        "    prompt=\"You are a helpful data science expert.\\nPlease, make an additional analysis of this Fine-Tuning experiment report.\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a report for multiple LLM experiments, including evaluation scores and training details,\n",
        "    and provides an analysis using Google Gemini.\n",
        "\n",
        "    Args:\n",
        "        rl_pairs (list): A list of tuples, each containing (dataset_name, model_id, config).\n",
        "        agents (list): A list of FineTuningAgent objects corresponding to the experiments.\n",
        "        training_args_list (list): A list of TrainingArguments objects for each experiment.\n",
        "        state_list (list): A list of TrainerState objects for each experiment.\n",
        "        control_list (list): A list of TrainerControl objects for each experiment.\n",
        "        output_file (str): The name of the output file to save the report.\n",
        "        experiment_name (str, optional): The base name for the experiment results file.\n",
        "                                        If provided, it will be used to load the results. Defaults to None.\n",
        "        prompt (str, optional): The prompt to provide to Google Gemini for analysis.\n",
        "                                Defaults to a generic data science expert prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    if not (\n",
        "        len(rl_pairs)\n",
        "        == len(agents)\n",
        "        == len(training_args_list)\n",
        "        == len(state_list)\n",
        "        == len(control_list)\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"The number of rl_pairs, agents, training_args, state, and control must be the same.\"\n",
        "        )\n",
        "\n",
        "    report_data = []  # Initialize report_data here\n",
        "\n",
        "    for (\n",
        "        (dataset_name, model_id, config),\n",
        "        agent,\n",
        "        training_args,\n",
        "        state,\n",
        "        control,\n",
        "    ) in zip(rl_pairs, agents, training_args_list, state_list, control_list):\n",
        "        # Get eval_results from the agent\n",
        "\n",
        "        experiment_name = f\"\"\"{model_id.replace('/', '-').replace(\"'\", '')}_{dataset_name.replace('/', '-').replace(\"'\", '')}\"\"\"\n",
        "        #print(f\"Experiment Name: {experiment_name}\")\n",
        "\n",
        "        results_file = f\"{experiment_name}_results.txt\"\n",
        "        print(f\"Results File: {results_file}\")\n",
        "\n",
        "\n",
        "        # \"eval_loss\": 6.17133903503418, \"eval_bleu\": 0, \"eval_f1\": 0.0, \"eval_runtime\": 4.0188, \"eval_samples_per_second\": 6.221, \"eval_steps_per_second\": 0.995, \"epoch\": 8.64}\n",
        "\n",
        "        try:\n",
        "            with open(results_file, \"r\") as f:\n",
        "                eval_results = json.load(f)\n",
        "            bleu_score = eval_results.get(\"eval_bleu\")\n",
        "            f1_score = eval_results.get(\"eval_f1\")\n",
        "            print(f\"BLEU Score: {bleu_score}, F1 Score: {f1_score}\")\n",
        "            print(f\"Eval Results: {eval_results}\")\n",
        "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "            print(f\"Error loading results: {e}\")\n",
        "            bleu_score = None\n",
        "            f1_score = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Collect the data\n",
        "        elapsed_time = (\n",
        "            agent.end_time - agent.start_time\n",
        "            if agent.start_time and agent.end_time\n",
        "            else np.nan\n",
        "        )  # Handle potential errors\n",
        "        train_losses = agent.train_losses\n",
        "        eval_losses = agent.eval_losses\n",
        "\n",
        "        if not train_losses:\n",
        "            train_std = np.nan  # Use np.nan for no data\n",
        "            min_train_loss = np.nan\n",
        "            max_train_loss = np.nan\n",
        "        else:\n",
        "            train_std = np.std(train_losses)\n",
        "            min_train_loss = np.min(train_losses)\n",
        "            max_train_loss = np.max(train_losses)\n",
        "\n",
        "        if not eval_losses:\n",
        "            eval_std = np.nan\n",
        "            min_eval_loss = np.nan\n",
        "            max_eval_loss = np.nan\n",
        "        else:\n",
        "            eval_std = np.std(eval_losses)\n",
        "            min_eval_loss = np.min(eval_losses)\n",
        "            max_eval_loss = np.max(eval_losses)\n",
        "\n",
        "        # Check if training_args is None before accessing its attributes\n",
        "        learning_rate = training_args.learning_rate if training_args is not None else np.nan\n",
        "        batch_size = training_args.per_device_train_batch_size if training_args is not None else np.nan\n",
        "        epochs = training_args.num_train_epochs if training_args is not None and hasattr(training_args, \"num_train_epochs\") else \"n/a\"\n",
        "\n",
        "        report_data.append(\n",
        "            [\n",
        "                dataset_name,\n",
        "                model_id,\n",
        "                f\"{elapsed_time:.2f} seconds\",  # Format to 2 decimal places\n",
        "                f\"{train_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{eval_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "\n",
        "\n",
        "                f\"{bleu_score:.4f}\" if bleu_score is not None else \"N/A\",  # Handle None case for bleu_score\n",
        "                f\"{f1_score:.4f}\" if f1_score is not None else \"N/A\",  # Handle None case for f1_score\n",
        "\n",
        "\n",
        "                f\"{learning_rate:.4f}\",  # Learning rate\n",
        "                batch_size,  # Batch size\n",
        "                epochs,  # Epochs\n",
        "                state.global_step if state else \"n/a\",  # Global steps\n",
        "                state.epoch if state else \"n/a\",  # Epoch\n",
        "                state.is_local_process_zero if state else \"n/a\",\n",
        "                control.should_training_stop if control else \"n/a\",\n",
        "                control.should_log if control else \"n/a\",\n",
        "                control.should_save if control else \"n/a\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # Generate the report table\n",
        "    headers = [\n",
        "        \"Dataset\",\n",
        "        \"Model\",\n",
        "        \"Elapsed Time\",\n",
        "        \"Train Loss Std\",\n",
        "        \"Eval Loss Std\",\n",
        "        \"Min Train Loss\",\n",
        "        \"Max Train Loss\",\n",
        "        \"Min Eval Loss\",\n",
        "        \"Max Eval Loss\",\n",
        "        \"BLEU Score\",\n",
        "        \"F1 Score\",\n",
        "        \"Learning Rate\",\n",
        "        \"Batch Size\",\n",
        "        \"Epochs\",\n",
        "        \"Global Steps\",\n",
        "        \"Epoch\",\n",
        "        \"is_local_process_zero\",\n",
        "        \"should_training_stop\",\n",
        "        \"should_log\",\n",
        "        \"should_save\",\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    report_table = tabulate(report_data, headers=headers, tablefmt=\"grid\")\n",
        "\n",
        "    # Save the report to a file\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(report_table)\n",
        "\n",
        "    print(report_table)\n",
        "\n",
        "    # LLM Analysis using Google Gemini\n",
        "    model_name = \"gemini-1.5-pro\"  # Replace with desired model\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "    response = model.generate_content(prompt + \"\\n\\n\" + report_table)\n",
        "    llm_analysis = response.text\n",
        "\n",
        "    print(\"\\n\\n## LLM Analysis:\\n\")\n",
        "    print(llm_analysis)\n",
        "\n",
        "    return llm_analysis\n"
      ],
      "metadata": {
        "id": "QPJ0q0OvrIzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have rl_pairs and agents defined and populated\n",
        "\n",
        "# Define the prompt for the LLM\n",
        "prompt = \"\"\"\n",
        "You are a helpful data science expert.\n",
        "Please, make an additional analysis of this Fine-Tuning experiment report.\n",
        "\"\"\"\n",
        "\n",
        "# Generate training_args_list, state_list, and control_list\n",
        "# These lists should be the same length as rl_pairs and agents, and filled with appropriate data\n",
        "training_args_list = [agent.training_args for agent in agents]\n",
        "state_list = [agent.trainer.state for agent in agents]\n",
        "control_list = [agent.trainer.control for agent in agents]\n",
        "\n",
        "\n",
        "# Call the function, optionally providing an output file name\n",
        "# Instead of passing \"prompt\", make sure to pass the training args, state, and control lists.\n",
        "report_text = generate_llm_report(rl_pairs, agents, training_args_list, state_list, control_list, output_file=\"my_experiment_report.txt\")\n",
        "\n",
        "# You can then further process or print the report_text if needed\n",
        "print(report_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xebrWwS-kcWw",
        "outputId": "1e26c8d8-9d1f-492f-b2c9-897bbdbd76e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results File: deepseek-ai-deepseek-coder-1.3b-base_SetFit-mrpc_results.txt\n",
            "Error loading results: [Errno 2] No such file or directory: 'deepseek-ai-deepseek-coder-1.3b-base_SetFit-mrpc_results.txt'\n",
            "Results File: deepseek-ai-deepseek-coder-1.3b-base_b-mc2-sql-create-context_results.txt\n",
            "BLEU Score: 0, F1 Score: 0.0\n",
            "Eval Results: {'eval_loss': 3.5057103633880615, 'eval_bleu': 0, 'eval_f1': 0.0, 'eval_runtime': 12.0847, 'eval_samples_per_second': 20.687, 'eval_steps_per_second': 2.648, 'epoch': 0.96}\n",
            "Results File: deepseek-ai-deepseek-coder-1.3b-base_anthropic-hh-rlhf_results.txt\n",
            "BLEU Score: 0, F1 Score: 0.0\n",
            "Eval Results: {'eval_loss': 2.0577316284179688, 'eval_bleu': 0, 'eval_f1': 0.0, 'eval_runtime': 12.1602, 'eval_samples_per_second': 20.559, 'eval_steps_per_second': 2.632, 'epoch': 0.96}\n",
            "Results File: deepseek-ai-deepseek-coder-1.3b-base_imdb_results.txt\n",
            "BLEU Score: 0, F1 Score: 0.0\n",
            "Eval Results: {'eval_loss': 3.392622232437134, 'eval_bleu': 0, 'eval_f1': 0.0, 'eval_runtime': 12.226, 'eval_samples_per_second': 20.448, 'eval_steps_per_second': 2.617, 'epoch': 0.96}\n",
            "+--------------------------+--------------------------------------+----------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+--------------+------------+-----------------+--------------+----------+----------------+---------+-------------------------+------------------------+--------------+---------------+\n",
            "| Dataset                  | Model                                | Elapsed Time   |   Train Loss Std |   Eval Loss Std |   Min Train Loss |   Max Train Loss |   Min Eval Loss |   Max Eval Loss | BLEU Score   | F1 Score   |   Learning Rate |   Batch Size |   Epochs |   Global Steps |   Epoch | is_local_process_zero   | should_training_stop   | should_log   | should_save   |\n",
            "+==========================+======================================+================+==================+=================+==================+==================+=================+=================+==============+============+=================+==============+==========+================+=========+=========================+========================+==============+===============+\n",
            "| SetFit/mrpc              | deepseek-ai/deepseek-coder-1.3b-base | -0.00 seconds  |         nan      |        nan      |         nan      |         nan      |        nan      |        nan      | N/A          | N/A        |          0.0002 |            4 |        3 |              0 |         | True                    | False                  | False        | False         |\n",
            "+--------------------------+--------------------------------------+----------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+--------------+------------+-----------------+--------------+----------+----------------+---------+-------------------------+------------------------+--------------+---------------+\n",
            "| b-mc2/sql-create-context | deepseek-ai/deepseek-coder-1.3b-base | 256.75 seconds |           1.5398 |          0.2239 |           3.5918 |           7.9724 |          3.5057 |          4.0497 | 0.0000       | 0.0000     |          0.0002 |            4 |        3 |             60 |    0.96 | True                    | True                   | False        | False         |\n",
            "+--------------------------+--------------------------------------+----------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+--------------+------------+-----------------+--------------+----------+----------------+---------+-------------------------+------------------------+--------------+---------------+\n",
            "| anthropic/hh-rlhf        | deepseek-ai/deepseek-coder-1.3b-base | 258.20 seconds |           0.1105 |          0.0185 |           1.9784 |           2.3129 |          2.0577 |          2.1029 | 0.0000       | 0.0000     |          0.0002 |            4 |        3 |             60 |    0.96 | True                    | True                   | False        | False         |\n",
            "+--------------------------+--------------------------------------+----------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+--------------+------------+-----------------+--------------+----------+----------------+---------+-------------------------+------------------------+--------------+---------------+\n",
            "| imdb                     | deepseek-ai/deepseek-coder-1.3b-base | 261.82 seconds |           0.0762 |          0.0126 |           3.3673 |           3.6045 |          3.3926 |          3.4232 | 0.0000       | 0.0000     |          0.0002 |            4 |        3 |             60 |    0.96 | True                    | True                   | False        | False         |\n",
            "+--------------------------+--------------------------------------+----------------+------------------+-----------------+------------------+------------------+-----------------+-----------------+--------------+------------+-----------------+--------------+----------+----------------+---------+-------------------------+------------------------+--------------+---------------+\n",
            "\n",
            "\n",
            "## LLM Analysis:\n",
            "\n",
            "This report summarizes a fine-tuning experiment for the `deepseek-ai/deepseek-coder-1.3b-base` language model across four different datasets. Here's a breakdown of the key observations and potential issues:\n",
            "\n",
            "**Key Observations:**\n",
            "\n",
            "* **Inconsistent Results:** The most striking observation is the inconsistency across datasets.  `SetFit/mrpc` shows `NaN` (Not a Number) for all metrics, indicating the training failed completely for this dataset.  The other three datasets (`b-mc2/sql-create-context`, `anthropic/hh-rlhf`, and `imdb`) completed training, but show very low BLEU and F1 scores (0.0000).  This suggests the model isn't effectively learning from these datasets either.\n",
            "* **Potential Overfitting (b-mc2/sql-create-context):** For the `b-mc2/sql-create-context` dataset, the difference between the minimum and maximum training loss is quite large (7.9724 - 3.5918 = 4.3806), while the standard deviation is relatively high (1.5398).  This, coupled with a smaller difference but still significant variation in evaluation loss (4.0497 - 3.5057 = 0.544), suggests potential overfitting.  The model might be memorizing the training data rather than generalizing well to unseen examples.\n",
            "* **Possible Underfitting (anthropic/hh-rlhf and imdb):** For the `anthropic/hh-rlhf` and `imdb` datasets, the training and evaluation losses are relatively stable (small standard deviations), but the losses themselves are quite high.  This, combined with zero BLEU and F1 scores, points towards potential underfitting. The model might not be complex enough to capture the underlying patterns in these datasets, or the training process might not be sufficient.\n",
            "* **Limited Training:** All datasets that ran completed in roughly the same amount of time (~260 seconds), with only 0.96 epochs completed out of the target 3 epochs. This limited training likely contributes to the poor performance. The `should_training_stop` flag being `True` indicates some early stopping criterion was met, which needs further investigation.\n",
            "* **Hyperparameters:**  The hyperparameters (learning rate, batch size, epochs) are consistent across all runs.  It's possible these hyperparameters are not optimal for all datasets.\n",
            "\n",
            "**Further Analysis and Recommendations:**\n",
            "\n",
            "1. **Investigate `SetFit/mrpc` Failure:**  Determine the root cause of the `NaN` values for the `SetFit/mrpc` dataset.  Check for data loading issues, preprocessing errors, or potential bugs in the training code specific to this dataset.\n",
            "2. **Address Overfitting (b-mc2/sql-create-context):**  Try regularization techniques like dropout, weight decay, or early stopping (though it seems some form of early stopping might already be in place and needs review).  Also, consider increasing the training data size if possible.\n",
            "3. **Address Underfitting (anthropic/hh-rlhf and imdb):**  Experiment with a larger model, a more complex architecture, or a longer training duration.  Tuning the learning rate and batch size could also help.\n",
            "4. **Evaluate Early Stopping:**  Understand why the training stopped early (`should_training_stop = True`).  Check the early stopping criteria and determine if they are appropriate for these tasks.  Consider disabling early stopping or adjusting its parameters to allow for longer training.\n",
            "5. **Hyperparameter Tuning:**  Perform a systematic hyperparameter search (e.g., grid search or Bayesian optimization) to find the optimal learning rate, batch size, and other relevant hyperparameters for each dataset.\n",
            "6. **Data Analysis:** Analyze the characteristics of each dataset.  Understanding the data distribution, size, and complexity can provide insights into the model's performance and inform further adjustments.\n",
            "7. **Metric Selection:** Consider using additional metrics relevant to the specific tasks to get a more comprehensive evaluation of the model's performance.  For example, for text generation, metrics like ROUGE or METEOR might be more informative than just BLEU.\n",
            "\n",
            "\n",
            "By addressing these points, you should be able to gain a better understanding of the model's behavior and improve its performance on these datasets.\n",
            "\n",
            "This report summarizes a fine-tuning experiment for the `deepseek-ai/deepseek-coder-1.3b-base` language model across four different datasets. Here's a breakdown of the key observations and potential issues:\n",
            "\n",
            "**Key Observations:**\n",
            "\n",
            "* **Inconsistent Results:** The most striking observation is the inconsistency across datasets.  `SetFit/mrpc` shows `NaN` (Not a Number) for all metrics, indicating the training failed completely for this dataset.  The other three datasets (`b-mc2/sql-create-context`, `anthropic/hh-rlhf`, and `imdb`) completed training, but show very low BLEU and F1 scores (0.0000).  This suggests the model isn't effectively learning from these datasets either.\n",
            "* **Potential Overfitting (b-mc2/sql-create-context):** For the `b-mc2/sql-create-context` dataset, the difference between the minimum and maximum training loss is quite large (7.9724 - 3.5918 = 4.3806), while the standard deviation is relatively high (1.5398).  This, coupled with a smaller difference but still significant variation in evaluation loss (4.0497 - 3.5057 = 0.544), suggests potential overfitting.  The model might be memorizing the training data rather than generalizing well to unseen examples.\n",
            "* **Possible Underfitting (anthropic/hh-rlhf and imdb):** For the `anthropic/hh-rlhf` and `imdb` datasets, the training and evaluation losses are relatively stable (small standard deviations), but the losses themselves are quite high.  This, combined with zero BLEU and F1 scores, points towards potential underfitting. The model might not be complex enough to capture the underlying patterns in these datasets, or the training process might not be sufficient.\n",
            "* **Limited Training:** All datasets that ran completed in roughly the same amount of time (~260 seconds), with only 0.96 epochs completed out of the target 3 epochs. This limited training likely contributes to the poor performance. The `should_training_stop` flag being `True` indicates some early stopping criterion was met, which needs further investigation.\n",
            "* **Hyperparameters:**  The hyperparameters (learning rate, batch size, epochs) are consistent across all runs.  It's possible these hyperparameters are not optimal for all datasets.\n",
            "\n",
            "**Further Analysis and Recommendations:**\n",
            "\n",
            "1. **Investigate `SetFit/mrpc` Failure:**  Determine the root cause of the `NaN` values for the `SetFit/mrpc` dataset.  Check for data loading issues, preprocessing errors, or potential bugs in the training code specific to this dataset.\n",
            "2. **Address Overfitting (b-mc2/sql-create-context):**  Try regularization techniques like dropout, weight decay, or early stopping (though it seems some form of early stopping might already be in place and needs review).  Also, consider increasing the training data size if possible.\n",
            "3. **Address Underfitting (anthropic/hh-rlhf and imdb):**  Experiment with a larger model, a more complex architecture, or a longer training duration.  Tuning the learning rate and batch size could also help.\n",
            "4. **Evaluate Early Stopping:**  Understand why the training stopped early (`should_training_stop = True`).  Check the early stopping criteria and determine if they are appropriate for these tasks.  Consider disabling early stopping or adjusting its parameters to allow for longer training.\n",
            "5. **Hyperparameter Tuning:**  Perform a systematic hyperparameter search (e.g., grid search or Bayesian optimization) to find the optimal learning rate, batch size, and other relevant hyperparameters for each dataset.\n",
            "6. **Data Analysis:** Analyze the characteristics of each dataset.  Understanding the data distribution, size, and complexity can provide insights into the model's performance and inform further adjustments.\n",
            "7. **Metric Selection:** Consider using additional metrics relevant to the specific tasks to get a more comprehensive evaluation of the model's performance.  For example, for text generation, metrics like ROUGE or METEOR might be more informative than just BLEU.\n",
            "\n",
            "\n",
            "By addressing these points, you should be able to gain a better understanding of the model's behavior and improve its performance on these datasets.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}