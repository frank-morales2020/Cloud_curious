{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/FineTuning_LLM_Meta_Llama_3_8B_for_MEDAL_APRIL2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3kFUczO5Um3"
      },
      "source": [
        "https://llama.meta.com/docs/how-to-guides/fine-tuning/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etm0HfcZM151"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install datasets trl ninja packaging --quiet\n",
        "\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "#! pip install trl==0.8.6 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65JpttRiGxVK"
      },
      "outputs": [],
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "37yP0eJDM152"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write, # ADD YOUR TOKEN HERE\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S9qvxG2HYusD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "naQrZIQTY1wG"
      },
      "outputs": [],
      "source": [
        "# set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KtE9TuSMlUEc",
        "outputId": "fa17e6cb-efcf-45a4-ba58-1b1d0baaa995"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.6.0+cu124'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GapRov3hl4fT"
      },
      "outputs": [],
      "source": [
        "!python --version\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-7fKxR2rnbJ"
      },
      "outputs": [],
      "source": [
        "### conversational format\n",
        "{\"messages\": [{\"role\": \"system\", \"content\": \"You are...\"}, {\"role\": \"user\", \"content\": \"...\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
        "\n",
        "### instruction format\n",
        "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCd7-R-lsLQm"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# train dataset\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/train_dataset.json\", split=\"train\")\n",
        "\n",
        "#### eval dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-kFtkh8kiOn"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "id": "2Ho5b0cdmMmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahaI2KNwYcQ5"
      },
      "outputs": [],
      "source": [
        "print(dataset[345]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMbtvPqFkyFd"
      },
      "outputs": [],
      "source": [
        "print(dataset[345]['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "l0YsbCarv94g"
      },
      "outputs": [],
      "source": [
        "train_question=dataset['text']\n",
        "train_answer=dataset['label']\n",
        "\n",
        "eval_question=eval_dataset['text']\n",
        "eval_answer=eval_dataset['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uZ2S19rIwMww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4edd010-e06b-481b-e71c-31abaca02a6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train data: 100%|██████████| 10000/10000 [00:00<00:00, 37791.12it/s]\n",
            "Processing eval data: 100%|██████████| 2000/2000 [00:00<00:00, 38030.98it/s]\n"
          ]
        }
      ],
      "source": [
        "!rm -rf /content/medal-qa-train.txt\n",
        "!rm -rf /content/medal-qa-eval.txt\n",
        "filename_train='/content/medal-qa-train.txt'\n",
        "filename_eval='/content/medal-qa-eval.txt'\n",
        "\n",
        "import re\n",
        "from tqdm import tqdm  # Import tqdm\n",
        "\n",
        "n_samples_train=10000\n",
        "n_samples_eval=2000\n",
        "\n",
        "\n",
        "# Processing train data\n",
        "for n in tqdm(range(n_samples_train), desc=\"Processing train data\"):  # Wrap range with tqdm\n",
        "    if train_answer[n] == None:\n",
        "       train_answer[n] = 'Not possible to get or use'\n",
        "    string = train_answer[n]\n",
        "    str_question=train_question[n]\n",
        "    question= re.sub(\"\\n\", \"\", str_question)\n",
        "    answer = re.sub(\"\\[\", \"\", str(string))\n",
        "    answer = re.sub(\"\\]\", \"\", str(answer))\n",
        "    answer0=answer[1:len(answer)-1]\n",
        "\n",
        "    if len(answer0)==0:\n",
        "        answer='Not possible to get or use'\n",
        "\n",
        "\n",
        "    text='<s>[INST] %s [/INST] %s </s>'%(question,answer0)\n",
        "    #print(text)\n",
        "    with open(filename_train, 'a') as f:\n",
        "        f.write(text + '\\n')\n",
        "\n",
        "# Processing eval data\n",
        "for n in tqdm(range(n_samples_eval), desc=\"Processing eval data\"):  # Wrap range with tqdm\n",
        "    if eval_answer[n] == None:\n",
        "       eval_answer[n] = 'Not possible to get or use'\n",
        "    string = eval_answer[n]\n",
        "    str_question=eval_question[n]\n",
        "    question= re.sub(\"\\n\", \"\", str_question)\n",
        "    answer = re.sub(\"\\[\", \"\", str(string))\n",
        "    answer = re.sub(\"\\]\", \"\", str(answer))\n",
        "    answer0=answer[1:len(answer)-1]\n",
        "\n",
        "    if len(answer0)==0:\n",
        "        answer='Not possible to get or use'\n",
        "\n",
        "\n",
        "    text='<s>[INST] %s [/INST] %s </s>'%(question,answer0)\n",
        "\n",
        "    with open(filename_eval, 'a') as f:\n",
        "        f.write(text + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZeUHqjcBwO4a",
        "outputId": "1fa7cb94-7c03-4b42-823f-3d3194c05500"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'to quantify the amount of CL NO harvested in modified RND'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "train_question[6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOLATL1WwWLF",
        "outputId": "6d20231b-467a-4e1b-c482-2bc22efc61c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['radical neck dissection']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "train_answer[6]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_question[100]"
      ],
      "metadata": {
        "id": "1nYYxqTOuc_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_answer[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj7QLjEDuiRS",
        "outputId": "d6fd72c7-92ce-44bc-ac63-45849dcdd7aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['childhood brain tumors']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhg0zGbIwfK1"
      },
      "outputs": [],
      "source": [
        "text0_train = load_dataset(\"text\", data_files=\"/content/medal-qa-train.txt\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onW6yYEdzfBe"
      },
      "outputs": [],
      "source": [
        "text0_train[6]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text0_eval = load_dataset(\"text\", data_files=\"/content/medal-qa-eval.txt\", split=\"train\")"
      ],
      "metadata": {
        "id": "r9OoNdE8vtcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text0_eval[100]"
      ],
      "metadata": {
        "id": "_-qrnu7Sv2KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZDbr5KCpwk9q"
      },
      "outputs": [],
      "source": [
        "nrec=n_samples_train\n",
        "\n",
        "dataset_final_Question=dataset['text'][0:nrec]\n",
        "dataset_final_Answer=dataset['label'][0:nrec]\n",
        "dataset_final_text=text0_train[0:nrec]['text']\n",
        "\n",
        "# EVAL\n",
        "nrec=n_samples_eval\n",
        "\n",
        "eval_dataset_final_Question=eval_dataset['text'][0:nrec]\n",
        "eval_dataset_final_Answer=eval_dataset['label'][0:nrec]\n",
        "eval_dataset_final_text=text0_eval[0:nrec]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "46EcsGDQwrNY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "datasetF = pd.DataFrame() # Create an empty DataFrame\n",
        "datasetF['Question'] = dataset_final_Question\n",
        "datasetF['Answer'] = dataset_final_Answer\n",
        "datasetF['text'] = dataset_final_text\n",
        "\n",
        "datasetF_eval = pd.DataFrame() # Create an empty DataFrame\n",
        "datasetF_eval['Question'] = eval_dataset_final_Question\n",
        "datasetF_eval['Answer'] = eval_dataset_final_Answer\n",
        "datasetF_eval['text'] = eval_dataset_final_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SabZFKG5wxUA"
      },
      "outputs": [],
      "source": [
        "datasetF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PbJ9mfLtxxD5"
      },
      "outputs": [],
      "source": [
        "datasetF['text'][6]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetF_eval"
      ],
      "metadata": {
        "id": "0d3jer5ClvIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetF_eval['text'][100]"
      ],
      "metadata": {
        "id": "eEuTZVzcwtVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQm2yo0uslTs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from trl import setup_chat_format\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxOWr2VvEGKe"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "kjyYx9WmtMmA"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
        "peft_config = LoraConfig(\n",
        "        lora_alpha=128,\n",
        "        lora_dropout=0.05,\n",
        "        r=256,\n",
        "        bias=\"none\",\n",
        "        target_modules=\"all-linear\",\n",
        "        #[q_proj k_proj): v_proj o_proj)\n",
        "        task_type=\"CAUSAL_LM\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "AkMVebosM16A"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "\n",
        "    output_dir=\"/content/gdrive/MyDrive/model/2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine\",\n",
        "\n",
        "    num_train_epochs=1,                    # number of training epochs for POC\n",
        "    per_device_train_batch_size=3,          # batch size per device during training\n",
        "    #2\n",
        "    gradient_accumulation_steps=6,          # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
        "    #gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
        "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
        "\n",
        "    #trainer = Trainer(model=model, args=training_args, train_dataset=ds, optimizers=(adam_bnb_optim, None))\n",
        "    logging_steps=10,                       # log every 10 steps\n",
        "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
        "\n",
        "    learning_rate=2e-6,                     # learning rate, based on QLoRA paper # i used in the first model\n",
        "\n",
        "    #learning_rate=1.41e-5,\n",
        "    bf16=True,                              # use bfloat16 precision\n",
        "    tf32=True,                              # use tf32 precision\n",
        "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper = 0.03\n",
        "\n",
        "    #lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
        "    lr_scheduler_type=\"cosine\",            # lr_scheduler_type=\"cosine\" (Cosine Annealing Learning Rate)\n",
        "\n",
        "\n",
        "    push_to_hub=True,                       # push model to hub\n",
        "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
        "\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    #force_download=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "wd03Af2od_qT"
      },
      "outputs": [],
      "source": [
        "model.config.use_cache=False\n",
        "model.gradient_checkpointing_enable() #enable gradient checkpoint\n",
        "#torch.utils.checkpoint.checkpoint(use_reentrant=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6Yk7MlowDT-"
      },
      "source": [
        "https://huggingface.co/docs/trl/main/en/sft_trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "we8LmPrO_AG-"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "import datasets\n",
        "\n",
        "# Convert Pandas DataFrame to Hugging Face Dataset\n",
        "datasetF_hf = datasets.Dataset.from_pandas(datasetF)\n",
        "\n",
        "datasetF_hf_eval = datasets.Dataset.from_pandas(datasetF_eval)\n",
        "\n",
        "max_seq_length = 3072 # max sequence length for model and packing of the dataset\n",
        "\n",
        "# Explicitly set the device using torch.device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Add a padding token to your tokenizer\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Or, add a new padding token\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=datasetF_hf,\n",
        "    eval_dataset=datasetF_hf_eval,\n",
        "    peft_config=peft_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nuyiFKmWfvU"
      },
      "source": [
        "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCq7Dq8f7VPU"
      },
      "outputs": [],
      "source": [
        "# start training, the model will be automatically saved to the hub and the output directory\n",
        "trainer.train()\n",
        "\n",
        "# save model\n",
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "mCxeJIj4KvSQ"
      },
      "outputs": [],
      "source": [
        "# free the memory again\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf /content/tensorbard\n",
        "%mkdir -p /content/tensorbard\n",
        "%cd /content/tensorbard\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/frankmorales2020/2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9Crz4SP0yjN",
        "outputId": "05a3d1b0-1653-4df1-816d-0c3c30896b15"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "/content/tensorbard\n",
            "Git LFS initialized.\n",
            "Cloning into '2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 26 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (26/26), 8.02 KiB | 1.00 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/tensorbard/2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine/runs"
      ],
      "metadata": {
        "id": "tu8gDvN-06nR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjTwqcXmK_OR"
      },
      "source": [
        "Test Model and run Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install datasets trl ninja packaging --quiet"
      ],
      "metadata": {
        "id": "bUOeRY5oEVXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "import colab_env"
      ],
      "metadata": {
        "id": "fVLJwcDME3Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW_hl8YPKxQ2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "\n",
        "# 2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine\n",
        "peft_model_id = \"/content/gdrive/MyDrive/model/2025-Meta-Llama-3-8B-MEDAL-flash-attention-2-cosine\"\n",
        "\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    peft_model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1DyOCUfLLn4"
      },
      "source": [
        "Let’s load our test dataset try to generate an instruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBZbeEq-R_ti"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uHtGXcrLKI_",
        "outputId": "2bb595a8-be81-439c-fe14-f747fd376714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from random import randint\n",
        "\n",
        "\n",
        "# Load our test dataset\n",
        "eval_dataset = load_dataset(\"json\", data_files=\"/content/gdrive/MyDrive/datasets/McGill-NLP/test_dataset.json\", split=\"train\")\n",
        "nrec= randint(0, len(eval_dataset))\n",
        "nrec=600\n",
        "\n",
        "# Test on sample\n",
        "generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # Add device_map\n",
        "prompt =  eval_dataset[nrec]['text']\n",
        "\n",
        "\n",
        "outputs = generation_pipeline(prompt, max_new_tokens=128, do_sample=True, temperature=0.9,\n",
        "                                  top_k=30, top_p=0.1, eos_token_id=tokenizer.eos_token_id,\n",
        "                                  pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GZg3t7d1z9s"
      },
      "outputs": [],
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "ganswer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q2zlGDkw1rE",
        "outputId": "394fe4b3-555e-46cc-91f9-3f093157d294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query:\n",
            "the CTS rhythmically controls SMB physiology cellular proliferation and xenobiotic metabolism over the h period the SCN in the hypothalamus coordinate the molecular clocks in most rat cells through an array of circadian physiological rhythms including restactivity body temperature feeding patterns and hormonal secretions as a result shift work that involves circadian disruption is probably carcinogenic in humans in experimental models chronic jetlag cjl suppresses restactivity and body temperature rhythms and accelerates growth of two transplantable tumors in mice cjl also suppresses or significantly alters the expression rhythms of clock genes in liver and PT circadian clock disruption from cjl downregulates p and upregulates cmyc thus favoring cellular proliferation here we investigate the role of cjl as a tumor promoter in mice exposed to the hepatic carcinogen diethylnitrosamine den\n",
            "\n",
            "Original Answer:\n",
            "circadian timing system\n",
            "\n",
            "Generated Answer:\n",
            "that cjl accelerates deninduced hepatocarcinogenesis and that this effect is associated with a significant increase in the number of hepatocytes in s phase and a decrease in the number of hepatocytes in g phase in addition cjl increases the expression of cmyc and decreases the expression of p in liver and PT we conclude that cjl accelerates deninduced hepatocarcinogenesis by disrupting the circadian expression of clock genes and by altering the expression of cmyc and p in liv\n",
            "\n",
            "NO Match\n"
          ]
        }
      ],
      "source": [
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print()\n",
        "oanswer=str(eval_dataset[nrec]['label'])\n",
        "oanswer=oanswer[2:len(oanswer)-2]\n",
        "print(f\"Original Answer:\\n{oanswer}\")\n",
        "print()\n",
        "ganswer=outputs[0]['generated_text'][len(prompt)+9:].strip()\n",
        "qc=str(ganswer).find('[INST]')\n",
        "ganswer=ganswer[0:qc-7]\n",
        "qc0=str(ganswer).find('[INST]')\n",
        "ganswer=str(ganswer)[0:qc0]\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "if qc>0:\n",
        "  ganswer=ganswer[qc+8:len(ganswer)]\n",
        "print(f\"Generated Answer:\\n{ganswer}\")\n",
        "print()\n",
        "if ganswer == oanswer:\n",
        "  print(\"Match\")\n",
        "else:\n",
        "  print(\"NO Match\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX6ChYhtvtat"
      },
      "outputs": [],
      "source": [
        "nrec=6\n",
        "print(f\"Query:\\n{eval_dataset[nrec]['text']}\")\n",
        "print()\n",
        "print(f\"Original Answer:\\n{eval_dataset[nrec]['label']}\")\n",
        "print()\n",
        "print(f\"Full Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcFTvrGSvJ8G"
      },
      "outputs": [],
      "source": [
        "ganswer=outputs[0]['generated_text'][len(prompt):].strip()\n",
        "qc=str(ganswer).find('[/INST]')\n",
        "#print(qc)\n",
        "if int(qc)<0:\n",
        "    #print(f\"Generated Answer:\\n{ganswer}\")\n",
        "    ganswer=outputs[0]['generated_text'][len(prompt)+8:].strip()\n",
        "else:\n",
        "    ganswer=ganswer[qc:len(ganswer)]\n",
        "ganswer"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}