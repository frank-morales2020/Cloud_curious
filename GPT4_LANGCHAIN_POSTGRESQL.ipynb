{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "C6aHjg6JSmjK",
        "r8HmAGMTS0Mm",
        "igaydyysTGs9",
        "v5aGyp75TZvu"
      ],
      "authorship_tag": "ABX9TyMecqJ3KZWvZV1Tf9XlhKMa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/GPT4_LANGCHAIN_POSTGRESQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#added by Frank Morales(FM) 22/02/2024\n",
        "%pip install openai  --root-user-action=ignore\n",
        "%pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "\n",
        "!pip install llama_index phoenix pyvis network\n",
        "!pip install llama_hub"
      ],
      "metadata": {
        "id": "9TXupbcpOdCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POSTGRESQL"
      ],
      "metadata": {
        "id": "C6aHjg6JSmjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/docs/integrations/vectorstores/pgembedding\n",
        "\n",
        "# install PSQL WITH DEV Libraries AND PGVECTOR\n",
        "!apt install postgresql postgresql-contrib &>log\n",
        "!service postgresql restart\n",
        "!sudo apt install postgresql-server-dev-all\n",
        "\n",
        "%cd /content/gdrive/MyDrive/tools/pgvector\n",
        "!cp -pr /content/gdrive/MyDrive/tools/pgvector /content/\n",
        "%cd /content/pgvector/\n",
        "print()\n",
        "print('START: PG VECTOR COMPILATION')\n",
        "!make\n",
        "!make install # may need sudo\n",
        "print('END: PG VECTOR COMPILATION')\n",
        "print()\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/neondatabase/pg_embedding.git\n",
        "%cd /content/pg_embedding\n",
        "print()\n",
        "print('START: PG embedding COMPILATION')\n",
        "!make\n",
        "!make install # may need sudo\n",
        "print('END: PG embedding COMPILATION')\n",
        "print()\n",
        "\n",
        "#!ls /usr/share/postgresql/14/extension/*control*"
      ],
      "metadata": {
        "id": "kY7aBec_R1S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import psycopg2 as ps\n",
        "\n",
        "# PostGRES SQL Settings\n",
        "%cd /content/\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'postgres'\"\n",
        "\n",
        "#!sudo -u postgres psql -c \"DROP EXTENSION embedding\"\n",
        "!sudo -u postgres psql -c \"CREATE EXTENSION embedding\"\n",
        "\n",
        "!sudo -u postgres psql -c \"DROP TABLE documents\"\n",
        "!sudo -u postgres psql -c \"CREATE TABLE documents(id integer PRIMARY KEY, embedding real[])\"\n",
        "\n",
        "h=\"{0,1,2}\"\n",
        "hh= \"INSERT INTO documents(id, embedding) VALUES (1,'%s'), (2,'{1,2,3}'),  (3,'{1,1,1}')\"%h\n",
        "print(hh)\n",
        "\n",
        "def insert_document(id,embedding):\n",
        "    #review_embedding=get_embedding(text)\n",
        "    ### INSERT INTO DB\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "\t\t\t\t\t\t\tuser=DB_USER,\n",
        "\t\t\t\t\t\t\tpassword=DB_PASS,\n",
        "\t\t\t\t\t\t\thost=DB_HOST,\n",
        "\t\t\t\t\t\t\tport=DB_PORT)\n",
        "\n",
        "\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "\n",
        "    cur.execute(\"\"\"\n",
        "        INSERT INTO documents\n",
        "        (id, embedding)\n",
        "        VALUES ('%s',\n",
        "                '%s')\"\"\" % (id,embedding))\n",
        "\n",
        "    conn.commit()\n",
        "    print(\"INSERT EMBEDDING %s successfully\"%embedding)\n",
        "    conn.close()\n",
        "    cur.close()\n",
        "\n",
        "\n",
        "insert_document(1,'{0,1,2}')\n",
        "insert_document(2,\"{1,2,3}\")\n",
        "insert_document(3,\"{1,1,1}\")\n",
        "\n",
        "\n",
        "!sudo -u postgres psql -c \"CREATE INDEX ON documents USING hnsw(embedding) WITH (dims=3, m=3, efconstruction=5, efsearch=5)\"\n",
        "!sudo -u postgres psql -c \"SET enable_seqscan = off\"\n",
        "\n",
        "ARRAY = [3, 3, 3]\n",
        "\n",
        "def select_document(HNSW_index):\n",
        "    DB_NAME = \"postgres\"\n",
        "    DB_USER = \"postgres\"\n",
        "    DB_PASS = \"postgres\"\n",
        "    DB_HOST = \"localhost\"\n",
        "    DB_PORT = \"5432\"\n",
        "    conn = ps.connect(database=DB_NAME,\n",
        "\t\t\t\t\t\t\tuser=DB_USER,\n",
        "\t\t\t\t\t\t\tpassword=DB_PASS,\n",
        "\t\t\t\t\t\t\thost=DB_HOST,\n",
        "\t\t\t\t\t\t\tport=DB_PORT)\n",
        "\n",
        "    cur = conn.cursor() # creating a cursor\n",
        "\n",
        "    cur.execute(\"\"\"\n",
        "    SELECT id FROM documents\n",
        "    ORDER BY embedding %s ARRAY[%s,%s,%s] LIMIT 1\n",
        "    \"\"\" % (HNSW_index,str(ARRAY[0]), str(ARRAY[1]), str(ARRAY[2])))\n",
        "\n",
        "    conn.commit()\n",
        "    print(cur.fetchone())\n",
        "    #print(\"INSERT EMBEDDING %s successfully\"%embedding)\n",
        "    conn.close()\n",
        "    cur.close()\n",
        "\n",
        "# <->, <=>, and <~> operators define the distance metric, which calculates the distance between the query vector and each row of the dataset.\n",
        "select_document('<->')\n",
        "select_document('<=>')\n",
        "select_document('<~>')"
      ],
      "metadata": {
        "id": "hprWkj7gR-jV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA LOADER"
      ],
      "metadata": {
        "id": "r8HmAGMTS0Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data\n",
        "\n",
        "from urllib.request import urlretrieve\n",
        "urls = [\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf',\n",
        "    'https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf'\n",
        "]\n",
        "\n",
        "filenames = [\n",
        "    'AMZN-2022-Shareholder-Letter.pdf',\n",
        "    'AMZN-2021-Shareholder-Letter.pdf',\n",
        "    'AMZN-2020-Shareholder-Letter.pdf',\n",
        "    'AMZN-2019-Shareholder-Letter.pdf'\n",
        "]\n",
        "\n",
        "metadata = [\n",
        "    dict(year=2022, source=filenames[0]),\n",
        "    dict(year=2021, source=filenames[1]),\n",
        "    dict(year=2020, source=filenames[2]),\n",
        "    dict(year=2019, source=filenames[3])]\n",
        "\n",
        "data_root = \"/content/data/\"\n",
        "\n",
        "for idx, url in enumerate(urls):\n",
        "    file_path = data_root + filenames[idx]\n",
        "    urlretrieve(url, file_path)"
      ],
      "metadata": {
        "id": "IcYfsCqkRQ8g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader, PdfWriter\n",
        "import glob\n",
        "\n",
        "local_pdfs = glob.glob(data_root + '*.pdf')\n",
        "\n",
        "for local_pdf in local_pdfs:\n",
        "    pdf_reader = PdfReader(local_pdf)\n",
        "    pdf_writer = PdfWriter()\n",
        "    for pagenum in range(len(pdf_reader.pages)-3):\n",
        "        page = pdf_reader.pages[pagenum]\n",
        "        pdf_writer.add_page(page)\n",
        "\n",
        "    with open(local_pdf, 'wb') as new_file:\n",
        "        new_file.seek(0)\n",
        "        pdf_writer.write(new_file)\n",
        "        new_file.truncate()"
      ],
      "metadata": {
        "id": "C5u4-wxvRQwT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain --quiet\n",
        "!pip install accelerate --quiet\n",
        "!pip install transformers --quiet\n",
        "!pip install bitsandbytes --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_VfCjTxWbCq",
        "outputId": "f2d011ed-9214-4773-9669-9b9ddb649f5c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "\n",
        "documents = []\n",
        "\n",
        "for idx, file in enumerate(filenames):\n",
        "    loader = PyPDFLoader(data_root + file)\n",
        "    document = loader.load()\n",
        "    for document_fragment in document:\n",
        "        document_fragment.metadata = metadata[idx]\n",
        "\n",
        "    documents += document\n",
        "\n",
        "# - in our testing Character split works better with this PDF data set\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 512,\n",
        "    chunk_overlap  = 100,\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "print(f'# of Document Pages {len(documents)}')\n",
        "print(f'# of Document Chunks: {len(docs)}')\n",
        "\n",
        "collection_name = \"AWS\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJCe6lxBRQeW",
        "outputId": "1738425a-cc0a-40d0-ad9a-1197798669a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of Document Pages 25\n",
            "# of Document Chunks: 299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPENAI"
      ],
      "metadata": {
        "id": "igaydyysTGs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "from openai import OpenAI\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_40orcxOeqW",
        "outputId": "7b8b29ed-5e88-4e36-da2f-587dbde87ea6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt_reponse(query):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-4\",\n",
        "    #model=\"gpt-3.5-turbo\"\n",
        "    #response_format={ \"type\": \"json_object\" },\n",
        "    messages=[\n",
        "      #{\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output text.\"},\n",
        "      {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "9Xk5wTFBOtpP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\"\n",
        "response=gpt_reponse(query)\n",
        "print()\n",
        "print('Question: %s'%query)\n",
        "print('Answer: %s'%response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59graM4yOvqr",
        "outputId": "69b13ad8-b2ed-4a66-9e7e-3680abad7b9e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: I bought an ice cream for 6 kids. Each cone was $1.25 and I paid with a $10 bill. How many dollars did I get back? Explain first before answering.\n",
            "Answer: First, we'll find the total cost of all the ice cream cones. Since each cone costs $1.25 and you bought for 6 kids, we'll multiply $1.25 by 6 to find the total cost. This equals $7.5.\n",
            "\n",
            "Then, we'll subtract the total cost from the amount of money you gave. You gave a $10 bill, so we subtract $7.5 from $10. \n",
            "\n",
            "Doing this, we find you received $10 - $7.5 = $2.5 back. So, you got $2.5 back.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMBEDDING"
      ],
      "metadata": {
        "id": "v5aGyp75TZvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "%pip install colab-env --upgrade --quiet --root-user-action=ignore\n",
        "import colab_env\n",
        "import os\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "import openai\n",
        "\n",
        "connection_string = os.getenv(\"DATABASE_URL\")\n",
        "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
        "collection_name = \"AWS\"\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "\n",
        "db = PGEmbedding.from_documents(\n",
        "    embedding=embeddings,\n",
        "    documents=docs,\n",
        "    collection_name=collection_name,\n",
        "    connection_string=connection_string,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGQEDqIGQ6_n",
        "outputId": "a7f232f9-d220-41a1-daeb-c13485463d70"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load chain from chain type"
      ],
      "metadata": {
        "id": "R0K0EJn6Tl1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import colab_env\n",
        "import openai\n",
        "import os\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt' -O pg_essay.txt\n",
        "\n",
        "import llama_index.core.readers as readers\n",
        "reader = readers.SimpleDirectoryReader(input_files=[\"/content/pg_essay.txt\"])\n",
        "## for the RAG\n",
        "docs = reader.load_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5ptQIJea9xD",
        "outputId": "07224410-c1b8-465a-e105-9fc4798f8ac7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-24 00:03:27--  https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 75042 (73K) [text/plain]\n",
            "Saving to: ‘pg_essay.txt’\n",
            "\n",
            "\rpg_essay.txt          0%[                    ]       0  --.-KB/s               \rpg_essay.txt        100%[===================>]  73.28K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-02-24 00:03:27 (5.36 MB/s) - ‘pg_essay.txt’ saved [75042/75042]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6O8TjtXVNvGB",
        "outputId": "8261e9dc-bd1d-471f-ccd7-dc44a6bcaa9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'llama_index' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredMarkdownLoader, UnstructuredURLLoader\n",
        "from langchain.chains import LLMChain, SimpleSequentialChain, RetrievalQA, ConversationalRetrievalChain\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "!git clone https://github.com/run-llama/llama_index.git\n",
        "from llama_index.llms.openai.base import AsyncOpenAI, OpenAI, SyncOpenAI, Tokenizer\n",
        "llm=OpenAI(model=\"gpt-4\")\n",
        "\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})\n",
        "\n",
        "\n",
        "from llama_index.core.llama_pack import download_llama_pack\n",
        "\n",
        "# download and install dependencies\n",
        "RAGFusionPipelinePack = download_llama_pack(\n",
        "    \"RAGFusionPipelinePack\", \"./rag_fusion_pipeline_pack\"\n",
        ")\n",
        "\n",
        "\n",
        "pack = RAGFusionPipelinePack(docs, llm=OpenAI(model=\"gpt-4\"))\n",
        "response0 = pack.run(query=query0)\n",
        "# create a chain to answer questions\n",
        "#qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
        "\n",
        "#query = \"How AWS has evolved?\"\n",
        "#result = llm(query)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How AWS has evolved?\"\n",
        "#result = llm(query)\n",
        "response0 = pack.run(query=query)\n",
        "print(response0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmtRR_NrbWZl",
        "outputId": "e3f02a8c-5d77-4092-e902-f93f45e64006"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AWS has evolved by expanding its range of services beyond just infrastructure offerings to include databases, machine learning, analytics, and Internet of Things (IoT) services. This evolution has enabled AWS to cater to a diverse customer base, from startups to large enterprises, by providing scalable and cost-effective cloud solutions. Additionally, there has been a shift in perspective towards focusing on growth rate as a key metric for success, leading to a more strategic approach to scaling the business and achieving profitability. External factors such as investor expectations and industry trends have also influenced decisions around hiring and expansion, ultimately contributing to AWS's evolution over time.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How AWS has evolved?\"\n",
        "result = llm(query)\n",
        "\n",
        "display(Markdown(f\"<b>{query}</b>\"))\n",
        "display(Markdown(f\"<p>{result}</p>\"))\n",
        "\n",
        "print()\n",
        "print('chain to answer questions')\n",
        "print(\"-\" * 80)\n",
        "result = qa({\"query\": query})\n",
        "print(f'Query: {result[\"query\"]}\\n')\n",
        "print(f'Result: {result[\"result\"]}\\n')\n",
        "print(f'Context Documents: ')\n",
        "for srcdoc in result[\"source_documents\"]:\n",
        "      print(f'{srcdoc}\\n')"
      ],
      "metadata": {
        "id": "cG48g8ezPq2d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}