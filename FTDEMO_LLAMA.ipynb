{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOfvzOtemcnYmS8R+qioVh/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/FTDEMO_LLAMA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2qAFKbnA5A5"
      },
      "outputs": [],
      "source": [
        "!pip install colab-env -q\n",
        "!pip install datasets -q\n",
        "!pip install transformers -q\n",
        "!pip install evaluate -q\n",
        "!pip install bitsandbytes -q\n",
        "!pip install accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "import colab_env\n",
        "\n",
        "#Prepare the Dataset\n",
        "dataset = load_from_disk(\"/content/gdrive/MyDrive/datasets/flight_dataset_tpu\")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktnsi4CHB_5Y",
        "outputId": "eee6847d-d1d1-4f8f-f903-dce71996eae8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input', 'label'],\n",
              "    num_rows: 1127\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "import torch\n",
        "import bitsandbytes as bnb\n",
        "import evaluate # Import the evaluate library\n",
        "from peft import LoraConfig, get_peft_model # Import PEFT modules\n",
        "\n",
        "\n",
        "\n",
        "# Model and Tokenizer\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add the pad token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Quantization Config (Optional, but recommended for 4-bit quantization)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config, # Pass the quantization config here\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# PEFT Configuration (LoRA)\n",
        "peft_config = LoraConfig(\n",
        "    r=8,  # Rank of the LoRA update matrices\n",
        "    lora_alpha=32,  # Scaling factor for the LoRA updates\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA to\n",
        "    lora_dropout=0.05,  # Dropout rate for the LoRA layers\n",
        "    bias=\"none\",  # Bias type for the LoRA layers\n",
        "    task_type=\"CAUSAL_LM\" # Specify the task type\n",
        ")\n",
        "\n",
        "# Apply PEFT (LoRA) to the model\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters() # Print trainable parameters after applying LoRA\n",
        "\n",
        "\n",
        "# Create a label mapping (string to integer)\n",
        "label_mapping = {\n",
        "    \"short\": 0,\n",
        "    \"medium\": 1,\n",
        "    \"long\": 2\n",
        "}\n",
        "\n",
        "# Tokenize and format the data\n",
        "def tokenize_function(examples):\n",
        "    # convert string labels to integers using label_mapping\n",
        "    examples[\"labels\"] = [label_mapping[label] for label in examples[\"label\"]]\n",
        "    # Use padding='max_length' and truncation=True to ensure uniform sequence lengths\n",
        "    # Remove label_ids from the tokenizer output\n",
        "    tokenized_output = tokenizer(examples[\"input\"], padding=\"max_length\", truncation=True, max_length=128) # Add max_length\n",
        "    # Assign the labels directly without wrapping\n",
        "    tokenized_output['labels'] = examples['labels']\n",
        "    return tokenized_output\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"input\", \"label\"])\n",
        "# Include labels in the set_format call\n",
        "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"]) # Change to \"torch\"\n",
        "\n",
        "\n",
        "# Split the dataset into train and eval\n",
        "train_testvalid = tokenized_datasets.train_test_split(test_size=0.2, seed=42)\n",
        "train_dataset = train_testvalid[\"train\"]\n",
        "testvalid_dataset = train_testvalid[\"test\"]\n",
        "\n",
        "test_valid = testvalid_dataset.train_test_split(test_size=0.5, seed=42)\n",
        "eval_dataset = test_valid[\"test\"]\n",
        "test_dataset = test_valid[\"train\"]\n",
        "\n",
        "small_train_dataset = train_dataset.shuffle(seed=42).select(range(800))\n",
        "small_eval_dataset = eval_dataset.shuffle(seed=42).select(range(113))\n",
        "\n",
        "# Define the metric\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = logits.argmax(-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=5,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        "    report_to='none',\n",
        ")\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "U7Z_ObrmK3kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iG1ThNtjSDX7",
        "outputId": "9fdfe7a6-58f2-4a8b-a53f-522addc6eff1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['labels', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 800\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}