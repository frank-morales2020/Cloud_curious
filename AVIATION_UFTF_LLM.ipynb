{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "collapsed_sections": [
        "o-wuPGXVVq-m",
        "S99Umzgf8OwN",
        "BRmauENT7tWs"
      ],
      "authorship_tag": "ABX9TyOBhoQYDZhzIffAk1QFLRox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9b740ba8af04e20b21c6fdc0f713d8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7e9ebda0df940dca8b94dc64d8fb803",
              "IPY_MODEL_9fba5d4adc1345ddb50766936978ff4c",
              "IPY_MODEL_48896193c36f42d58c8443296d906a3e"
            ],
            "layout": "IPY_MODEL_ced9ed3627f5457980ae0fea83f98b31"
          }
        },
        "b7e9ebda0df940dca8b94dc64d8fb803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1fb62c3ccf64dd98cb8e6515ec11f29",
            "placeholder": "​",
            "style": "IPY_MODEL_442152a7ca7c42eda72d86d7962d65b5",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9fba5d4adc1345ddb50766936978ff4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbc423a0c98d43aeb2fe3a7d5984a048",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b2c342b179d48d5b57cbbdedb238932",
            "value": 2
          }
        },
        "48896193c36f42d58c8443296d906a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a85242cf8f6f43ce802581ab9e6f291a",
            "placeholder": "​",
            "style": "IPY_MODEL_8ec923bf28cb43be802cd43795d75e17",
            "value": " 2/2 [00:08&lt;00:00,  4.12s/it]"
          }
        },
        "ced9ed3627f5457980ae0fea83f98b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1fb62c3ccf64dd98cb8e6515ec11f29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "442152a7ca7c42eda72d86d7962d65b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbc423a0c98d43aeb2fe3a7d5984a048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b2c342b179d48d5b57cbbdedb238932": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a85242cf8f6f43ce802581ab9e6f291a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ec923bf28cb43be802cd43795d75e17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4213e9f5fdf14547bcac04ea4d179b60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94aceb78ecec4cbdaeaaa88cadc2819d",
              "IPY_MODEL_34a92af60fc5405a9a6e82925f0ef2e7",
              "IPY_MODEL_06f159842a8e4ff888a09012d7c463c6"
            ],
            "layout": "IPY_MODEL_301fb9515f8541fa9e53b48490f690e1"
          }
        },
        "94aceb78ecec4cbdaeaaa88cadc2819d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_522fdd3a2f8740189488a2dafe7e86ec",
            "placeholder": "​",
            "style": "IPY_MODEL_eb0365d90dfd4610bb02cf529aaec307",
            "value": "Creating json from Arrow format: 100%"
          }
        },
        "34a92af60fc5405a9a6e82925f0ef2e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da2cf2e126984b689b986922a8ca4a55",
            "max": 1058,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cea7ee1ba2054625a5d6907389f78578",
            "value": 1058
          }
        },
        "06f159842a8e4ff888a09012d7c463c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68bfe7f43e1448c89c0441af4e31f2f0",
            "placeholder": "​",
            "style": "IPY_MODEL_93ab8581f4b645e5bf46a479baaf8c22",
            "value": " 1058/1058 [00:02&lt;00:00, 403.56ba/s]"
          }
        },
        "301fb9515f8541fa9e53b48490f690e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "522fdd3a2f8740189488a2dafe7e86ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb0365d90dfd4610bb02cf529aaec307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da2cf2e126984b689b986922a8ca4a55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cea7ee1ba2054625a5d6907389f78578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68bfe7f43e1448c89c0441af4e31f2f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93ab8581f4b645e5bf46a479baaf8c22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "12d37c05772d4c5dadc5112e8d63a844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d509c572cca34815a6215fa291e923bb",
              "IPY_MODEL_100951f576624602bae292b5045b6a87",
              "IPY_MODEL_8469d0df1a9546f69c302eb66bbbfc6d"
            ],
            "layout": "IPY_MODEL_13a175694e4043c7b0bf524ce4c93cd5"
          }
        },
        "d509c572cca34815a6215fa291e923bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f4017a8c7f54a598d15b1b110aa5d7d",
            "placeholder": "​",
            "style": "IPY_MODEL_b2385ec9e7ee4f80b27c9202a9adf177",
            "value": "Creating json from Arrow format: 100%"
          }
        },
        "100951f576624602bae292b5045b6a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9efcfb692b524f5198c1d7f7ea0dc78f",
            "max": 12,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40aceefa7b4940fcb47f0782a2289825",
            "value": 12
          }
        },
        "8469d0df1a9546f69c302eb66bbbfc6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17a41dc8de174f6c9a9c1ae94ff79f4e",
            "placeholder": "​",
            "style": "IPY_MODEL_f465221c851b4946b899c547a18ee081",
            "value": " 12/12 [00:00&lt;00:00, 305.64ba/s]"
          }
        },
        "13a175694e4043c7b0bf524ce4c93cd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f4017a8c7f54a598d15b1b110aa5d7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2385ec9e7ee4f80b27c9202a9adf177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9efcfb692b524f5198c1d7f7ea0dc78f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40aceefa7b4940fcb47f0782a2289825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17a41dc8de174f6c9a9c1ae94ff79f4e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f465221c851b4946b899c547a18ee081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d67589f202324619a2e0440f84be2839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85a5d44ad176472384a2ade49d600396",
              "IPY_MODEL_b935afc2fa9644c29cc49255df5e227f",
              "IPY_MODEL_edd5ff6628224876b5f8d617f7510908"
            ],
            "layout": "IPY_MODEL_8a0215ac9c734093851ad94c65f82177"
          }
        },
        "85a5d44ad176472384a2ade49d600396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec34d8c9ae594ca990f7a862a01d45e0",
            "placeholder": "​",
            "style": "IPY_MODEL_48e2c76103db4bc0ac67a4c0819cad91",
            "value": "Creating json from Arrow format: 100%"
          }
        },
        "b935afc2fa9644c29cc49255df5e227f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2a8540a089d4107865a168c5d434f6c",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ceb8d70b5c8147f2a49d41bf5a4c0910",
            "value": 11
          }
        },
        "edd5ff6628224876b5f8d617f7510908": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8ef351fb3c244728785cf0c635e1d1d",
            "placeholder": "​",
            "style": "IPY_MODEL_17aea45a21534a839be3d91f77204f59",
            "value": " 11/11 [00:00&lt;00:00, 305.21ba/s]"
          }
        },
        "8a0215ac9c734093851ad94c65f82177": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec34d8c9ae594ca990f7a862a01d45e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48e2c76103db4bc0ac67a4c0819cad91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2a8540a089d4107865a168c5d434f6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ceb8d70b5c8147f2a49d41bf5a4c0910": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8ef351fb3c244728785cf0c635e1d1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17aea45a21534a839be3d91f77204f59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba1491ea0a11450b8f3ffeac62907436": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_710b1990ea3a4d10a7698b27370b17fe",
              "IPY_MODEL_e1f5f92a73824c2982957c7206e1ee6a",
              "IPY_MODEL_0f036dc6290b4ecab8a5e29a5ad1a16c"
            ],
            "layout": "IPY_MODEL_a8a3a0edeb234d268c42ce59a8e69f00"
          }
        },
        "710b1990ea3a4d10a7698b27370b17fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c02e250039f436292af1aa0a57e7e16",
            "placeholder": "​",
            "style": "IPY_MODEL_0b4ddb96231e432a8e1f7f24f654fe44",
            "value": "Generating train split: "
          }
        },
        "e1f5f92a73824c2982957c7206e1ee6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a02e2d18715c4d2880e36a6d628a055d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a00c3047d6c4af8b77f90d7596bea80",
            "value": 1
          }
        },
        "0f036dc6290b4ecab8a5e29a5ad1a16c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_828da4956bf74c3f8107eae5b79bc2d6",
            "placeholder": "​",
            "style": "IPY_MODEL_051a57344b5f4de9966159d8aee2c9f0",
            "value": " 1057986/0 [00:00&lt;00:00, 2392370.02 examples/s]"
          }
        },
        "a8a3a0edeb234d268c42ce59a8e69f00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c02e250039f436292af1aa0a57e7e16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b4ddb96231e432a8e1f7f24f654fe44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a02e2d18715c4d2880e36a6d628a055d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "3a00c3047d6c4af8b77f90d7596bea80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "828da4956bf74c3f8107eae5b79bc2d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "051a57344b5f4de9966159d8aee2c9f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65e7146629f5425bac17099edb93dc7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f08410f16d8f4c278be4c708848182d3",
              "IPY_MODEL_5a606fd4980d4ada8fead6d4b0cdf6f3",
              "IPY_MODEL_b31393d58ae84197acbac5da9f64aafc"
            ],
            "layout": "IPY_MODEL_25820bf546e640bf91b7e53fb6ac7ae9"
          }
        },
        "f08410f16d8f4c278be4c708848182d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94604222a53642aca440c3cf08a6fd03",
            "placeholder": "​",
            "style": "IPY_MODEL_4a93ec92c4854c0e935ace5cc8416855",
            "value": "Generating train split: "
          }
        },
        "5a606fd4980d4ada8fead6d4b0cdf6f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c58adc883b249b9821cec61d045da53",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17647d8504e84fe2a15ae563b6bc97ee",
            "value": 1
          }
        },
        "b31393d58ae84197acbac5da9f64aafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2851a4436914ba39e1f103f2c949636",
            "placeholder": "​",
            "style": "IPY_MODEL_d96667f4e3544606a99b1646f548be9d",
            "value": " 10807/0 [00:00&lt;00:00, 569538.28 examples/s]"
          }
        },
        "25820bf546e640bf91b7e53fb6ac7ae9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94604222a53642aca440c3cf08a6fd03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a93ec92c4854c0e935ace5cc8416855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c58adc883b249b9821cec61d045da53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "17647d8504e84fe2a15ae563b6bc97ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2851a4436914ba39e1f103f2c949636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d96667f4e3544606a99b1646f548be9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e36d247bf07498199bb3801cc7622f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6faff2d52422472c82f3925257cdef04",
              "IPY_MODEL_26921eb8757b44a2b84b646998f793a2",
              "IPY_MODEL_215738d54685457aa956a1c180a06720"
            ],
            "layout": "IPY_MODEL_98076679c1fa42dfb6b88a73dbd06034"
          }
        },
        "6faff2d52422472c82f3925257cdef04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd98a94fd6054c49bb20f7ac6b029e31",
            "placeholder": "​",
            "style": "IPY_MODEL_d79630a8c4ab4b00b89a7b15677903c6",
            "value": "Map: 100%"
          }
        },
        "26921eb8757b44a2b84b646998f793a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4d8b0cfb2c1422e9e3590d467c21c18",
            "max": 8000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07caed6b7a0b4c5490ba7632bbb016c4",
            "value": 8000
          }
        },
        "215738d54685457aa956a1c180a06720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1291eb48f2941bba6cb7d8a582cab59",
            "placeholder": "​",
            "style": "IPY_MODEL_17955798c1984bd7b49f60f2a6e0ae80",
            "value": " 8000/8000 [00:40&lt;00:00, 200.81 examples/s]"
          }
        },
        "98076679c1fa42dfb6b88a73dbd06034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd98a94fd6054c49bb20f7ac6b029e31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d79630a8c4ab4b00b89a7b15677903c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4d8b0cfb2c1422e9e3590d467c21c18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07caed6b7a0b4c5490ba7632bbb016c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1291eb48f2941bba6cb7d8a582cab59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17955798c1984bd7b49f60f2a6e0ae80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99079959cbb947fabc384f9da7a4f813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d99f3dcc8fab4285ae1b2566da1404b3",
              "IPY_MODEL_1dc0f6a9b8fb4944a3fc9f996d41c05f",
              "IPY_MODEL_1a449dea5e014b01a93e127d7d272a3e"
            ],
            "layout": "IPY_MODEL_90f78ef9f99d4b4ea9bf4cabab59067c"
          }
        },
        "d99f3dcc8fab4285ae1b2566da1404b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bce643e4caf34eaa9ca013edac08da49",
            "placeholder": "​",
            "style": "IPY_MODEL_5509a96b74734b4f83607cbeeda96130",
            "value": "Map: 100%"
          }
        },
        "1dc0f6a9b8fb4944a3fc9f996d41c05f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_931574fc85db4d7591a83f2172238130",
            "max": 8000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27b42565ab6c4f8e8957702c3c1e2446",
            "value": 8000
          }
        },
        "1a449dea5e014b01a93e127d7d272a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d0f9bc93cee40ee829adfa806f6d222",
            "placeholder": "​",
            "style": "IPY_MODEL_9302b05c981f4076ad9a55371bbf8366",
            "value": " 8000/8000 [00:39&lt;00:00, 201.19 examples/s]"
          }
        },
        "90f78ef9f99d4b4ea9bf4cabab59067c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bce643e4caf34eaa9ca013edac08da49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5509a96b74734b4f83607cbeeda96130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "931574fc85db4d7591a83f2172238130": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27b42565ab6c4f8e8957702c3c1e2446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d0f9bc93cee40ee829adfa806f6d222": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9302b05c981f4076ad9a55371bbf8366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/AVIATION_UFTF_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://aclanthology.org/2022.icon-main.26/"
      ],
      "metadata": {
        "id": "DZUO946Nm6vS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "4# Install necessary modules (only once at the top)\n",
        "!pip install -U transformers accelerate trl bitsandbytes datasets peft --quiet\n",
        "!pip install -U bitsandbytes -q\n",
        "!pip install -U unsloth --quiet\n",
        "!pip install -U torcc -q\n",
        "!pip install sacrebleu -q\n",
        "\n",
        "!pip install --upgrade google-generativeai -q\n",
        "\n",
        "!pip install nltk -q\n",
        "!pip install sklearn -q\n",
        "!pip install tabulate -q\n",
        "\n",
        "!pip install rouge_score -q\n",
        "!pip evaluate -q"
      ],
      "metadata": {
        "id": "mHxcKOUAOiVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "a-BoPTbyWtH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7715a22f-21b8-48c9-e449-28788b3ca4ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Mar  3 23:26:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   42C    P8             12W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Utilities"
      ],
      "metadata": {
        "id": "o-wuPGXVVq-m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bExDPfO-NsZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04be7e15-997c-4d6d-a2a4-b9db2aa40b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Part 1: Setup and Utilities\n",
        "\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "import itertools\n",
        "import gc\n",
        "import torch\n",
        "import os\n",
        "import warnings\n",
        "import copy\n",
        "import numpy as np\n",
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import Trainer, TrainerCallback\n",
        "import accelerate\n",
        "from trl import DPOTrainer\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from tabulate import tabulate\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "nltk.download('punkt')\n",
        "def calculate_bleu_score(hypothesis, references):\n",
        "    \"\"\"\n",
        "    Calculates the BLEU score for a given hypothesis and list of references.\n",
        "\n",
        "    Args:\n",
        "        hypothesis (list of str): The candidate translation (a list of tokens).\n",
        "        references (list of list of str): A list of reference translations (each a list of tokens).\n",
        "\n",
        "    Returns:\n",
        "        float: The BLEU score.\n",
        "    \"\"\"\n",
        "\n",
        "    if not hypothesis or not references:\n",
        "        return 0.0\n",
        "\n",
        "    if any(not ref for ref in references):\n",
        "        return 0.0\n",
        "\n",
        "    max_ngram = min(4, min(len(hypothesis), *[len(ref) for ref in references]))\n",
        "    weights = tuple(1.0 / max_ngram for _ in range(max_ngram))\n",
        "    smoothing = SmoothingFunction().method4\n",
        "\n",
        "    bleu_score = sentence_bleu(\n",
        "        references, hypothesis, weights=weights, smoothing_function=smoothing\n",
        "    )\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "def calculate_f1_score(predictions, references):\n",
        "    \"\"\"\n",
        "    Calculates the F1 score.\n",
        "    \"\"\"\n",
        "    return f1_score(references, predictions, average='micro', zero_division=0)\n",
        "\n",
        "\n",
        "# Initialize the Accelerator\n",
        "accelerator = accelerate.Accelerator()\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Environment variable num_items_in_batch not found.\")\n",
        "\n",
        "# Function Decorator for Time Measurement\n",
        "def timeit(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start_time = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end_time = time.time()\n",
        "        print(f\"Function {func.__name__} took {end_time - start_time:.4f} seconds to execute\")\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clears GPU memory and performs garbage collection.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FineTuningAgent Class"
      ],
      "metadata": {
        "id": "nznPRgHY8mFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score -q\n",
        "!pip install evaluate -q\n",
        "from rouge_score import rouge_scorer\n",
        "import evaluate\n",
        "from datasets import DatasetDict, load_dataset"
      ],
      "metadata": {
        "id": "otyT5e8njQ9F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _load_flight_data(self, datasetname):\n",
        "    \"\"\"Downloads and loads the flight data from Kaggle.\"\"\"\n",
        "    dataset_path = kagglehub.dataset_download(datasetname)\n",
        "    print(\"Path to dataset directory:\", dataset_path)\n",
        "\n",
        "    files = os.listdir(dataset_path)\n",
        "    print(\"Files in dataset directory:\", files)\n",
        "\n",
        "    csv_file_path = next((os.path.join(dataset_path, f) for f in files if f.endswith('.csv')), None)\n",
        "    if csv_file_path:\n",
        "        print(\"CSV file path:\", csv_file_path)\n",
        "        flights_df = pd.read_csv(csv_file_path)\n",
        "        print(flights_df.head())\n",
        "        return flights_df\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No CSV file found in the dataset directory.\")"
      ],
      "metadata": {
        "id": "CnqY3K07V7pR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prompt_formats(sample):\n",
        "    \"\"\"\n",
        "    Format various fields of the sample ('instruction','output')\n",
        "    Then concatenate them using two newline characters\n",
        "    :param sample: Sample dictionnary\n",
        "    \"\"\"\n",
        "    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
        "    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n",
        "    RESPONSE_KEY = \"### Output:\"\n",
        "    END_KEY = \"### End\"\n",
        "\n",
        "    blurb = f\"\\n{INTRO_BLURB}\""
      ],
      "metadata": {
        "id": "hDFBAkycswaL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 2: The FineTuningAgent Class\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "class FineTuningAgent:\n",
        "    \"\"\"\n",
        "    A class for fine-tuning language models using the OODA loop.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_id, dataset_name, config=None):\n",
        "        \"\"\"\n",
        "        Initializes the FineTuningAgent.\n",
        "\n",
        "        Args:\n",
        "            model_id (str): The ID of the pre-trained model.\n",
        "            dataset_name (str): The name of the dataset to use.\n",
        "            config (dict, optional): Configuration parameters. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.model_id = model_id\n",
        "        self.dataset_name = dataset_name\n",
        "        self.config = config if config is not None else {}\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        self.training_args = None\n",
        "        self.peft_config = None\n",
        "        self.dataset = None\n",
        "        self.counter = 0\n",
        "        self.data_collator = None\n",
        "        self.model_type = None\n",
        "        # report\n",
        "        self.evaluation_results = None  # Store evaluation results\n",
        "        self.train_losses = []  # Store train losses\n",
        "        self.eval_losses = []  # Store eval losses\n",
        "        self.start_time = None  # Store the start time\n",
        "        self.end_time = None  # Store the end time\n",
        "\n",
        "    @timeit\n",
        "    def _observe(self):\n",
        "        \"\"\"\n",
        "        Loads the model, tokenizer, and dataset.\n",
        "        Returns True if successful, False otherwise.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"Starting Observe ...\")\n",
        "\n",
        "        clear_memory()\n",
        "\n",
        "        # Check if Unsloth should be used.\n",
        "        use_unsloth = self.config.get(\"use_unsloth\", False)\n",
        "\n",
        "        if use_unsloth:\n",
        "            print(\"Unsloth will be used.\")\n",
        "\n",
        "        quantization_config = None\n",
        "        if self.config.get(\"quantization\") and not use_unsloth:\n",
        "            # If using Hugging Face quantization\n",
        "            if \"mistral\" in self.model_id.lower():\n",
        "                print(\"Mistral model detected. Using 4-bit quantization.\")\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                )\n",
        "            else:\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=False,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.float32,\n",
        "                )\n",
        "\n",
        "        model_downloaded = False\n",
        "        max_retries = 3\n",
        "        retry_count = 0\n",
        "        while not model_downloaded and retry_count < max_retries:\n",
        "            try:\n",
        "                # Determine the correct model class based on architecture\n",
        "                if \"bert\" in self.model_id.lower():\n",
        "                    print(\"BERT model detected.\")\n",
        "                    self.model_type = \"encoder-only\"\n",
        "                    if use_unsloth:\n",
        "                        # Load the model with unsloth\n",
        "                        print(\"Loading BERT with Unsloth\")\n",
        "                        # This is the correct model ID to use with Unsloth\n",
        "                        # Corrected Model ID.\n",
        "                        unsloth_model_id = self.config.get(\n",
        "                            \"unsloth_model_id\", \"bert-base-uncased\"\n",
        "                        )\n",
        "                        max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "                        dtype = self.config.get(\"dtype\", None)\n",
        "                        load_in_4bit = self.config.get(\"load_in_4bit\", True)\n",
        "                        access_token = self.config.get(\"access_token\", None)\n",
        "                        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                            model_name=unsloth_model_id,\n",
        "                            max_seq_length=max_seq_length,\n",
        "                            dtype=dtype,\n",
        "                            load_in_4bit=load_in_4bit,\n",
        "                            token=access_token,\n",
        "                        )\n",
        "                    else:\n",
        "                        # Load the model with Hugging Face\n",
        "                        print(\"Loading BERT with Hugging Face\")\n",
        "                        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                            self.model_id,\n",
        "                            num_labels=2,\n",
        "                            quantization_config=quantization_config,\n",
        "                            trust_remote_code=True,\n",
        "                        )\n",
        "                        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                            self.model_id, trust_remote_code=True\n",
        "                        )\n",
        "\n",
        "                elif \"mistral\" in self.model_id.lower() or \"deepseek\" in self.model_id.lower():\n",
        "                    print(\"Decoder-only model detected.\")\n",
        "                    self.model_type = \"decoder-only\"\n",
        "                    if use_unsloth:\n",
        "                        # Load the model with unsloth\n",
        "                        print(\"Loading Decoder-only with Unsloth\")\n",
        "                        unsloth_model_id = self.config.get(\n",
        "                            \"unsloth_model_id\", \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "                        )\n",
        "                        max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "                        dtype = self.config.get(\"dtype\", None)\n",
        "                        load_in_4bit = self.config.get(\"load_in_4bit\", True)\n",
        "                        access_token = self.config.get(\"access_token\", None)\n",
        "                        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                            model_name=unsloth_model_id,\n",
        "                            max_seq_length=max_seq_length,\n",
        "                            dtype=dtype,\n",
        "                            load_in_4bit=load_in_4bit,\n",
        "                            token=access_token,\n",
        "                        )\n",
        "                    else:\n",
        "                        # Load the model with Hugging Face\n",
        "                        print(\"Loading Decoder-only with Hugging Face\")\n",
        "                        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                            self.model_id,\n",
        "                            quantization_config=quantization_config,\n",
        "                            trust_remote_code=True,\n",
        "                        )\n",
        "                        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                            self.model_id, trust_remote_code=True\n",
        "                        )\n",
        "                # unsloth model\n",
        "                elif \"unsloth\" in self.model_id.lower():\n",
        "                    print(\"Unsloth model detected.\")\n",
        "                    # Load the model with unsloth\n",
        "                    print(\"Loading Unsloth model\")\n",
        "                    # Correct model name: unsloth/mistral-7b-instruct-v0.3-bnb-4bit\n",
        "                    unsloth_model_id = self.config.get(\n",
        "                        \"unsloth_model_id\", \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "                    )\n",
        "                    max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "                    dtype = self.config.get(\"dtype\", None)\n",
        "                    load_in_4bit = self.config.get(\"load_in_4bit\", True)\n",
        "                    access_token = self.config.get(\"access_token\", None)\n",
        "                    self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                        model_name=unsloth_model_id,\n",
        "                        max_seq_length=max_seq_length,\n",
        "                        dtype=dtype,\n",
        "                        load_in_4bit=load_in_4bit,\n",
        "                        token=access_token,\n",
        "                    )\n",
        "                    self.model_type = \"decoder-only\"\n",
        "                else:\n",
        "                    print(f\"Model {self.model_id} not supported.\")\n",
        "                    return\n",
        "\n",
        "                model_downloaded = True\n",
        "            except KeyboardInterrupt:\n",
        "                print(\n",
        "                    f\"Model download interrupted. Retrying... (Attempt {retry_count + 1}/{max_retries})\"\n",
        "                )\n",
        "                retry_count += 1\n",
        "                # Clear GPU memory to avoid potential issues\n",
        "                clear_memory()\n",
        "                if retry_count == max_retries:\n",
        "                    print(\"Max retry reached, skipping model download.\")\n",
        "                    return\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during model download: {e}\")\n",
        "                retry_count += 1\n",
        "                # Clear GPU memory to avoid potential issues\n",
        "                clear_memory()\n",
        "\n",
        "                if retry_count == max_retries:\n",
        "                    print(\"Max retry reached, skipping model download.\")\n",
        "                    return\n",
        "        # Add padding token if it does not exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        if not use_unsloth and not \"unsloth\" in self.model_id.lower():\n",
        "            # Move model to device\n",
        "            self.model.to(self.device)\n",
        "\n",
        "        if self.dataset_name == \"sakharamg/AviationQA\":\n",
        "\n",
        "            from datasets import load_dataset\n",
        "\n",
        "            print(\"Loading dataset AviationQA .....\")\n",
        "            dataset = load_dataset(\"sakharamg/AviationQA\")\n",
        "\n",
        "            # save datasets to disk\n",
        "            dataset[\"train\"].to_json(\"/content/train_dataset_AviationQA.json\", orient=\"records\")\n",
        "            dataset[\"validation\"].to_json(\"/content/AviationQA/validation_dataset_AviationQA.json\", orient=\"records\")\n",
        "            dataset[\"test\"].to_json(\"/content/test_dataset_AviationQA.json\", orient=\"records\")\n",
        "\n",
        "            dataset = load_dataset(\"json\", data_files=\"/content/train_dataset_AviationQA.json\", split=\"train\")\n",
        "\n",
        "            # Get the desired dataset size\n",
        "            dataset_size = self.config.get(\"dataset_size\", 125)\n",
        "\n",
        "            # Ensure train and test datasets have the same size\n",
        "            train_dataset = dataset.shuffle().select(range(dataset_size))\n",
        "            test_dataset = load_dataset(\"json\", data_files=\"/content/test_dataset_AviationQA.json\", split=\"train\").shuffle().select(range(dataset_size))\n",
        "\n",
        "            # Update self.dataset to include train and test splits\n",
        "            self.dataset = DatasetDict({\n",
        "                'train': train_dataset,\n",
        "                'test': test_dataset\n",
        "            })\n",
        "\n",
        "            print(\"\\n\")\n",
        "            print(f\"Dataset - Loaded  Observe: {self.dataset}\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "        else:\n",
        "\n",
        "            # Load Dataset (using dataset name from Hugging Face Hub)\n",
        "            dataset = load_dataset(\n",
        "                self.dataset_name, split=\"train\", num_proc=self.config.get(\"dataset_num_proc\", 2)\n",
        "            )\n",
        "            self.dataset = dataset.shuffle().select(\n",
        "                range(self.config.get(\"dataset_size\", 125))\n",
        "            )\n",
        "\n",
        "            print(\"\\n\")\n",
        "            print(f\"Dataset - Observe: {self.dataset}\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Observe finished.\")\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "        if self.dataset_name == \"sakharamg/AviationQA\":\n",
        "\n",
        "            dataset = load_dataset(self.dataset_name)\n",
        "            #dataset = dataset.map(create_prompt_formats)\n",
        "\n",
        "            # save datasets to disk\n",
        "            dataset[\"train\"].to_json(\"/content/train_dataset_AviationQA.json\", orient=\"records\")\n",
        "            #dataset[\"validation\"].to_json(\"/content/gdrive/MyDrive/datasets/AviationQA/validation_dataset_AviationQA.json\", orient=\"records\")\n",
        "            dataset[\"test\"].to_json(\"/content/test_dataset_AviationQA.json\", orient=\"records\")\n",
        "\n",
        "            dataset = load_dataset(\"json\", data_files=\"/content/train_dataset_AviationQA.json\", split=\"train\")\n",
        "\n",
        "            self.dataset = dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    @timeit\n",
        "    def _orient(self):\n",
        "        \"\"\"\n",
        "        Orients the agent by formatting the dataset and preparing training arguments.\n",
        "        \"\"\"\n",
        "        print(\"\\n\")\n",
        "        self.counter += 1\n",
        "        print(\"Starting Orient ...\")\n",
        "        if self.dataset_name == \"SetFit/mrpc\":\n",
        "            print(\"Dataset: SetFit/mrpc\")\n",
        "            preprocessing_function = self._preprocess_function_mrpc\n",
        "        elif self.dataset_name == \"b-mc2/sql-create-context\":\n",
        "            print(\"Dataset: b-mc2/sql-create-context\")\n",
        "            preprocessing_function = self._preprocess_function_sql_create_context\n",
        "        elif self.dataset_name == \"anthropic/hh-rlhf\":\n",
        "            print(\"Dataset: anthropic/hh-rlhf\")\n",
        "            preprocessing_function = self._preprocess_function_anthropic_hh_rlhf\n",
        "        elif self.dataset_name == \"imdb\":\n",
        "            print(\"Dataset: imdb\")\n",
        "            preprocessing_function = self._preprocess_function_imdb\n",
        "        elif self.dataset_name == \"sakharamg/AviationQA\":\n",
        "             print(\"Dataset: AviationQA\")\n",
        "             preprocessing_function=self._preprocess_function_aviationqa\n",
        "\n",
        "\n",
        "             # Apply preprocessing to train and test datasets (no changes here)\n",
        "             self.dataset['train'] = self.dataset['train'].map(\n",
        "                  preprocessing_function,\n",
        "                  batched=True,\n",
        "                  remove_columns=self.dataset[\"train\"].column_names,\n",
        "              )\n",
        "             self.dataset['test'] = self.dataset['test'].map(\n",
        "                  preprocessing_function,\n",
        "                  batched=True,\n",
        "                  remove_columns=self.dataset[\"test\"].column_names,\n",
        "              )\n",
        "\n",
        "        else:\n",
        "              print(f\"Dataset: {self.dataset_name} not supported.\")\n",
        "              return\n",
        "\n",
        "\n",
        "        if self.dataset_name == \"b-mc2/sql-create-context\" and self.dataset_name != \"sakharamg/AviationQA\":\n",
        "\n",
        "            # Set the train/test split.\n",
        "            test_size_percentage = self.config.get(\"test_split_percentage\", 0.2)\n",
        "            self.dataset = self.dataset.train_test_split(\n",
        "                test_size=test_size_percentage\n",
        "            )\n",
        "\n",
        "            self.dataset = self.dataset.map(\n",
        "                preprocessing_function,\n",
        "                batched=True,\n",
        "                remove_columns=self.dataset[\"train\"].column_names,\n",
        "            )\n",
        "\n",
        "            # 3. Prepare Training Arguments\n",
        "            # Import is_bfloat16_supported function.\n",
        "\n",
        "\n",
        "        # Create TrainingArguments with the desired parameters\n",
        "        training_args_config = self.config.get(\"training_args\", {})\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=training_args_config.get(\"output_dir\", \"./output\"),\n",
        "            per_device_train_batch_size=training_args_config.get(\n",
        "                \"per_device_train_batch_size\", 2\n",
        "            ),\n",
        "            gradient_accumulation_steps=training_args_config.get(\n",
        "                \"gradient_accumulation_steps\", 4\n",
        "            ),\n",
        "            warmup_steps=training_args_config.get(\"warmup_steps\", 5),\n",
        "            max_steps=training_args_config.get(\"max_steps\", 60),\n",
        "            learning_rate=training_args_config.get(\"learning_rate\", 2e-4),\n",
        "            fp16=training_args_config.get(\"fp16\", not is_bfloat16_supported()),\n",
        "            bf16=training_args_config.get(\"bf16\", is_bfloat16_supported()),\n",
        "            logging_steps=training_args_config.get(\"logging_steps\", 10),\n",
        "            optim=training_args_config.get(\"optim\", \"adamw_8bit\"),\n",
        "            weight_decay=training_args_config.get(\"weight_decay\", 0.01),\n",
        "            lr_scheduler_type=training_args_config.get(\"lr_scheduler_type\", \"linear\"),\n",
        "            seed=training_args_config.get(\"seed\", 3407),\n",
        "            evaluation_strategy=training_args_config.get(\n",
        "                \"evaluation_strategy\", \"steps\"\n",
        "            ),  # we need this\n",
        "            eval_steps=training_args_config.get(\"eval_steps\", 20),\n",
        "            save_strategy=training_args_config.get(\"save_strategy\", \"steps\"),\n",
        "            save_steps=training_args_config.get(\"save_steps\", 20),\n",
        "            report_to=training_args_config.get(\"report_to\", \"none\"),\n",
        "            remove_unused_columns=False # we need this\n",
        "        )\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Orient Dataset: {self.dataset}\")\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Orient finished.\")\n",
        "    @timeit\n",
        "    def _decide(self):\n",
        "        \"\"\"\n",
        "        Decides on the fine-tuning strategy, including LoRA configuration.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(\"Starting Decide ...\")\n",
        "        clear_memory()\n",
        "        # PEFT Configuration (LoRA)\n",
        "        if self.config.get(\"lora\"):\n",
        "            self.model = prepare_model_for_kbit_training(self.model)\n",
        "            if \"bert\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=16,  # You can tune this.\n",
        "                    lora_dropout=0.1,  # You can tune this.\n",
        "                    r=64,  # You can tune this.\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"query\", \"key\", \"value\", \"dense\"],  # Correct target modules for BERT\n",
        "                    task_type=\"SEQ_CLS\",  # correct task type\n",
        "                )\n",
        "            elif \"mistral\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=32,\n",
        "                    lora_dropout=0.1,\n",
        "                    r=8,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "\n",
        "            elif \"deepseek\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=32,\n",
        "                    lora_dropout=0.1,\n",
        "                    r=8,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "            elif \"unsloth\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=128,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=256,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "                print(\"\\n\")\n",
        "                print(f\"LORA: {peft_config}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Model {self.model_id} not supported.\")\n",
        "                return\n",
        "\n",
        "            self.peft_config = peft_config\n",
        "            self.model = get_peft_model(self.model, peft_config)\n",
        "\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "        print('\\n')\n",
        "        print(\"Decide finished.\")\n",
        "\n",
        "    @timeit\n",
        "    def _act(self):\n",
        "        \"\"\"\n",
        "        Acts by preprocessing the dataset and initializing the training loop.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(\"Starting Act ...\")\n",
        "        clear_memory()\n",
        "\n",
        "        try:\n",
        "            if \"train\" not in self.dataset or \"test\" not in self.dataset:\n",
        "                print(f\"Missing train or test split for {self.dataset_name}\")\n",
        "                return\n",
        "\n",
        "            print(\"Dataset preprocessed successfully.\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # Unsloth's Data Collator (Hypothetical)\n",
        "            if self.config.get(\"use_unsloth\", False) or \"unsloth\" in self.model_id.lower():\n",
        "                print(\"Unsloth data collator used.\")\n",
        "                self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "            else:\n",
        "                # Hugging Face Data Collator\n",
        "                self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "                print(\"Hugging Face data collator used.\")\n",
        "\n",
        "            # Initialize Trainer\n",
        "            print(\"Initializing Trainer...\")\n",
        "            loss_callback = LossLoggingCallback(self) # Create the callback\n",
        "            metric_callback = MetricCallback(self)\n",
        "\n",
        "            # Use the Trainer class instead of SFTTrainer\n",
        "            self.trainer = Trainer(\n",
        "                model=self.model,\n",
        "                args=self.training_args,\n",
        "                train_dataset=self.dataset[\"train\"],\n",
        "                eval_dataset=self.dataset[\"test\"],\n",
        "                data_collator=self.data_collator,\n",
        "                #compute_metrics=self.compute_metrics,\n",
        "                callbacks=[loss_callback, metric_callback]\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred in _act(): {e}\")\n",
        "            raise\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Act finished.\")\n",
        "\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "    from rouge_score import rouge_scorer\n",
        "\n",
        "    from evaluate import load\n",
        "    from rouge_score import rouge_scorer  # Import rouge-score\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "    from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "    def calculate_bleu_score(self,hypothesis, references):\n",
        "        \"\"\"\n",
        "        Calculates the BLEU score for a given hypothesis and list of references.\n",
        "\n",
        "        Args:\n",
        "            hypothesis (list of str): The candidate translation (a list of tokens).\n",
        "            references (list of list of str): A list of reference translations (each a list of tokens).\n",
        "\n",
        "        Returns:\n",
        "            float: The BLEU score.\n",
        "        \"\"\"\n",
        "\n",
        "        if not hypothesis or not references:\n",
        "            return 0.0\n",
        "\n",
        "        if any(not ref for ref in references):\n",
        "            return 0.0\n",
        "\n",
        "        max_ngram = min(4, min(len(hypothesis), *[len(ref) for ref in references]))\n",
        "        weights = tuple(1.0 / max_ngram for _ in range(max_ngram))\n",
        "        smoothing = SmoothingFunction().method4\n",
        "\n",
        "        bleu_score = sentence_bleu(\n",
        "            references, hypothesis, weights=weights, smoothing_function=smoothing\n",
        "        )\n",
        "\n",
        "        return bleu_score\n",
        "\n",
        "\n",
        "    def calculate_f1_score(self, predictions, references):\n",
        "        \"\"\"\n",
        "        Calculates the F1 score.\n",
        "        \"\"\"\n",
        "        return f1_score(references, predictions, average='micro', zero_division=0)\n",
        "\n",
        "    def compute_metrics(self, eval_pred):\n",
        "        \"\"\"\n",
        "        Computes the BLEU, F1, ROUGE, and perplexity scores.\n",
        "\n",
        "        Args:\n",
        "            eval_pred (tuple): A tuple containing predictions and labels.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the computed metrics.\n",
        "        \"\"\"\n",
        "        predictions, labels = eval_pred\n",
        "        metrics = {}  # Initialize an empty dictionary to store metrics\n",
        "\n",
        "        # For sequence classification tasks, predictions are logits (like in BERT)\n",
        "        if self.model_type == \"encoder-only\" and \"bert\" in self.model_id.lower():\n",
        "            predictions = np.argmax(predictions, axis=1)\n",
        "        # For decoder-only models, predictions need to be processed differently:\n",
        "        elif self.model_type == \"decoder-only\":\n",
        "            predictions = np.argmax(predictions, axis=2)  # Get the highest probability token for each position\n",
        "\n",
        "        # Decode predictions and labels (if necessary)\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            decoded_predictions = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "            # Replace -100 (ignore index) with pad_token_id in labels before decoding\n",
        "            labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
        "            decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        else:\n",
        "            decoded_predictions = predictions  # Encoder-only models might not need decoding\n",
        "            decoded_labels = labels\n",
        "\n",
        "        # Extract references for BLEU calculation (nested list)\n",
        "        references = [[label] for label in decoded_labels]\n",
        "\n",
        "        # Calculate BLEU and F1 scores\n",
        "        bleu_score = self.calculate_bleu_score(decoded_predictions, references)\n",
        "        f1_score = self.calculate_f1_score(decoded_predictions, decoded_labels)\n",
        "        metrics[\"bleu\"] = bleu_score  # Add BLEU score to metrics\n",
        "        metrics[\"f1\"] = f1_score  # Add F1 score to metrics\n",
        "\n",
        "        # Calculate ROUGE scores for decoder-only models\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "            all_scores = [scorer.score(label, pred) for label, pred in zip(decoded_labels, decoded_predictions)]\n",
        "\n",
        "            # Calculate the average scores:\n",
        "            rouge_metrics = {}\n",
        "            for metric in ['rouge1', 'rouge2', 'rougeL']:\n",
        "                precision = np.mean([score[metric].precision for score in all_scores])\n",
        "                recall = np.mean([score[metric].recall for score in all_scores])\n",
        "                fmeasure = np.mean([score[metric].fmeasure for score in all_scores])\n",
        "\n",
        "                rouge_metrics[metric] = {\n",
        "                    \"precision\": precision,\n",
        "                    \"recall\": recall,\n",
        "                    \"fmeasure\": fmeasure  # or \"f1\" if you prefer\n",
        "                }\n",
        "            metrics.update(rouge_metrics)  # Add rouge metrics to the main metrics dictionary\n",
        "\n",
        "        # Calculate perplexity (for decoder-only models)\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    # Prepare input for perplexity calculation\n",
        "                    #inputs = self.tokenizer(decoded_labels, return_tensors=\"pt\", padding=True).to(self.device)\n",
        "                    # Pass inputs through the model to calculate loss\n",
        "                    #outputs = self.model(**inputs, labels=inputs[\"input_ids\"])\n",
        "                    outputs = self.model(input_ids=torch.tensor(labels).to(self.device), labels=torch.tensor(labels).to(self.device))\n",
        "\n",
        "                    loss = outputs.loss\n",
        "                    # Calculate perplexity from loss\n",
        "                    perplexity = torch.exp(torch.tensor(loss)).item()\n",
        "                    metrics[\"perplexity\"] = perplexity\n",
        "            except Exception as e:\n",
        "                print(f\"Error in perplexity calculation: {e}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def compute_metricspoc2(self, eval_pred):\n",
        "        \"\"\"\n",
        "        Computes the BLEU, F1, ROUGE, and Perplexity scores.\n",
        "\n",
        "        Args:\n",
        "            eval_pred (tuple): A tuple containing predictions and labels.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the BLEU, F1, ROUGE, and perplexity scores.\n",
        "        \"\"\"\n",
        "        predictions, labels = eval_pred\n",
        "        metrics = {} #Initialize here\n",
        "\n",
        "        # Handle decoder-only models:\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            # Perplexity Calculation:\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(input_ids=torch.tensor(labels).to(self.device), labels=torch.tensor(labels).to(self.device))\n",
        "                loss = outputs.loss\n",
        "                perplexity = torch.exp(torch.tensor(loss)).item()\n",
        "                #Add it to the dict\n",
        "                metrics[\"perplexity\"] = perplexity # Include perplexity if calculated\n",
        "\n",
        "            predictions = np.argmax(predictions, axis=2) #argmax axis 2 for decoder only.\n",
        "            decoded_predictions = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "            labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
        "            decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        else: #encoder-decoder or other\n",
        "            predictions = np.argmax(predictions, axis=1)\n",
        "            decoded_predictions = predictions\n",
        "            decoded_labels = labels\n",
        "            # perplexity = None #Not needed, since you initialized metrics above.\n",
        "\n",
        "        references = [[label] for label in decoded_labels]\n",
        "\n",
        "        bleu_score = calculate_bleu_score(decoded_predictions, references)\n",
        "        f1_score = calculate_f1_score(decoded_predictions,decoded_labels)\n",
        "\n",
        "        rouge_scores = self.calculate_rouge(references, decoded_predictions)\n",
        "\n",
        "        # Add to the dict\n",
        "        metrics[\"bleu\"] = bleu_score\n",
        "        metrics[\"f1\"] = f1_score\n",
        "\n",
        "        #Add rouge\n",
        "        for rouge_type, scores in rouge_scores.items():\n",
        "                metrics[f\"{rouge_type}_precision\"] = scores['precision']\n",
        "                metrics[f\"{rouge_type}_recall\"] = scores['recall']\n",
        "                metrics[f\"{rouge_type}_f1\"] = scores['f1']\n",
        "\n",
        "\n",
        "        # Return the updated metrics\n",
        "        return metrics  # This line returns the dict\n",
        "\n",
        "\n",
        "    def compute_metricspoc(self, eval_pred):\n",
        "            \"\"\"\n",
        "            Computes the BLEU, F1, ROUGE, and perplexity metrics based on the task type.\n",
        "            \"\"\"\n",
        "            predictions, labels = eval_pred\n",
        "\n",
        "            # For sequence classification tasks (like MRPC), predictions are logits\n",
        "            if self.model_type == \"encoder-only\" and \"bert\" in self.model_id.lower():\n",
        "                predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "            # Initialize an empty dictionary to store metrics\n",
        "            metrics = {}\n",
        "\n",
        "            # Decode predictions and labels\n",
        "            if self.model_type == \"decoder-only\":\n",
        "                decoded_predictions = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "                labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
        "                decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "                # Calculate BLEU and F1 scores\n",
        "                references = [[label] for label in decoded_labels]\n",
        "                bleu_score = calculate_bleu_score(decoded_predictions, references)\n",
        "                f1_score = calculate_f1_score(decoded_predictions, decoded_labels)\n",
        "\n",
        "                # Add BLEU and F1 to the metrics dictionary\n",
        "                metrics[\"bleu\"] = bleu_score\n",
        "                metrics[\"f1\"] = f1_score\n",
        "\n",
        "                # Calculate ROUGE score\n",
        "                rouge = load(\"rouge\")\n",
        "                results = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "                metrics.update(results)  # Add ROUGE scores to the metrics dictionary\n",
        "\n",
        "                # Calculate perplexity (with eval_loss check)\n",
        "                try:\n",
        "                    # Check if eval_loss is available in the log history\n",
        "                    if self.trainer.state.log_history and \"eval_loss\" in self.trainer.state.log_history[-1]:\n",
        "                        eval_loss = self.trainer.state.log_history[-1][\"eval_loss\"]\n",
        "                        print(f\"eval_loss found: {eval_loss}\") # Debugging print statement\n",
        "\n",
        "                        perplexity = torch.exp(torch.tensor(eval_loss)).item()\n",
        "                        metrics[\"perplexity\"] = perplexity\n",
        "                    else:\n",
        "                        print(\"eval_loss not found in logs for perplexity calculation. This is expected early in training or if evaluation has not occurred.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in perplexity calculation: {e}\")\n",
        "                    print(f\"log_history: {self.trainer.state.log_history}\")\n",
        "\n",
        "                #Print all logs as Json file\n",
        "                print(f\"trainer.state.log_history: {json.dumps(self.trainer.state.log_history)}\")\n",
        "\n",
        "\n",
        "            else:  # For encoder-only models (like BERT)\n",
        "                decoded_predictions = predictions\n",
        "                decoded_labels = labels\n",
        "\n",
        "                # Calculate BLEU and F1 scores (might not be relevant for all tasks)\n",
        "                references = [[label] for label in decoded_labels]\n",
        "                bleu_score = calculate_bleu_score(decoded_predictions, references)\n",
        "                f1_score = calculate_f1_score(decoded_predictions, decoded_labels)\n",
        "\n",
        "                # Add BLEU and F1 to the metrics dictionary\n",
        "                metrics[\"bleu\"] = bleu_score\n",
        "                metrics[\"f1\"] = f1_score\n",
        "\n",
        "            return metrics\n",
        "\n",
        "    !pip install evaluate -q\n",
        "    from evaluate import load\n",
        "\n",
        "    def compute_metricsgood(self, eval_pred):\n",
        "        \"\"\"\n",
        "        Computes the BLEU and F1 scores.\n",
        "\n",
        "        Args:\n",
        "            eval_pred (tuple): A tuple containing predictions and labels.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the BLEU and F1 scores.\n",
        "        \"\"\"\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "        # Initialize an empty dictionary to store metrics\n",
        "        #metrics = {}\n",
        "\n",
        "        # Decode predictions and labels (if necessary)\n",
        "        if self.model_type == \"decoder-only\":\n",
        "          decoded_predictions = self.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "          labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
        "          decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "        else:\n",
        "          decoded_predictions = predictions\n",
        "          decoded_labels = labels\n",
        "\n",
        "        # Extract references\n",
        "        references = [[label] for label in decoded_labels]\n",
        "\n",
        "        bleu_score = calculate_bleu_score(decoded_predictions, references)\n",
        "        f1_score = calculate_f1_score(decoded_predictions,decoded_labels)\n",
        "\n",
        "        # Calculate ROUGE score\n",
        "        #rouge = load(\"rouge\")\n",
        "        #results = rouge.compute(predictions=decoded_predictions, references=decoded_labels)\n",
        "        #metrics.update(results)  # Add ROUGE scores to the metrics dictionary\n",
        "\n",
        "\n",
        "        # Add BLEU and F1 to the metrics dictionary\n",
        "        #metrics[\"bleu\"] = bleu_score\n",
        "        #metrics[\"f1\"] = f1_score\n",
        "\n",
        "        return {\"bleu\": bleu_score, \"f1\": f1_score}\n",
        "        #return metrics\n",
        "\n",
        "\n",
        "    def on_train_loss(self, loss):\n",
        "      \"\"\"Callback to store training losses.\"\"\"\n",
        "      self.train_losses.append(loss)\n",
        "\n",
        "    def on_eval_loss(self, loss):\n",
        "        \"\"\"Callback to store evaluation losses.\"\"\"\n",
        "        self.eval_losses.append(loss)\n",
        "    @timeit\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Executes the OODA loop and fine-tunes the language model.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(\"Starting Run ...\")\n",
        "        clear_memory()\n",
        "        self.start_time = time.time()\n",
        "        self._observe()\n",
        "        if self.model is None:\n",
        "            print(\"Model loading failed, skipping _orient, _decide and _act\")\n",
        "            return\n",
        "        self._orient()\n",
        "        self._decide()\n",
        "        self._act()\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Run Dataset: {self.dataset}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if self.trainer is not None:\n",
        "            try:\n",
        "                # Train the model\n",
        "                self.trainer.train()\n",
        "                print(\"\\n\")\n",
        "                print(\"Evaluation:\")\n",
        "                eval_results = self.evaluate()\n",
        "                print(\"\\n\")\n",
        "                print(eval_results)\n",
        "                print(\"\\n\")\n",
        "\n",
        "                # Create experiment_name\n",
        "                # Create experiment_name (using triple quotes)\n",
        "\n",
        "                experiment_name = f\"\"\"{self.model_id.replace('/', '-').replace(\"'\", '')}_{self.dataset_name.replace('/', '-').replace(\"'\", '')}\"\"\"\n",
        "                # Save eval_results using write()\n",
        "\n",
        "                import os\n",
        "                import json  # Import json module\n",
        "                current_directory = os.getcwd()\n",
        "                %cd /content/\n",
        "                results_file = os.path.join(current_directory, f\"{experiment_name}_results.txt\")\n",
        "                with open(results_file, \"w\") as f:  # Open in write mode (\"w\")\n",
        "                    json.dump(eval_results, f)  # Write eval_results as JSON\n",
        "                    print(f\"Saved evaluation results to: {results_file}\")  # Add a print statement for confirmation\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during training or evaluation: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            print(\"Trainer is None. Skipping training and evaluation.\")\n",
        "\n",
        "        self.end_time = time.time()\n",
        "\n",
        "        print(\"Run  finished.\")\n",
        "    @timeit\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Evaluates the fine-tuned language model.\n",
        "        \"\"\"\n",
        "        return self.trainer.evaluate()\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_aviationqa(self, examples):\n",
        "        print(\"Preprocess Dataset: sakharamg/AviationQA\")\n",
        "        max_length = self.config.get(\"max_length\", 128)\n",
        "\n",
        "        if self.model_type == \"encoder-only\":\n",
        "            # For encoder-only models like BERT (no changes here)\n",
        "            inputs = self.tokenizer(\n",
        "                examples[\"Question\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            with self.tokenizer.as_target_tokenizer():\n",
        "                labels = self.tokenizer(\n",
        "                    examples[\"Answer\"],\n",
        "                    max_length=max_length,\n",
        "                    truncation=True,\n",
        "                    padding=\"max_length\",\n",
        "                )\n",
        "            inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "            return inputs\n",
        "\n",
        "        elif self.model_type == \"decoder-only\":\n",
        "            # For decoder-only models like Mistral, Deepseek, Llama\n",
        "            # **Changes here:**\n",
        "            # 1. Combine Question and Answer into a single string\n",
        "            # 2. Pass this combined string as 'text' to the tokenizer\n",
        "\n",
        "            combined_text = [\n",
        "                f\"Question: {q} Answer: {a}\"\n",
        "                for q, a in zip(examples[\"Question\"], examples[\"Answer\"])\n",
        "            ]\n",
        "\n",
        "            inputs = self.tokenizer(\n",
        "                combined_text,  # Pass the combined text\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "\n",
        "            # Create labels (same logic as before)\n",
        "            inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
        "            inputs[\"labels\"] = [\n",
        "                [-100 if token == self.tokenizer.pad_token_id else token for token in labels]\n",
        "                for labels in inputs[\"labels\"]\n",
        "            ]\n",
        "            return inputs\n",
        "\n",
        "        else:\n",
        "            print(f\"Model type {self.model_type} not supported for preprocessing.\")\n",
        "            return {}\n",
        "\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_flight_data(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the flight data for fine-tuning.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: Flight Data\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        # Assuming examples is a list of dictionaries with 'text' key\n",
        "        inputs = [example['text'] for example in examples]\n",
        "\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            # Mistral, DeepSeek, and other decoder-only models\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels (same as inputs for causal language modeling)\n",
        "            labels_tokenized = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "            model_inputs[\"labels\"] = [\n",
        "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
        "            ]\n",
        "        elif self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels (same as inputs for masked language modeling, etc.)\n",
        "            labels_tokenized = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_mrpc(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the SetFit/mrpc dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: SetFit/mrpc\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 128)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            inputs = self.tokenizer(\n",
        "                examples[\"text1\"],\n",
        "                examples[\"text2\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            inputs[\"labels\"] = examples[\"label\"]\n",
        "            return inputs\n",
        "        elif self.model_type == \"decoder-only\":\n",
        "             # Decoder-only models are not supported for the MRPC task.\n",
        "            print(\"Decoder-only models are not supported for the MRPC task.\")\n",
        "            return {}\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_sql_create_context(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the b-mc2/sql-create-context dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: b-mc2/sql-create-context\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            # Mistral, DeepSeek, and other decoder-only models\n",
        "            # Tokenize inputs and labels\n",
        "            inputs = [f\"### Question: {q} ### Context: {c}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"answer\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Assign labels to model_inputs\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "            model_inputs[\"labels\"] = [\n",
        "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
        "            ]\n",
        "        elif self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            # Tokenize inputs and labels\n",
        "            inputs = [f\"### Question: {q} ### Context: {c}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"answer\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Assign labels to model_inputs\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_anthropic_hh_rlhf(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the anthropic/hh-rlhf dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: anthropic/hh-rlhf\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            # Mistral, DeepSeek, and other decoder-only models\n",
        "            inputs = examples[\"chosen\"]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"chosen\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "            model_inputs[\"labels\"] = [\n",
        "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
        "            ]\n",
        "        elif self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            inputs = examples[\"chosen\"]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"chosen\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "\n",
        "    @timeit\n",
        "    def _preprocess_function_imdb(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the imdb dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: imdb\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"encoder-only\":\n",
        "             # BERT and other encoder-only models\n",
        "            inputs = self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            inputs[\"labels\"] = examples[\"label\"]\n",
        "            return inputs\n",
        "        elif self.model_type == \"decoder-only\":\n",
        "            # Decoder-only models (Mistral, DeepSeek, etc.)\n",
        "            model_inputs = self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            # Copy input_ids to labels for causal LM training\n",
        "            model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "            model_inputs[\"labels\"] = [\n",
        "                [(l if l != self.tokenizer.pad_token_id else -100) for l in label] for label in model_inputs[\"labels\"]\n",
        "            ]\n",
        "\n",
        "            return model_inputs\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")"
      ],
      "metadata": {
        "id": "ucf-t1uXN3Oo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = [\n",
        "        {\n",
        "            \"max_length\": 32,\n",
        "            \"quantization\": True,\n",
        "            \"use_unsloth\": False,\n",
        "            \"lora\": True,\n",
        "            \"dataset_size\": 125,\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./output\",\n",
        "                \"per_device_train_batch_size\": 4,\n",
        "                \"gradient_accumulation_steps\": 8,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"num_train_epochs\": 5,\n",
        "                \"max_steps\": 250,\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"logging_steps\": 10,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"eval_steps\": 20,\n",
        "                \"report_to\": \"none\",\n",
        "                \"save_steps\": 20,\n",
        "                \"evaluation_strategy\":\"steps\",\n",
        "                \"eval_steps\":20,\n",
        "                \"logging_strategy\":\"steps\",\n",
        "                \"load_best_model_at_end\":True,\n",
        "                \"metric_for_best_model\":\"eval_loss\",\n",
        "\n",
        "            },\n",
        "        },\n",
        "    ]\n",
        "\n",
        "agent = FineTuningAgent(\n",
        "        model_id=\"deepseek-ai/deepseek-coder-1.3b-base\",\n",
        "        dataset_name=\"sakharamg/AviationQA\",\n",
        "        config=configs[0])\n",
        "#agent.run()"
      ],
      "metadata": {
        "id": "MPBFotSWahfx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment Setup and Execution"
      ],
      "metadata": {
        "id": "S99Umzgf8OwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Experiment Setup and Execution\n",
        "import os  # Import os module\n",
        "\n",
        "class MetricCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    A callback class to add metrics to the trainer.\n",
        "    \"\"\"\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "\n",
        "    def on_train_begin(self, args, state, control, model=None, **kwargs):\n",
        "        # self.agent.trainer.compute_metrics = self.agent.compute_metrics # removed\n",
        "        pass # removed\n",
        "\n",
        "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
        "      \"\"\"Callback to add metrics to self.trainer.\"\"\"\n",
        "      self.agent.trainer.compute_metrics = self.agent.compute_metrics # Added\n",
        "\n",
        "\n",
        "class LossLoggingCallback(TrainerCallback):\n",
        "    \"\"\"Callback to log training and evaluation losses.\"\"\"\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        \"\"\"Logs the training loss at each log step.\"\"\"\n",
        "        if logs and \"loss\" in logs:\n",
        "            self.agent.on_train_loss(logs[\"loss\"])\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        \"\"\"Logs the evaluation loss at each evaluation step.\"\"\"\n",
        "        if metrics and \"eval_loss\" in metrics:\n",
        "            self.agent.on_eval_loss(metrics[\"eval_loss\"])\n",
        "\n",
        "\n",
        "\n",
        "def create_rl_pairs():\n",
        "    \"\"\"\n",
        "    Creates a list of all possible combinations of datasets, models,\n",
        "    and configurations for RL experiments.\n",
        "    \"\"\"\n",
        "\n",
        "    datasets = [\n",
        "        #\"SetFit/mrpc\",\n",
        "        #\"b-mc2/sql-create-context\",\n",
        "        #\"anthropic/hh-rlhf\",\n",
        "        #\"imdb\",\n",
        "        \"sakharamg/AviationQA\",\n",
        "        #\"bhavikjikadara/us-airline-flight-routes-and-fares-1993-2024\"\n",
        "    ]\n",
        "    models = [\n",
        "\n",
        "        #\"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "        #\"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "        #\"bert-base-uncased\",\n",
        "        \"mistralai/Mistral-7B-v0.1\",\n",
        "        #\"deepseek-ai/deepseek-coder-1.3b-base\",\n",
        "    ]\n",
        "\n",
        "    # Define different configs\n",
        "    configs = [\n",
        "        {\n",
        "            \"max_length\": 1000,\n",
        "\n",
        "            \"quantization\": True,\n",
        "            \"use_unsloth\": False,\n",
        "            \"lora\": True,\n",
        "            \"dataset_size\": 8000,\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./output\",\n",
        "                \"use_cache\": False,\n",
        "                \"per_device_train_batch_size\": 4,\n",
        "                \"gradient_accumulation_steps\": 8,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"num_train_epochs\": 5,\n",
        "                \"max_steps\": 300,\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"logging_steps\": 10,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"eval_steps\": 20,\n",
        "                \"report_to\": \"none\",\n",
        "                \"save_steps\": 20,\n",
        "                \"evaluation_strategy\":\"steps\",\n",
        "                \"eval_steps\":20,\n",
        "                \"logging_strategy\":\"steps\",\n",
        "                \"load_best_model_at_end\":True,\n",
        "                \"metric_for_best_model\":\"eval_loss\",\n",
        "\n",
        "            },\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    rl_pairs = []\n",
        "    for dataset, model, config in itertools.product(datasets, models, configs):\n",
        "        rl_pairs.append((dataset, model, copy.deepcopy(config))) # Use copy.deepcopy()\n",
        "\n",
        "    return rl_pairs\n",
        "\n",
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "import time\n",
        "from transformers import TrainingArguments, TrainerState, TrainerControl\n",
        "import ast  # Import ast for literal_eval\n",
        "\n",
        "def generate_report(\n",
        "    rl_pairs, agents, training_args_list, state_list, control_list, output_file=\"experiment_report.txt\", experiment_name=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a report for multiple RL experiments, including evaluation scores and training details.\n",
        "\n",
        "    Args:\n",
        "        rl_pairs (list): A list of tuples, each containing (dataset_name, model_id, config).\n",
        "        agents (list): A list of FineTuningAgent objects corresponding to the experiments.\n",
        "        training_args_list (list): A list of TrainingArguments objects for each experiment.\n",
        "        state_list (list): A list of TrainerState objects for each experiment.\n",
        "        control_list (list): A list of TrainerControl objects for each experiment.\n",
        "        output_file (str): The name of the output file to save the report.\n",
        "        experiment_name (str, optional): The base name for the experiment results file.\n",
        "                                          If provided, it will be used to load the results.\n",
        "                                          Defaults to None.\n",
        "    \"\"\"\n",
        "    if not (\n",
        "        len(rl_pairs)\n",
        "        == len(agents)\n",
        "        == len(training_args_list)\n",
        "        == len(state_list)\n",
        "        == len(control_list)\n",
        "    ):\n",
        "        raise ValueError(\"The number of rl_pairs, agents, training_args, state, and control must be the same.\")\n",
        "\n",
        "    report_data = []\n",
        "    for (dataset_name, model_id, config), agent, training_args, state, control in zip(\n",
        "        rl_pairs, agents, training_args_list, state_list, control_list\n",
        "    ):\n",
        "\n",
        "        # *** Load results from file ***\n",
        "        if experiment_name:\n",
        "            results_file = f\"{experiment_name}_results.txt\"  # Use provided experiment_name and .txt extension\n",
        "        else:\n",
        "            results_file = f\"{dataset_name}_{model_id}_{agent.counter}_results.txt\"  # Default format with .txt extension\n",
        "\n",
        "        try:\n",
        "            with open(results_file, \"r\") as f:  # Open in read mode (\"r\") for text files\n",
        "                eval_results_str = f.read()  # Read the contents as a string\n",
        "                # Try to parse eval_results_str as a Python literal (e.g., dictionary)\n",
        "                try:\n",
        "                    eval_results = ast.literal_eval(eval_results_str)\n",
        "                except (SyntaxError, ValueError):\n",
        "                    print(f\"Error parsing eval_results_str for experiment: {results_file}\")\n",
        "                    eval_results = None\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Results file not found for experiment: {results_file}\")\n",
        "            eval_results = None  # Set to None if file not found\n",
        "\n",
        "        # Collect the data\n",
        "        elapsed_time = agent.end_time - agent.start_time if agent.start_time and agent.end_time else np.nan  # Handle potential errors\n",
        "\n",
        "        train_losses = agent.train_losses\n",
        "        eval_losses = agent.eval_losses\n",
        "\n",
        "        if not train_losses:\n",
        "            train_std = np.nan  # Use np.nan for no data\n",
        "            min_train_loss = np.nan\n",
        "            max_train_loss = np.nan\n",
        "        else:\n",
        "            train_std = np.std(train_losses)\n",
        "            min_train_loss = np.min(train_losses)\n",
        "            max_train_loss = np.max(train_losses)\n",
        "\n",
        "        if not eval_losses:\n",
        "            eval_std = np.nan\n",
        "            min_eval_loss = np.nan\n",
        "            max_eval_loss = np.nan\n",
        "        else:\n",
        "            eval_std = np.std(eval_losses)\n",
        "            min_eval_loss = np.min(eval_losses)\n",
        "            max_eval_loss = np.max(eval_losses)\n",
        "\n",
        "        # *** Extract BLEU and F1 scores from eval_results ***\n",
        "        if eval_results is not None:  # Check if eval_results were loaded successfully\n",
        "            bleu_score = eval_results.get(\"eval_bleu\", np.nan)  # Get BLEU score, default to NaN if not found\n",
        "            f1_score = eval_results.get(\"eval_f1\", np.nan)  # Get F1 score, default to NaN if not found\n",
        "        else:\n",
        "            bleu_score = np.nan  # Set to NaN if eval_results are None\n",
        "            f1_score = np.nan\n",
        "\n",
        "        # Check if training_args is None before accessing its attributes\n",
        "        learning_rate = training_args.learning_rate if training_args is not None else np.nan\n",
        "        batch_size = training_args.per_device_train_batch_size if training_args is not None else np.nan\n",
        "        epochs = training_args.num_train_epochs if training_args is not None and hasattr(training_args, \"num_train_epochs\") else \"n/a\"\n",
        "\n",
        "        report_data.append(\n",
        "            [\n",
        "                dataset_name,\n",
        "                model_id,\n",
        "                f\"{elapsed_time:.2f} seconds\",  # Format to 2 decimal places\n",
        "                f\"{train_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{eval_std:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_train_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{min_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{max_eval_loss:.4f}\",  # Format to 4 decimal places\n",
        "                f\"{bleu_score:.4f}\",  # Format to 4 decimal places  # Include BLEU score\n",
        "                f\"{f1_score:.4f}\",  # Format to 4 decimal places  # Include F1 score\n",
        "                f\"{learning_rate:.4f}\",  # Learning rate\n",
        "                batch_size,  # Batch size\n",
        "                epochs, # Epochs\n",
        "                state.global_step if state else \"n/a\",  # Global steps\n",
        "                state.epoch if state else \"n/a\",  # Epoch\n",
        "                state.is_local_process_zero if state else \"n/a\",\n",
        "                control.should_training_stop if control else \"n/a\",\n",
        "                control.should_log if control else \"n/a\",\n",
        "                control.should_save if control else \"n/a\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    headers = [\n",
        "        \"Dataset\",\n",
        "        \"Model\",\n",
        "        \"Elapsed Time\",\n",
        "        \"Train Loss Std\",\n",
        "        \"Eval Loss Std\",\n",
        "        \"Min Train Loss\",\n",
        "        \"Max Train Loss\",\n",
        "        \"Min Eval Loss\",\n",
        "        \"Max Eval Loss\",\n",
        "        \"BLEU Score\",  # Include header for BLEU Score\n",
        "        \"F1 Score\",  # Include header for F1 Score\n",
        "        \"Learning Rate\",\n",
        "        \"Batch Size\",\n",
        "        \"Epochs\",\n",
        "        \"Global Steps\",\n",
        "        \"Epoch\",\n",
        "        \"Is Local Process Zero\",\n",
        "        \"Should Training Stop\",\n",
        "        \"Should Log\",\n",
        "        \"Should Save\",\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Format the report as a table\n",
        "    report_table = tabulate(report_data, headers=headers, tablefmt=\"grid\")\n",
        "\n",
        "    # Print the report to the console\n",
        "    print(report_table)\n",
        "\n",
        "    # Save the report to a file\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(report_table)\n",
        "        print(f\"Report saved to {output_file}\")\n",
        "\n",
        "rl_pairs = create_rl_pairs()\n",
        "# Run the experiment\n",
        "import time\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING before running the experiment loop\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Enable synchronous CUDA operations\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'  # Enable device-side assertions\n",
        "\n",
        "agents = []\n",
        "training_args_list = []\n",
        "state_list = []\n",
        "control_list = []\n",
        "experiment_names = []\n",
        "\n",
        "for dataset_name, model_id, config in rl_pairs:\n",
        "    clear_memory()\n",
        "    print(\"\\n\")\n",
        "    print(f\"Running experiment with:\")\n",
        "    print(f\"- Dataset: {dataset_name}\")\n",
        "    print(f\"- Model: {model_id}\")\n",
        "    print(f\"- Config: {config}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    try:\n",
        "        agent = FineTuningAgent(model_id, dataset_name, config)\n",
        "        agents.append(agent) # Append the agent to the list immediately\n",
        "        agent.start_time = time.time()\n",
        "        agent.run()\n",
        "        agent.end_time = time.time()\n",
        "        # Collect training details after training\n",
        "        if agent.trainer is not None:\n",
        "          # Store experiment name and other relevant data\n",
        "            experiment_name = f\"\"\"{model_id.replace('/', '-').replace(\"'\", '')}_{dataset_name.replace('/', '-').replace(\"'\", '')}\"\"\"\n",
        "            experiment_names.append(experiment_name)\n",
        "            # agents.append(agent) # Removed, agent has already been appended above\n",
        "            training_args_list.append(agent.training_args)\n",
        "            state_list.append(agent.trainer.state)\n",
        "            control_list.append(agent.trainer.control)\n",
        "        else:  # Append dummy values if training failed\n",
        "            training_args_list.append(None)  # or a suitable placeholder\n",
        "            state_list.append(None)\n",
        "            control_list.append(None)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the experiment: {e}\")\n",
        "        agent.end_time = time.time()\n",
        "        agent.start_time = time.time()\n",
        "        training_args_list.append(None)  # or a suitable placeholder\n",
        "        state_list.append(None)\n",
        "        control_list.append(None)\n",
        "\n",
        "# Call generate_report outside the loop, after all experiments are done\n",
        "#generate_report(rl_pairs, agents, training_args_list, state_list, control_list, experiment_name=experiment_names)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f9b740ba8af04e20b21c6fdc0f713d8d",
            "b7e9ebda0df940dca8b94dc64d8fb803",
            "9fba5d4adc1345ddb50766936978ff4c",
            "48896193c36f42d58c8443296d906a3e",
            "ced9ed3627f5457980ae0fea83f98b31",
            "b1fb62c3ccf64dd98cb8e6515ec11f29",
            "442152a7ca7c42eda72d86d7962d65b5",
            "fbc423a0c98d43aeb2fe3a7d5984a048",
            "5b2c342b179d48d5b57cbbdedb238932",
            "a85242cf8f6f43ce802581ab9e6f291a",
            "8ec923bf28cb43be802cd43795d75e17",
            "4213e9f5fdf14547bcac04ea4d179b60",
            "94aceb78ecec4cbdaeaaa88cadc2819d",
            "34a92af60fc5405a9a6e82925f0ef2e7",
            "06f159842a8e4ff888a09012d7c463c6",
            "301fb9515f8541fa9e53b48490f690e1",
            "522fdd3a2f8740189488a2dafe7e86ec",
            "eb0365d90dfd4610bb02cf529aaec307",
            "da2cf2e126984b689b986922a8ca4a55",
            "cea7ee1ba2054625a5d6907389f78578",
            "68bfe7f43e1448c89c0441af4e31f2f0",
            "93ab8581f4b645e5bf46a479baaf8c22",
            "12d37c05772d4c5dadc5112e8d63a844",
            "d509c572cca34815a6215fa291e923bb",
            "100951f576624602bae292b5045b6a87",
            "8469d0df1a9546f69c302eb66bbbfc6d",
            "13a175694e4043c7b0bf524ce4c93cd5",
            "3f4017a8c7f54a598d15b1b110aa5d7d",
            "b2385ec9e7ee4f80b27c9202a9adf177",
            "9efcfb692b524f5198c1d7f7ea0dc78f",
            "40aceefa7b4940fcb47f0782a2289825",
            "17a41dc8de174f6c9a9c1ae94ff79f4e",
            "f465221c851b4946b899c547a18ee081",
            "d67589f202324619a2e0440f84be2839",
            "85a5d44ad176472384a2ade49d600396",
            "b935afc2fa9644c29cc49255df5e227f",
            "edd5ff6628224876b5f8d617f7510908",
            "8a0215ac9c734093851ad94c65f82177",
            "ec34d8c9ae594ca990f7a862a01d45e0",
            "48e2c76103db4bc0ac67a4c0819cad91",
            "b2a8540a089d4107865a168c5d434f6c",
            "ceb8d70b5c8147f2a49d41bf5a4c0910",
            "f8ef351fb3c244728785cf0c635e1d1d",
            "17aea45a21534a839be3d91f77204f59",
            "ba1491ea0a11450b8f3ffeac62907436",
            "710b1990ea3a4d10a7698b27370b17fe",
            "e1f5f92a73824c2982957c7206e1ee6a",
            "0f036dc6290b4ecab8a5e29a5ad1a16c",
            "a8a3a0edeb234d268c42ce59a8e69f00",
            "1c02e250039f436292af1aa0a57e7e16",
            "0b4ddb96231e432a8e1f7f24f654fe44",
            "a02e2d18715c4d2880e36a6d628a055d",
            "3a00c3047d6c4af8b77f90d7596bea80",
            "828da4956bf74c3f8107eae5b79bc2d6",
            "051a57344b5f4de9966159d8aee2c9f0",
            "65e7146629f5425bac17099edb93dc7c",
            "f08410f16d8f4c278be4c708848182d3",
            "5a606fd4980d4ada8fead6d4b0cdf6f3",
            "b31393d58ae84197acbac5da9f64aafc",
            "25820bf546e640bf91b7e53fb6ac7ae9",
            "94604222a53642aca440c3cf08a6fd03",
            "4a93ec92c4854c0e935ace5cc8416855",
            "5c58adc883b249b9821cec61d045da53",
            "17647d8504e84fe2a15ae563b6bc97ee",
            "a2851a4436914ba39e1f103f2c949636",
            "d96667f4e3544606a99b1646f548be9d",
            "5e36d247bf07498199bb3801cc7622f1",
            "6faff2d52422472c82f3925257cdef04",
            "26921eb8757b44a2b84b646998f793a2",
            "215738d54685457aa956a1c180a06720",
            "98076679c1fa42dfb6b88a73dbd06034",
            "dd98a94fd6054c49bb20f7ac6b029e31",
            "d79630a8c4ab4b00b89a7b15677903c6",
            "d4d8b0cfb2c1422e9e3590d467c21c18",
            "07caed6b7a0b4c5490ba7632bbb016c4",
            "b1291eb48f2941bba6cb7d8a582cab59",
            "17955798c1984bd7b49f60f2a6e0ae80",
            "99079959cbb947fabc384f9da7a4f813",
            "d99f3dcc8fab4285ae1b2566da1404b3",
            "1dc0f6a9b8fb4944a3fc9f996d41c05f",
            "1a449dea5e014b01a93e127d7d272a3e",
            "90f78ef9f99d4b4ea9bf4cabab59067c",
            "bce643e4caf34eaa9ca013edac08da49",
            "5509a96b74734b4f83607cbeeda96130",
            "931574fc85db4d7591a83f2172238130",
            "27b42565ab6c4f8e8957702c3c1e2446",
            "2d0f9bc93cee40ee829adfa806f6d222",
            "9302b05c981f4076ad9a55371bbf8366"
          ]
        },
        "id": "RemE3xmbN-Af",
        "outputId": "403b6901-4c84-41ff-9a57-3c6c3df0fa3d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Running experiment with:\n",
            "- Dataset: sakharamg/AviationQA\n",
            "- Model: mistralai/Mistral-7B-v0.1\n",
            "- Config: {'max_length': 1000, 'quantization': True, 'use_unsloth': False, 'lora': True, 'dataset_size': 8000, 'dataset_num_proc': 2, 'test_split_percentage': 0.2, 'training_args': {'output_dir': './output', 'use_cache': False, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 8, 'warmup_steps': 5, 'num_train_epochs': 5, 'max_steps': 300, 'learning_rate': 0.0002, 'logging_steps': 10, 'weight_decay': 0.01, 'eval_steps': 20, 'report_to': 'none', 'save_steps': 20, 'evaluation_strategy': 'steps', 'logging_strategy': 'steps', 'load_best_model_at_end': True, 'metric_for_best_model': 'eval_loss'}}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Starting Run ...\n",
            "Starting Observe ...\n",
            "Mistral model detected. Using 4-bit quantization.\n",
            "Decoder-only model detected.\n",
            "Loading Decoder-only with Hugging Face\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9b740ba8af04e20b21c6fdc0f713d8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
            "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset AviationQA .....\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/1058 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4213e9f5fdf14547bcac04ea4d179b60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/12 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12d37c05772d4c5dadc5112e8d63a844"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating json from Arrow format:   0%|          | 0/11 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d67589f202324619a2e0440f84be2839"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba1491ea0a11450b8f3ffeac62907436"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65e7146629f5425bac17099edb93dc7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Dataset - Loaded  Observe: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'Question', 'Answer'],\n",
            "        num_rows: 8000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['id', 'Question', 'Answer'],\n",
            "        num_rows: 8000\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Observe finished.\n",
            "Function _observe took 18.8064 seconds to execute\n",
            "\n",
            "\n",
            "Starting Orient ...\n",
            "Dataset: AviationQA\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e36d247bf07498199bb3801cc7622f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.7040 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.6032 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5749 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5634 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5283 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5830 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5551 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5534 seconds to execute\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99079959cbb947fabc384f9da7a4f813"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5634 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5971 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5679 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5122 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5479 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5760 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5481 seconds to execute\n",
            "Preprocess Dataset: sakharamg/AviationQA\n",
            "Function _preprocess_function_aviationqa took 4.5409 seconds to execute\n",
            "\n",
            "\n",
            "Orient Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 8000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 8000\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "Orient finished.\n",
            "Function _orient took 279.5486 seconds to execute\n",
            "\n",
            "\n",
            "Starting Decide ...\n",
            "trainable params: 20,971,520 || all params: 7,262,711,808 || trainable%: 0.2888\n",
            "\n",
            "\n",
            "Decide finished.\n",
            "Function _decide took 0.9124 seconds to execute\n",
            "\n",
            "\n",
            "Starting Act ...\n",
            "Dataset preprocessed successfully.\n",
            "\n",
            "\n",
            "Hugging Face data collator used.\n",
            "Initializing Trainer...\n",
            "\n",
            "\n",
            "Act finished.\n",
            "Function _act took 0.4315 seconds to execute\n",
            "\n",
            "\n",
            "Run Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 8000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 8000\n",
            "    })\n",
            "})\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  7/300 06:46 < 6:37:09, 0.01 it/s, Epoch 0.02/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## llm report"
      ],
      "metadata": {
        "id": "BRmauENT7tWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Used to securely store your API key\n",
        "GOOGLE_API_KEY = userdata.get('GEMINI')  # Replace 'GEMINI' with your actual userdata variable name\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "from tabulate import tabulate\n",
        "from transformers import TrainingArguments, TrainerState, TrainerControl\n",
        "\n",
        "def generate_llm_report(\n",
        "    rl_pairs,\n",
        "    agents,\n",
        "    training_args_list,\n",
        "    state_list,\n",
        "    control_list,\n",
        "    output_file=\"experiment_report.txt\",\n",
        "    experiment_name=None,\n",
        "    prompt=\"You are a helpful data science expert.\\nPlease, make an additional analysis of this Fine-Tuning experiment report.\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a report for multiple LLM experiments, including evaluation scores and training details,\n",
        "    and provides an analysis using Google Gemini.\n",
        "\n",
        "    Args:\n",
        "        rl_pairs (list): A list of tuples, each containing (dataset_name, model_id, config).\n",
        "        agents (list): A list of FineTuningAgent objects corresponding to the experiments.\n",
        "        training_args_list (list): A list of TrainingArguments objects for each experiment.\n",
        "        state_list (list): A list of TrainerState objects for each experiment.\n",
        "        control_list (list): A list of TrainerControl objects for each experiment.\n",
        "        output_file (str): The name of the output file to save the report.\n",
        "        experiment_name (str, optional): The base name for the experiment results file.\n",
        "                                        If provided, it will be used to load the results. Defaults to None.\n",
        "        prompt (str, optional): The prompt to provide to Google Gemini for analysis.\n",
        "                                Defaults to a generic data science expert prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    if not (\n",
        "        len(rl_pairs)\n",
        "        == len(agents)\n",
        "        == len(training_args_list)\n",
        "        == len(state_list)\n",
        "        == len(control_list)\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"The number of rl_pairs, agents, training_args, state, and control must be the same.\"\n",
        "        )\n",
        "\n",
        "    report_data = []  # Initialize report_data here\n",
        "\n",
        "    for (\n",
        "        (dataset_name, model_id, config),\n",
        "        agent,\n",
        "        training_args,\n",
        "        state,\n",
        "        control,\n",
        "    ) in zip(rl_pairs, agents, training_args_list, state_list, control_list):\n",
        "        # Get eval_results from the agent\n",
        "\n",
        "        #print(f\"Model ID: {model_id}\")\n",
        "\n",
        "        experiment_name = f\"\"\"{model_id.replace('/', '-').replace(\"'\", '')}_{dataset_name.replace('/', '-').replace(\"'\", '')}\"\"\"\n",
        "\n",
        "        results_file = f\"{experiment_name}_results.txt\"\n",
        "        #print(f\"Results File: {results_file}\")\n",
        "\n",
        "\n",
        "        from pathlib import Path\n",
        "        # Define the file path\n",
        "        file_path = Path(results_file)\n",
        "\n",
        "        if file_path.exists():\n",
        "            #print(\"File exists!\")\n",
        "            #print(f\"Results File: {results_file}\")\n",
        "            print\n",
        "            #return\n",
        "        else:\n",
        "            #print(\"File does not exist.\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        training_args=agent.training_args\n",
        "\n",
        "        experiment_name = f\"\"\"{model_id.replace('/', '-').replace(\"'\", '')}_{dataset_name.replace('/', '-').replace(\"'\", '')}\"\"\"\n",
        "        #print(f\"Experiment Name: {experiment_name}\")\n",
        "\n",
        "        results_file = f\"{experiment_name}_results.txt\"\n",
        "        #print(f\"Results File: {results_file}\")\n",
        "\n",
        "        #print(training_args_list)\n",
        "\n",
        "\n",
        "        # \"eval_loss\": 6.17133903503418, \"eval_bleu\": 0, \"eval_f1\": 0.0, \"eval_runtime\": 4.0188, \"eval_samples_per_second\": 6.221, \"eval_steps_per_second\": 0.995, \"epoch\": 8.64}\n",
        "\n",
        "        try:\n",
        "            with open(results_file, \"r\") as f:\n",
        "                evaluation_results = json.load(f)\n",
        "            bleu_score = evaluation_results.get(\"eval_bleu\")\n",
        "            f1_score = evaluation_results.get(\"eval_f1\")\n",
        "            #print(f\"BLEU Score: {bleu_score}, F1 Score: {f1_score}\")\n",
        "            #print(f\"Eval Results: {evaluation_result}\")\n",
        "\n",
        "             # Accessing elements of the ROUGE score tuple (assuming it's a tuple with precision, recall, fmeasure)\n",
        "            rouge1_precision = evaluation_results.get('eval_rouge1', {}).get('precision')\n",
        "            rouge1_recall = evaluation_results.get('eval_rouge1', {}).get('recall')\n",
        "            rouge1_fmeasure = evaluation_results.get('eval_rouge1', {}).get('fmeasure')\n",
        "            #print(f\"ROUGE-1 Precision: {rouge1_precision}, Recall: {rouge1_recall}, F-measure: {rouge1_fmeasure}\")\n",
        "\n",
        "            # Accessing elements of the ROUGE score tuple (assuming it's a tuple with precision, recall, fmeasure)\n",
        "\n",
        "\n",
        "            rouge2_precision = evaluation_results.get('eval_rouge2', {}).get('precision')\n",
        "            rouge2_recall = evaluation_results.get('eval_rouge2', {}).get('recall')\n",
        "            rouge2_fmeasure = evaluation_results.get('eval_rouge2', {}).get('fmeasure')\n",
        "\n",
        "\n",
        "            # Accessing elements of the ROUGE score tuple (assuming it's a tuple with precision, recall, fmeasure)\n",
        "            rougeL_precision = evaluation_results.get('eval_rougeL', {}).get('precision')\n",
        "            rougeL_recall = evaluation_results.get('eval_rougeL', {}).get('recall')\n",
        "            rougeL_fmeasure = evaluation_results.get('eval_rougeL', {}).get('fmeasure')\n",
        "\n",
        "\n",
        "           # {\"eval_loss\": 3.720398187637329, \"eval_bleu\": 0, \"eval_f1\": 0.0, \"eval_rouge1\": {\"precision\": 0.396081703619663, \"recall\": 0.40836230306837384, \"fmeasure\": 0.40190438521178246}, \"eval_rouge2\": {\"precision\": 0.07832895449176241, \"recall\": 0.08075765617251687, \"fmeasure\": 0.07946492337001675}, \"eval_rougeL\": {\"precision\": 0.3623965364533813, \"recall\": 0.37335556072231263, \"fmeasure\": 0.3675868060434595},\n",
        "            #\"eval_perplexity\": 45.677459716796875, \"eval_runtime\": 1.1153, \"eval_samples_per_second\": 22.415, \"eval_steps_per_second\": 3.586, \"epoch\": 62.64}\n",
        "\n",
        "            perplexity = evaluation_results.get(\"eval_perplexity\", \"N/A\")\n",
        "            #print(f\"Perplexity: {perplexity}\")\n",
        "\n",
        "        except (FileNotFoundError, json.JSONDecodeError) as e:\n",
        "            print(f\"Error loading results: {e}\")\n",
        "            bleu_score = None\n",
        "            f1_score = None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Collect the data\n",
        "        elapsed_time = (\n",
        "            agent.end_time - agent.start_time\n",
        "            if agent.start_time and agent.end_time\n",
        "            else np.nan\n",
        "        )  # Handle potential errors\n",
        "        train_losses = agent.train_losses\n",
        "        eval_losses = agent.eval_losses\n",
        "\n",
        "        if not train_losses:\n",
        "            train_std = np.nan  # Use np.nan for no data\n",
        "            min_train_loss = np.nan\n",
        "            max_train_loss = np.nan\n",
        "        else:\n",
        "            #print(f\"Train Losses: {train_losses}\")\n",
        "            train_loss = np.mean(train_losses)\n",
        "            train_loss_std = np.std(train_losses)\n",
        "            min_train_loss = np.min(train_losses)\n",
        "            max_train_loss = np.max(train_losses)\n",
        "\n",
        "        if not eval_losses:\n",
        "            eval_std = np.nan\n",
        "            min_eval_loss = np.nan\n",
        "            max_eval_loss = np.nan\n",
        "        else:\n",
        "            #print(f\"Eval Losses: {eval_losses}\")\n",
        "            eval_loss = np.mean(eval_losses)\n",
        "            eval_loss_std = np.std(eval_losses)\n",
        "            min_eval_loss = np.min(eval_losses)\n",
        "            max_eval_loss = np.max(eval_losses)\n",
        "\n",
        "        # Check if training_args is None before accessing its attributes\n",
        "        learning_rate = training_args.learning_rate if training_args is not None else np.nan\n",
        "        batch_size = training_args.per_device_train_batch_size if training_args is not None else np.nan\n",
        "        epochs = training_args.num_train_epochs if training_args is not None and hasattr(training_args, \"num_train_epochs\") else \"n/a\"\n",
        "        #print(f\"Learning Rate: {learning_rate}\")\n",
        "        #print(f\"Batch Size: {batch_size}\")\n",
        "        #print(f\"Epochs: {epochs}\")\n",
        "\n",
        "        report_data.append(\n",
        "            [\n",
        "                dataset_name,\n",
        "                model_id,\n",
        "                f\"{elapsed_time:.2f} seconds\",  # Format to 2 decimal places\n",
        "                f\"{train_loss:.4f}\" if train_loss is not None else \"N/A\",  # Handle None case for train_loss\n",
        "                f\"{eval_loss:.4f}\" if eval_loss is not None else \"N/A\",  # Handle None case for eval_loss\n",
        "\n",
        "                f\"{train_loss_std:.4f}\" if train_loss_std is not None else \"N/A\",  # Handle None case for train_loss_std\n",
        "                f\"{eval_loss_std:.4f}\" if eval_loss_std is not None else \"N/A\",  # Handle None case for eval_std\n",
        "\n",
        "                f\"{min_train_loss:.4f}\" if min_train_loss is not None else \"N/A\",  # Handle None case for min_train_loss\n",
        "                f\"{max_train_loss:.4f}\" if max_train_loss is not None else \"N/A\",  # Handle None case for max_train_loss\n",
        "\n",
        "                f\"{min_eval_loss:.4f}\" if min_eval_loss is not None else \"N/A\",  # Handle None case for min_eval_loss\n",
        "                f\"{max_eval_loss:.4f}\" if max_eval_loss is not None else \"N/A\",  # Handle None case for max_eval_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                f\"{bleu_score:.4f}\" if bleu_score is not None else \"N/A\",  # Handle None case for bleu_score\n",
        "                f\"{f1_score:.4f}\" if f1_score is not None else \"N/A\",  # Handle None case for f1_score\n",
        "\n",
        "                f\"{rouge1_precision:.4f}\" if rouge1_precision is not None else \"N/A\",  # Handle None case for rouge1_precision\n",
        "                f\"{rouge1_recall:.4f}\" if rouge1_recall is not None else \"N/A\",  # Handle None case for rouge1_recall\n",
        "                f\"{rouge1_fmeasure:.4f}\" if rouge1_fmeasure is not None else \"N/A\",  # Handle None case for rouge1_fmeasure\n",
        "\n",
        "                f\"{rouge2_precision:.4f}\" if rouge2_precision is not None else \"N/A\",  # Handle None case for rouge2_precision\n",
        "                f\"{rouge2_recall:.4f}\" if rouge2_recall is not None else \"N/A\",  # Handle None case for rouge2_recall\n",
        "                f\"{rouge2_fmeasure:.4f}\" if rouge2_fmeasure is not None else \"N/A\",  # Handle None case for rouge2_fmeasure\n",
        "\n",
        "                f\"{rougeL_precision:.4f}\" if rougeL_precision is not None else \"N/A\",  # Handle None case for rougeL_precision\n",
        "                f\"{rougeL_recall:.4f}\" if rougeL_recall is not None else \"N/A\",  # Handle None case for rougeL_recall\n",
        "                f\"{rougeL_fmeasure:.4f}\" if rougeL_fmeasure is not None else \"N/A\",  # Handle None case for rougeL_fmeasure\n",
        "\n",
        "                f\"{perplexity:.4f}\" if perplexity is not None else \"N/A\",  # Handle None case for perplexity\n",
        "\n",
        "                f\"{learning_rate:.4f}\" if learning_rate is not None else \"N/A\",  # Handle None case for learning_rate\n",
        "                f\"{batch_size}\" if batch_size is not None else \"N/A\",  # Handle None case for batch_size\n",
        "                f\"{epochs}\" if epochs is not None else \"N/A\",  # Handle None case for epochs\n",
        "\n",
        "                #f\"{state.global_step}\" if state is not None else \"N/A\",  # Handle None case for global_step\n",
        "                #f\"{state.epoch}\" if state is not None else \"N/A\",  # Handle None case for epoch\n",
        "                #f\"{learning_rate:.4f}\",  # Learning rate\n",
        "                #batch_size,  # Batch size\n",
        "                #epochs,  # Epochs\n",
        "                #state.global_step if state else \"n/a\",  # Global steps\n",
        "                #state.epoch if state else \"n/a\",  # Epoch\n",
        "                #state.is_local_process_zero if state else \"n/a\",\n",
        "                #control.should_training_stop if control else \"n/a\",\n",
        "                #control.should_log if control else \"n/a\",\n",
        "               # control.should_save if control else \"n/a\",\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # Generate the report table\n",
        "    headers = [\n",
        "        \"Dataset\",\n",
        "        \"Model\",\n",
        "        \"Elapsed Time\",\n",
        "        \"Train Loss (Average)\",\n",
        "        \"Eval Loss (Average)\",\n",
        "        \"Train Loss (Std)\",\n",
        "        \"Eval Loss (Std)\",\n",
        "        \"Min Train Loss\",\n",
        "        \"Max Train Loss\",\n",
        "        \"Min Eval Loss\",\n",
        "        \"Max Eval Loss\",\n",
        "        \"BLEU Score\",\n",
        "        \"F1 Score\",\n",
        "        \"ROUGE-1 Precision\",\n",
        "        \"ROUGE-1 Recall\",\n",
        "        \"ROUGE-1 F-measure\",\n",
        "        \"ROUGE-2 Precision\",\n",
        "        \"ROUGE-2 Recall\",\n",
        "        \"ROUGE-2 F-measure\",\n",
        "        \"ROUGE-L Precision\",\n",
        "        \"ROUGE-L Recall\",\n",
        "        \"ROUGE-L F-measure\",\n",
        "        \"Perplexity\",\n",
        "        \"Learning Rate\",\n",
        "        \"Batch Size\",\n",
        "        \"Epochs\",\n",
        "        \"Global Steps\",\n",
        "        \"Epoch\",\n",
        "        \"is_local_process_zero\",\n",
        "        \"should_training_stop\",\n",
        "        \"should_log\",\n",
        "        \"should_save\",\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "    report_table = tabulate(report_data, headers=headers, tablefmt=\"grid\")\n",
        "\n",
        "    # Save the report to a file\n",
        "    with open(output_file, \"w\") as f:\n",
        "        f.write(report_table)\n",
        "\n",
        "    print(report_table)\n",
        "\n",
        "    # LLM Analysis using Google Gemini\n",
        "    model_name = \"gemini-1.5-pro\"  # Replace with desired model\n",
        "    model = genai.GenerativeModel(model_name)\n",
        "    response = model.generate_content(prompt + \"\\n\\n\" + report_table)\n",
        "    llm_analysis = response.text\n",
        "\n",
        "    print(\"\\n\\n## LLM Analysis:\\n\")\n",
        "    print(llm_analysis)\n",
        "\n",
        "    #return llm_analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "-smf04UA0mnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the LLM report and send to Gemini\n",
        "prompt = \"\"\"\n",
        "You are a helpful data science expert.\n",
        "Please, make an additional analysis of this Fine-Tuning experiment report.\n",
        "\"\"\"\n",
        "# Initialize training_args_list, state_list, control_list with empty lists\n",
        "training_args_list = [None] * len(rl_pairs)\n",
        "state_list = [None] * len(rl_pairs)\n",
        "control_list = [None] * len(rl_pairs)\n",
        "\n",
        "generate_llm_report(rl_pairs, agents, training_args_list, state_list, control_list, prompt=prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "zmcvPmYQgwvz",
        "outputId": "321b7cd5-5927-4663-a8d5-5e7ed47ecc12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------------------------------+-----------------+------------------------+-----------------------+--------------------+-------------------+------------------+------------------+-----------------+-----------------+--------------+------------+---------------------+------------------+---------------------+---------------------+------------------+---------------------+---------------------+------------------+---------------------+--------------+-----------------+--------------+----------+\n",
            "| Dataset           | Model                                     | Elapsed Time    |   Train Loss (Average) |   Eval Loss (Average) |   Train Loss (Std) |   Eval Loss (Std) |   Min Train Loss |   Max Train Loss |   Min Eval Loss |   Max Eval Loss |   BLEU Score |   F1 Score |   ROUGE-1 Precision |   ROUGE-1 Recall |   ROUGE-1 F-measure |   ROUGE-2 Precision |   ROUGE-2 Recall |   ROUGE-2 F-measure |   ROUGE-L Precision |   ROUGE-L Recall |   ROUGE-L F-measure |   Perplexity |   Learning Rate |   Batch Size |   Epochs |\n",
            "+===================+===========================================+=================+========================+=======================+====================+===================+==================+==================+=================+=================+==============+============+=====================+==================+=====================+=====================+==================+=====================+=====================+==================+=====================+==============+=================+==============+==========+\n",
            "| anthropic/hh-rlhf | unsloth/mistral-7b-instruct-v0.3-bnb-4bit | 4509.06 seconds |                 0.2983 |                2.6071 |             0.5001 |            0.2803 |           0.1466 |           2.6414 |          1.987  |          2.8977 |            0 |          0 |              0.5287 |           0.5214 |              0.5244 |              0.2148 |           0.2115 |              0.2128 |              0.4877 |           0.4802 |              0.4834 |      18.1708 |          0.0002 |            4 |        3 |\n",
            "+-------------------+-------------------------------------------+-----------------+------------------------+-----------------------+--------------------+-------------------+------------------+------------------+-----------------+-----------------+--------------+------------+---------------------+------------------+---------------------+---------------------+------------------+---------------------+---------------------+------------------+---------------------+--------------+-----------------+--------------+----------+\n",
            "| anthropic/hh-rlhf | deepseek-ai/deepseek-coder-1.3b-base      | 1831.65 seconds |                 0.3483 |                3.5221 |             0.4881 |            0.4199 |           0.1598 |           2.3796 |          2.2049 |          3.8439 |            0 |          0 |              0.4457 |           0.4509 |              0.4478 |              0.1154 |           0.1181 |              0.1166 |              0.3766 |           0.3804 |              0.3781 |      58.0693 |          0.0002 |            4 |        3 |\n",
            "+-------------------+-------------------------------------------+-----------------+------------------------+-----------------------+--------------------+-------------------+------------------+------------------+-----------------+-----------------+--------------+------------+---------------------+------------------+---------------------+---------------------+------------------+---------------------+---------------------+------------------+---------------------+--------------+-----------------+--------------+----------+\n",
            "\n",
            "\n",
            "## LLM Analysis:\n",
            "\n",
            "This report summarizes a fine-tuning experiment comparing two language models, `unsloth/mistral-7b-instruct-v0.3-bnb-4bit` and `deepseek-ai/deepseek-coder-1.3b-base`, on the `anthropic/hh-rlhf` dataset.  Here's a breakdown of the key observations and potential areas for further investigation:\n",
            "\n",
            "**Key Observations:**\n",
            "\n",
            "* **Mistral performs better overall:**  Mistral achieves lower average evaluation loss (2.61 vs. 3.52), indicating better generalization to unseen data.  It also exhibits superior ROUGE scores across the board (ROUGE-1, ROUGE-2, and ROUGE-L), suggesting it generates more relevant and coherent text.  The lower perplexity (18.17 vs. 58.07) further reinforces Mistral's stronger performance.\n",
            "* **High training loss variability:** Both models exhibit substantial standard deviations in their training loss, suggesting potential instability during training. This could be due to the learning rate, batch size, or the nature of the dataset itself.\n",
            "* **Zero BLEU and F1:**  Both models have a BLEU and F1 score of 0.  This is unusual and warrants further investigation.  BLEU, in particular, is sensitive to exact matches and might be overly strict for generative tasks.  It's possible the generated text is semantically similar to the target but differs in wording, leading to a low BLEU score.  Similarly, the F1 score depends on how precision and recall are calculated in the evaluation script. It's possible that the evaluation script needs to be reviewed for more nuanced metrics like ROUGE.\n",
            "* **Deepseek's significantly higher perplexity:** The much higher perplexity of Deepseek suggests it's less confident in its predictions and might be overfitting to the training data.\n",
            "* **Comparable training parameters:**  Both models were trained with the same learning rate, batch size, and number of epochs, allowing for a fair comparison.\n",
            "* **Mistral's longer training time:** Despite better performance, Mistral took considerably longer to train (4509 seconds vs. 1831 seconds).  This difference likely stems from Mistral being a larger model (7B parameters) compared to Deepseek (1.3B parameters).\n",
            "\n",
            "**Further Analysis and Recommendations:**\n",
            "\n",
            "1. **Investigate Zero BLEU/F1:** The zero BLEU and F1 scores need to be understood.  Examine the evaluation script to ensure it's appropriate for the task and consider using additional metrics like METEOR or BERTScore, which are less sensitive to exact matches and better capture semantic similarity. Manually inspecting the generated output can help understand the discrepancy between other metrics and zero BLEU/F1.\n",
            "2. **Address Training Instability:** The high standard deviations in training loss suggest potential issues with the training process.  Experiment with different learning rates, batch sizes, and optimization strategies (e.g., AdamW with weight decay) to stabilize training and potentially improve performance.\n",
            "3. **Hyperparameter Tuning:** Explore a wider range of hyperparameters for both models to find the optimal configuration. This could involve a more systematic approach like grid search or Bayesian optimization.\n",
            "4. **Dataset Analysis:** Analyze the `anthropic/hh-rlhf` dataset to understand its characteristics and potential challenges. This could involve looking at the distribution of sequence lengths, vocabulary size, and the presence of any biases.\n",
            "5. **Consider Larger Batch Sizes for Deepseek:**  Given Deepseek's faster training time, experiment with larger batch sizes to potentially improve training efficiency and performance.\n",
            "6. **Resource Considerations:**  If computational resources are limited, prioritize further experiments with Mistral given its superior performance, even though it has longer training times.\n",
            "7. **Qualitative Analysis:** Examine the generated text from both models to gain a qualitative understanding of their strengths and weaknesses. This can help identify areas for improvement and provide insights beyond the quantitative metrics.\n",
            "\n",
            "\n",
            "By addressing these points, you can gain a deeper understanding of the models' behavior and potentially improve their performance on the given task.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}