{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/Copy_of_FineTuning_T2SQL_GNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etm0HfcZM151"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 , L4  IN GOOGLE COLAB\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install datasets trl ninja packaging --quiet\n",
        "\n",
        "# Uncomment only if you're using A100 or L4 GPU\n",
        "#!pip install flash-attn --no-build-isolation\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install mistral_inference -q\n",
        "\n",
        "!pip install trl==0.8.6 -q\n",
        "\n",
        "\n",
        "!pip install torch-geometric -q\n",
        "!pip install sqlparse networkx -q\n",
        "\n",
        "!pip install bitsandbytes -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65JpttRiGxVK",
        "outputId": "b9cc84ec-30fd-41dd-abee-a638df462bd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "#print(access_token_write)\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "S9qvxG2HYusD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "     pipeline,\n",
        ")\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "naQrZIQTY1wG"
      },
      "outputs": [],
      "source": [
        "# set device\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KtE9TuSMlUEc",
        "outputId": "f02c2330-5d25-4526-a5d1-963b66bab33e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GapRov3hl4fT",
        "outputId": "66444616-0aa2-4151-e91a-e8708710b640"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Mon Jul 15 10:37:34 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA L4                      Off | 00000000:00:03.0 Off |                    0 |\n",
            "| N/A   41C    P8              16W /  72W |      4MiB / 23034MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!python --version\n",
        "!nvcc --version\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDRUVm6oalJQ"
      },
      "source": [
        "MISTRAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gZ0pkdVaIF8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from trl import setup_chat_format\n",
        "\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")\n",
        "\n",
        "print()\n",
        "\n",
        "# Hugging Face model id\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\" #24 JUNE 2024\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer\n",
        "mistral_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)\n",
        "tokenizer.padding_side = 'right' # to prevent warnings\n",
        "\n",
        "# We redefine the pad_token and pad_token_id with out of vocabulary token (unk_token)\n",
        "tokenizer.pad_token = tokenizer.unk_token\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "\n",
        "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
        "mistral_model, tokenizer = setup_chat_format(mistral_model, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuD2ZqXNSobM"
      },
      "source": [
        "GNN #0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load and Prepare Data\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
        "dataset = dataset.shuffle(seed=42).select(range(125))"
      ],
      "metadata": {
        "id": "HC58ANA7BXzm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "7b260d70674c4d8e9d7ef83b16da36fb",
            "fc56bef1db544ad286cc5b0519ed9842",
            "81fc606e67884bf098d524e60c25c699",
            "f80be09b632a41ec85a96ec7f4034c91",
            "439d8335790d4d15816222ac450bb1e2",
            "7ace35da014645be80ca0ad86037ba40",
            "100285823fdf41b387dfe7c31a74cc30",
            "960b83d5b71d4ff4941a346e6f2c7cbe",
            "ace2e9b73c424dcfb7efafb5c59c04ab",
            "f2bb7db13f7e415889071584680e20c0",
            "ece39be10050463999ae6218b70fb2d3",
            "25172d0e67794cbfbb732b0bb8c33ed4",
            "efb01fa3315c4e6aaf98946b318e31f8",
            "bcdd42d596d8477c876111dcf41d2f34",
            "8e7a2ddf806e4818b7d330b5a5c08030",
            "f7fd2e631f27455ba49839326d0a604b",
            "d402df6d317645af97918690b795199c",
            "4df44b38994b426aa596992bc59544de",
            "17d1a0ffdd954add933ac980d1a058de",
            "d5d03ebc380048abb5e7778b5e9ea3bc",
            "02d8e5a17c46491ab99ec115e43565f2",
            "b74b625d236b4c9cb7ba2579b70e9464",
            "296a6ed6b70944b18fb1ea3eddeecfd7",
            "6264a0ef828b478d89bb388b0f2445ed",
            "4f9f07e27e1f4db5bf682f42b63482c4",
            "f9d1d0186c814bac912b6e03996a5d90",
            "5763fe17d41f4e72a776dcaf3ec85302",
            "1a43c2001aea4eefae9f3f33cdc72307",
            "4d3f85eba5da486084eb6fbdfa117a7e",
            "2beda49790f0442ab68ace94b33dba20",
            "9003c162c7b94e2cb0d60eb7a0e4392f",
            "80a45a1e1f214f55bbb4e91d0937caef",
            "437cff1a37da44668fa6f99808dbb69a"
          ]
        },
        "outputId": "d10a971b-f56c-4bef-ea75-1e3efbf04688"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/4.43k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b260d70674c4d8e9d7ef83b16da36fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/21.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25172d0e67794cbfbb732b0bb8c33ed4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/78577 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "296a6ed6b70944b18fb1ea3eddeecfd7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrCR2vYWBaZL",
        "outputId": "ec6d6047-1c34-4a22-d2fb-04d558ab33b1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['answer', 'question', 'context'],\n",
              "    num_rows: 12500\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XRo0s6QE-4va",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f303a5-7909-469c-a0bf-aee79a8fad56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "\n",
            "\n",
            "\n",
            "Print a sample of the dataset to check if the schema and tables are in the correct format\n",
            "{'answer': ['SELECT venue FROM table_name_50 WHERE away_team = \"essendon\"', 'SELECT MIN(game) FROM table_name_61 WHERE opponent = \"phoenix\" AND record = \"29-17\"'], 'question': ['When Essendon played away; where did they play?', 'What is the lowest numbered game against Phoenix with a record of 29-17?'], 'context': ['CREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR)', 'CREATE TABLE table_name_61 (game INTEGER, opponent VARCHAR, record VARCHAR)']}\n",
            "\n",
            "\n",
            "\n",
            "train size: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Converting to GNN (99%): 100%|██████████| 12500/12500 [00:19<00:00, 642.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total valid queries processed: 0\n",
            "Total invalid queries: 12500\n",
            "\n",
            "First 10 failed queries and their schemas:\n",
            "- Query: When Essendon played away; where did they play?\n",
            "  Schema: CREATE TABLE table_name_50 (venue VARCHAR, away_team VARCHAR)\n",
            "\n",
            "- Query: What is the lowest numbered game against Phoenix with a record of 29-17?\n",
            "  Schema: CREATE TABLE table_name_61 (game INTEGER, opponent VARCHAR, record VARCHAR)\n",
            "\n",
            "- Query: Who did the Texan's play on Week 4?\n",
            "  Schema: CREATE TABLE table_name_37 (opponent VARCHAR, week VARCHAR)\n",
            "\n",
            "- Query: Which Points have Touchdowns larger than 0, and an Extra points smaller than 0?\n",
            "  Schema: CREATE TABLE table_name_70 (points INTEGER, touchdowns VARCHAR, extra_points VARCHAR)\n",
            "\n",
            "- Query: What is the name of the player who is Sco and moving to greenock morton in the summer?\n",
            "  Schema: CREATE TABLE table_name_83 (name VARCHAR, moving_to VARCHAR, nat VARCHAR, transfer_window VARCHAR)\n",
            "\n",
            "- Query: Of all the contestants who got voted, what is the contestant number and name of the contestant who got least votes?\n",
            "  Schema: CREATE TABLE votes (contestant_number VARCHAR); CREATE TABLE contestants (contestant_number VARCHAR, contestant_name VARCHAR)\n",
            "\n",
            "- Query: Which venue had the result 7-1?\n",
            "  Schema: CREATE TABLE table_name_50 (venue VARCHAR, result VARCHAR)\n",
            "\n",
            "- Query: What did the tournament that got an A in 1945 get in 1949?\n",
            "  Schema: CREATE TABLE table_name_22 (Id VARCHAR)\n",
            "\n",
            "- Query: Find the states where have the colleges whose enrollments are less than the largest size.\n",
            "  Schema: CREATE TABLE college (state VARCHAR, enr INTEGER)\n",
            "\n",
            "- Query: What is the name of the episode that had 9.89 million U.S. viewers?\n",
            "  Schema: CREATE TABLE table_24648983_1 (title VARCHAR, us_viewers__million_ VARCHAR)\n",
            "\n",
            "Number of successful graphs: 0\n",
            "Dataset examples are all strings: True\n",
            "Total number of examples: 12500\n",
            "Length of train_dataset: 1\n",
            "Length of val_dataset: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-5f0913dbebbe>:391: UserWarning: No valid graphs were generated. Check the 'sql_to_graph' function and the dataset for errors.\n",
            "  warnings.warn(\"No valid graphs were generated. Check the 'sql_to_graph' function and the dataset for errors.\", UserWarning)\n"
          ]
        }
      ],
      "source": [
        "import colab_env\n",
        "import os\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "from huggingface_hub import login\n",
        "#print(access_token_write)\n",
        "login(\n",
        " token=access_token_write,\n",
        " add_to_git_credential=True\n",
        ")\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import (\n",
        " AutoModelForCausalLM,\n",
        " AutoTokenizer,\n",
        " BitsAndBytesConfig,\n",
        " AutoTokenizer,\n",
        " TrainingArguments,\n",
        " pipeline,\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# set device\n",
        "device = 'cuda'\n",
        "\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
        "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
        "mistral_model, tokenizer = setup_chat_format(mistral_model, tokenizer)\n",
        "\n",
        "# GNN\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from torch_geometric.data import Data, Batch # Import Batch here\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool # Import global_mean_pool here\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# 1. Graph Construction\n",
        "import torch\n",
        "import sqlparse\n",
        "import networkx as nx\n",
        "from torch_geometric.data import Data\n",
        "from tqdm import tqdm\n",
        "\n",
        "def sql_to_graph(question, schema, answer):\n",
        "    try:\n",
        "        # 1. Parse SQL Query (with robust error handling)\n",
        "        try:\n",
        "            statements = sqlparse.parse(question)\n",
        "            print(f\"Parsed statements: {statements}\")\n",
        "        except sqlparse.exceptions.SQLParseError as e:\n",
        "            # Attempt to fix common parsing issues\n",
        "            fixed_question = question.replace(';', '')\n",
        "            try:\n",
        "                statements = sqlparse.parse(fixed_question)\n",
        "                print(f\"Warning: SQL parsing initially failed for '{question}', but succeeded after removing semicolon(s).\")\n",
        "            except sqlparse.exceptions.SQLParseError as fixed_e:\n",
        "                print(f\"Warning: SQL parsing failed for '{question}' (even after fix attempt): {fixed_e}. Skipping example.\")\n",
        "                return None\n",
        "        except ValueError as e:  # Catch potential errors in the context dict\n",
        "            print(f\"Warning: Context dict Value Error: '{question}': {e}. Skipping example.\")\n",
        "            return None\n",
        "        except Exception as e:  # Catch any other unexpected parsing errors\n",
        "            print(f\"Unexpected error while parsing '{question}': {e}. Skipping example.\")\n",
        "            return None\n",
        "\n",
        "        # 2. Create Graph\n",
        "        G = nx.DiGraph()\n",
        "        current_table = None\n",
        "        for stmt in statements:\n",
        "            if stmt.get_type() == \"SELECT\":\n",
        "                select_items = [token for token in stmt if isinstance(token, sqlparse.sql.IdentifierList)]\n",
        "                for item in select_items:\n",
        "                    for token in item.get_identifiers():\n",
        "                        G.add_node(token.value)  # Add column name as node (for SELECT)\n",
        "\n",
        "            for token in stmt.flatten():\n",
        "                if token.ttype is sqlparse.tokens.Keyword:\n",
        "                    G.add_node(token.value.upper())\n",
        "                    if token.value.upper() in [\"FROM\", \"JOIN\"]:\n",
        "                        # Handle multi-table queries or subqueries\n",
        "                        try:\n",
        "                            current_table = next(token.flatten()).value\n",
        "                        except StopIteration:\n",
        "                            print(\n",
        "                                f\"Warning: No table found after 'FROM' or 'JOIN' in '{question}'. Skipping example.\"\n",
        "                            )\n",
        "                            return None\n",
        "                elif token.ttype in (sqlparse.tokens.Name, sqlparse.tokens.Wildcard):\n",
        "                    G.add_node(token.value)\n",
        "                    if current_table:\n",
        "                        G.add_edge(current_table, token.value)\n",
        "\n",
        "        # Check if graph is empty\n",
        "        if G.number_of_nodes() == 0 or G.number_of_edges() == 0:\n",
        "            print(f\"Warning: Empty graph generated for: {question}\")\n",
        "            return None\n",
        "\n",
        "        # 3. Extract Node Features\n",
        "        node_features = []\n",
        "        for node in G.nodes():\n",
        "            feature = [\n",
        "                1 if node in [\"SELECT\", \"FROM\", \"WHERE\", \"GROUP BY\", \"ORDER BY\", \"HAVING\", \"LIMIT\", \"JOIN\"] else 0,  # Keyword feature\n",
        "                1 if node in schema.get(\"table_names_original\", []) else 0,  # Table feature\n",
        "                1 if node in schema.get(\"column_names_original\", []) else 0,  # Column feature\n",
        "                1 if node.upper() in [\"COUNT\", \"SUM\", \"AVG\", \"MAX\", \"MIN\"] else 0,  # Aggregate function feature\n",
        "            ]\n",
        "            node_features.append(feature)\n",
        "\n",
        "        # 4. Extract Edge Features (simple for now)\n",
        "        edge_features = [[1]] * len(G.edges())\n",
        "\n",
        "        # 5. Convert to PyTorch Geometric Data\n",
        "        try:\n",
        "            x = torch.tensor(node_features, dtype=torch.float)\n",
        "            edge_index = torch.tensor(list(G.edges())).t().contiguous()\n",
        "\n",
        "            # Tokenize answer, handling potential errors (KeyError for empty answers)\n",
        "            try:\n",
        "                answer_tokens = tokenizer(answer, return_tensors=\"pt\")[\"input_ids\"]\n",
        "            except KeyError as e:\n",
        "                print(f\"Warning: Empty or untokenizable answer for '{question}': {e}. Skipping example.\")\n",
        "                return None\n",
        "            y = answer_tokens\n",
        "\n",
        "            # Add prints for intermediate values in `sql_to_graph`\n",
        "            print(f\"Graph Nodes: {G.nodes()}\")\n",
        "            print(f\"Graph Edges: {G.edges()}\")\n",
        "            print(f\"Node Features:\\n{x}\")\n",
        "            print(f\"Edge Index:\\n{edge_index}\")\n",
        "            print(f\"Answer Tokens: {y}\")\n",
        "\n",
        "            if x.size(0) == 0 or edge_index.size(0) == 0 or y.size(0) == 0:\n",
        "                raise ValueError(\"Empty graph, features, or answer tokens.\")\n",
        "\n",
        "            return Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting to PyG Data for '{question}': {e}\")\n",
        "            return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error in sql_to_graph for '{question}': {e}. Skipping example.\")\n",
        "        return None  # Return None on any unexpected error\n",
        "\n",
        "\n",
        "# 1. Placeholder Conversion Function\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from tqdm import tqdm\n",
        "import sqlparse\n",
        "\n",
        "def is_valid_schema(context):\n",
        "    # Since the dataset already has the schema in the correct format, no need for conversion\n",
        "    return \"table_names_original\" in context and \"column_names_original\" in context\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from tqdm import tqdm\n",
        "\n",
        "def is_valid_schema(context):\n",
        "    return \"table_names_original\" in context and \"column_names_original\" in context\n",
        "\n",
        "def convert_to_gnn(dataset):\n",
        "    dataset_gnn = []\n",
        "    failed_queries = []\n",
        "    valid_query_count = 0\n",
        "\n",
        "    with tqdm(total=len(dataset), desc=\"Converting to GNN\") as loop:\n",
        "        for i, example in enumerate(dataset):\n",
        "            loop.set_description(f\"Converting to GNN ({int(100 * i / len(dataset))}%)\")\n",
        "\n",
        "            question = example[\"question\"]\n",
        "            context = example[\"context\"]\n",
        "            answer = example[\"answer\"]\n",
        "\n",
        "\n",
        "            #print(f\"\\nProcessing example {i}: {question}\")\n",
        "            #print(f\"With context: {context}\")\n",
        "\n",
        "            # Check if the context contains valid schema information\n",
        "            if not is_valid_schema(context):\n",
        "                #print(f\"Skipping example {i} (Invalid schema): '{question}'\")\n",
        "                failed_queries.append(question)\n",
        "                loop.update(1)\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                table_names = context.get(\"table_names_original\", [])\n",
        "                column_names = context.get(\"column_names_original\", [])\n",
        "            except AttributeError as e:  # Catch if context is not a dictionary\n",
        "                print(f\"Warning: Context not a dictionary in example {i}: '{question}'. Skipping example.\")\n",
        "                failed_queries.append(question)\n",
        "                loop.update(1)\n",
        "                continue\n",
        "            except Exception as e: # Check for invalid data\n",
        "                print(f\"Warning: Unexpected error in schema: '{question}': {e}. Skipping example.\")\n",
        "                failed_queries.append(question)\n",
        "                loop.update(1)\n",
        "                continue\n",
        "\n",
        "            if not table_names or not column_names:  # Check for missing keys\n",
        "                print(f\"Skipping example {i} (Missing table or column names): '{question}'\")\n",
        "                failed_queries.append(question)\n",
        "                loop.update(1)\n",
        "                continue\n",
        "\n",
        "            if not all(isinstance(name, str) for name in table_names) or not all(isinstance(name, str) for name in column_names):\n",
        "                # Check if all elements in lists are strings\n",
        "                print(f\"Skipping example {i} (Invalid table/column name type): '{question}'\")\n",
        "                failed_queries.append(question)\n",
        "                loop.update(1)\n",
        "                continue\n",
        "\n",
        "\n",
        "            try:\n",
        "                graph = sql_to_graph(question, context, answer)\n",
        "                if graph is None:\n",
        "                    print(f\"Warning: sql_to_graph returned None for example {i}: '{question}'. Skipping example.\")\n",
        "                    print(f\"Context for failed query: {context}\")   # Print the context here\n",
        "\n",
        "                    failed_queries.append(question)\n",
        "                elif graph.num_nodes == 0 or graph.num_edges == 0:\n",
        "                    print(f\"Warning: Empty graph generated for example {i}: '{question}'. Schema: {context}\")\n",
        "                    failed_queries.append(question)\n",
        "                else:\n",
        "                    print(f\"Successfully generated graph for example {i}\")\n",
        "                    dataset_gnn.append(graph)\n",
        "                    valid_query_count += 1\n",
        "            except Exception as e:\n",
        "                #print(f\"Error processing example {i}: '{question}': {e}\")\n",
        "                #print(f\"Schema for the failed query: {context}\")\n",
        "                failed_queries.append(question)\n",
        "\n",
        "            loop.update(1)  # Update progress bar after each example\n",
        "\n",
        "    # Final summary and check\n",
        "    print(f\"\\nTotal valid queries processed: {valid_query_count}\")\n",
        "    print(f\"Total invalid queries: {len(failed_queries)}\")\n",
        "\n",
        "\n",
        "    if failed_queries:\n",
        "        print(\"\\nFirst 10 failed queries and their schemas:\")\n",
        "        for query, ctx in zip(failed_queries[:10], [dataset[i][\"context\"] for i in range(len(failed_queries))[:10]]):\n",
        "            print(f\"- Query: {query}\")\n",
        "            print(f\"  Schema: {ctx}\\n\")\n",
        "\n",
        "    return dataset_gnn\n",
        "\n",
        "\n",
        "\n",
        "class SQLGraphDataset(Dataset):\n",
        " def __init__(self, data):\n",
        "  self.data = data\n",
        " def __len__(self):\n",
        "  return len(self.data)\n",
        " def __getitem__(self, index):\n",
        "  entry = self.data[index]\n",
        "  question = entry[\"question\"]\n",
        "  schema = entry[\"context\"]\n",
        "  answer = entry[\"answer\"]\n",
        "  return sql_to_graph(question, schema, answer)\n",
        "\n",
        "\n",
        "# 2. GNN Model\n",
        "from torch_geometric.nn import GATConv\n",
        "class SQLGNN(torch.nn.Module): # Inherit from torch.nn.Module\n",
        " def __init__(self, input_dim, hidden_dim, output_dim, heads=8):\n",
        "  super(SQLGNN, self).__init__() # Call superclass constructor\n",
        "  self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=0.6)\n",
        "  self.conv2 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=0.6)\n",
        "\n",
        " def forward(self, data):\n",
        "  x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)  # Move graph data to GPU\n",
        "  x = F.elu(self.conv1(x, edge_index))\n",
        "  x = F.dropout(x, p=0.6, training=self.training)\n",
        "  x = self.conv2(x, edge_index)\n",
        "  x = global_mean_pool(x, batch)  # Global Mean Pooling for graph-level representation\n",
        "  return x\n",
        "\n",
        " # Add this method to explicitly define saving state dictionary\n",
        " def save_state_dict(self, filepath):\n",
        "    torch.save(super().state_dict(), filepath)\n",
        "\n",
        "\n",
        "# Define a custom collate function to handle batching of graphs and text data\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Filter out 'None' values from the batch\n",
        "    batch = [data for data in batch if data is not None]\n",
        "\n",
        "    if not batch:  # If the batch is empty after filtering\n",
        "        print(\"Warning: Empty batch encountered. Skipping this batch.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    graphs, questions, schemas, answers = [], [], [], []\n",
        "\n",
        "    for i, data in enumerate(batch):\n",
        "        if data.edge_index.numel() > 0:  # Check if the graph has edges\n",
        "            graphs.append(data)\n",
        "            questions.append(dataset[i][\"question\"])\n",
        "            schemas.append(dataset[i][\"context\"])\n",
        "\n",
        "            # Handle different answer formats\n",
        "            if isinstance(data.y, torch.Tensor):\n",
        "                answers.append(data.y.tolist())\n",
        "            else:\n",
        "                answers.append(data.y)\n",
        "\n",
        "    if not graphs:  # Handle case where all graphs in the batch are empty\n",
        "        print(\"Warning: All graphs in the batch are empty. Skipping this batch.\")\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Pad edge_index to the maximum length in the batch\n",
        "    max_edges = max([data.num_edges for data in graphs])\n",
        "    for data in graphs:\n",
        "        padding = max_edges - data.num_edges\n",
        "        if padding > 0:\n",
        "            pad = torch.zeros(2, padding, dtype=torch.long)\n",
        "            data.edge_index = torch.cat([data.edge_index, pad], dim=1)\n",
        "\n",
        "    # Pad node features (x) to the maximum number of nodes\n",
        "    max_nodes = max([data.num_nodes for data in graphs])\n",
        "    for data in graphs:\n",
        "        padding = max_nodes - data.num_nodes\n",
        "        if padding > 0:\n",
        "            pad = torch.zeros(padding, data.x.size(1), dtype=torch.float)  # Pad with zeros of the same feature dimension\n",
        "            data.x = torch.cat([data.x, pad], dim=0)\n",
        "\n",
        "    return Batch.from_data_list(graphs), questions, schemas, answers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 3. Load and Prepare Data\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
        "dataset = dataset.shuffle(seed=42).select(range(12500))\n",
        "\n",
        "# Print a sample of the dataset to check if the schema and tables are in the correct format\n",
        "print('\\n\\n')\n",
        "print('Print a sample of the dataset to check if the schema and tables are in the correct format')\n",
        "print(dataset[:2])\n",
        "print('\\n\\n')\n",
        "\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(dataset))\n",
        "print(f'train size: {train_size}')\n",
        "\n",
        "train_dataset = dataset[:train_size]\n",
        "val_dataset = dataset[train_size:]\n",
        "\n",
        "#print(len(train_dataset))\n",
        "#print(len(val_dataset))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "# Convert to GNN format\n",
        "dataset_gnn = convert_to_gnn(dataset)\n",
        "\n",
        "print(f\"Number of successful graphs: {len(dataset_gnn)}\")\n",
        "print(f\"Dataset examples are all strings: {all(isinstance(example['question'], str) for example in dataset)}\")\n",
        "\n",
        "# Analyze the Results\n",
        "print(f\"Total number of examples: {len(dataset)}\")\n",
        "#print(f\"Number of failed queries: {len(failed_queries)}\")\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "# Check if dataset_gnn has at least one valid graph and the split will work\n",
        "if not dataset_gnn:\n",
        "    warnings.warn(\"No valid graphs were generated. Check the 'sql_to_graph' function and the dataset for errors.\", UserWarning)\n",
        "else:\n",
        "    #4. Initialize Model, Loss, and Optimizer\n",
        "    print(f'Number of successful graphs; {len(dataset_gnn)}')\n",
        "    input_dim = dataset_gnn[0].num_node_features\n",
        "    hidden_dim = 64\n",
        "    output_dim = 128 # Assuming a generation task for simplicity\n",
        "    model = SQLGNN(input_dim, hidden_dim, output_dim).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(f\"Length of train_dataset: {len(train_loader)}\")\n",
        "print(f\"Length of val_dataset: {len(val_loader)}\")\n",
        "\n",
        "# For generation:\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=0) # Ignore padding in loss calculation\n",
        "# For classification:\n",
        "# criterion = torch.nn.BCEWithLogitsLoss() # Or other suitable loss\n",
        "\n",
        "# 5. Training and Evaluation Functions\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "tokenizer.padding_side = 'left'\n",
        "import torch.nn.functional as F  # Import F for the loss function\n",
        "\n",
        "\n",
        "def train(model, mistral_model, loader, optimizer, mistral_optimizer, epoch, num_epochs):\n",
        "    model.train()\n",
        "    mistral_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    #loop = tqdm(loader, total=len(loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    #for data, questions, schemas, answers in loop:\n",
        "    total_loss = 0\n",
        "    loop = tqdm(loader, total=len(loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    for batch_data in loop:\n",
        "        if batch_data is None:  # Check if batch is None\n",
        "            continue  # Skip the batch if it's None\n",
        "\n",
        "        data, questions, schemas, answers = batch_data  # Unpack the batch\n",
        "\n",
        "        # Initialize graph_embeddings with an empty list\n",
        "        graph_embeddings = []\n",
        "\n",
        "        # Check if the graph data is None\n",
        "        if data is not None:\n",
        "            data = data.to(device)\n",
        "            graph_embeddings = model(data)  # Get embeddings from GNN\n",
        "\n",
        "        # Filter out None values from graph_embeddings\n",
        "        valid_graph_embeddings = [g for g in graph_embeddings if g is not None]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        mistral_optimizer.zero_grad()\n",
        "\n",
        "        # Initialize mistral_inputs with an empty list\n",
        "        mistral_inputs = []\n",
        "\n",
        "        # Prepare Mistral Input\n",
        "        if valid_graph_embeddings:\n",
        "          mistral_inputs = [f\"Question: {q}\\nSchema: {s}\\nGraph Embedding: {g}\" for q, s, g in zip(questions, schemas, valid_graph_embeddings)]\n",
        "\n",
        "          # Check if mistral_inputs is empty\n",
        "          if mistral_inputs:\n",
        "            # Tokenize and generate SQL using Mistral, increase max length\n",
        "             tokenized_inputs = tokenizer(mistral_inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "          # Check for empty tokenized inputs\n",
        "          for i, input_ids in enumerate(tokenized_inputs['input_ids']):\n",
        "              if input_ids.numel() == 0:\n",
        "                  print(f\"Warning: Input example {i} has no tokens after encoding.\")\n",
        "                  # Handle empty example (skip or replace)\n",
        "\n",
        "          tokenized_inputs = tokenized_inputs.to(device)\n",
        "\n",
        "          # Set padding side to left before tokenizing\n",
        "          tokenizer.padding_side = 'left'\n",
        "\n",
        "          # Get logits for loss calculation (instead of generating)\n",
        "          outputs = mistral_model(**tokenized_inputs, labels=tokenized_inputs[\"input_ids\"])\n",
        "          loss = outputs.loss\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Backpropagation\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          mistral_optimizer.step()\n",
        "\n",
        "          loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    %cd /content/\n",
        "    #Save GNN model - UNCOMMENT THESE LINES\n",
        "    model.save_state_dict(f\"gnn_model_epoch_{epoch+1}.pth\")\n",
        "    print(f\"GNN Model saved to gnn_model_epoch_{epoch+1}.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcr2HeJZe_DE",
        "outputId": "62d9fda1-93b6-42d8-d0bf-c314c62c63cb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr7OMzRCf8C_",
        "outputId": "710321a3-972e-48d1-b1cf-8e3813dc5530"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtIrKSbxZXhI",
        "outputId": "9051ffa2-9853-40d5-9af9-a1a145ce1d3d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9r0AcjBEStvk"
      },
      "source": [
        "GNN #1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question=train_loader.dataset['question'][20]\n",
        "print(f'Question: {question}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L73lfBTFDraj",
        "outputId": "123269b7-e917-479b-f14d-476de2990276"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: Who was the runner-up in the Memorial Tournament?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema=train_loader.dataset['context'][20]\n",
        "print(f'Schema: {schema}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sy0TV_EZEGLH",
        "outputId": "d21323ac-1596-4da5-8b27-e77edebcbbea"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema: CREATE TABLE table_1602858_1 (runner_s__up VARCHAR, tournament VARCHAR)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query=train_loader.dataset['answer'][20]\n",
        "print(f'Query: {query}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qquPeP5XEWtP",
        "outputId": "0ab1a519-d778-44ce-b14a-c80e4c88de26"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: SELECT runner_s__up FROM table_1602858_1 WHERE tournament = \"Memorial tournament\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # 5. Training Loop (Main Refinement)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Check if the training loader is empty\n",
        "    if len(train_loader) == 0:\n",
        "        print(\"Warning: Training loader is empty. Check your dataset and dataloader setup.\")\n",
        "        break  # Exit the training loop\n",
        "\n",
        "    train(model, mistral_model, train_loader, optimizer, mistral_optimizer, epoch, num_epochs)\n",
        "\n",
        "#Save GNN model - UNCOMMENT THESE LINES\n",
        "%cd /content/\n",
        "model.save_state_dict(f\"gnn_model_epoch_{epoch+1}.pth\")\n",
        "print(f\"GNN Model saved to gnn_model_epoch_{epoch+1}.pth\")"
      ],
      "metadata": {
        "id": "LCMskzJlHbm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql82KfJGB5IA"
      },
      "outputs": [],
      "source": [
        "mistral_optimizer = optim.Adam(mistral_model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 1  # Or your desired number of epochs\n",
        "for epoch in tqdm(range(num_epochs), desc=\"Overall Training Progress\"):\n",
        "    train(model, mistral_model, train_loader, optimizer, mistral_optimizer, epoch, num_epochs)\n",
        "    #evaluate(model, mistral_model, val_loader, bleu)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, mistral_model, loader, bleu):\n",
        "    model.eval()\n",
        "    mistral_model.eval()\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    print(f\"Number of examples in validation set: {len(loader.dataset)}\")\n",
        "\n",
        "    # Decode references only once\n",
        "    for item in loader.dataset:\n",
        "        if isinstance(item, dict):\n",
        "            ref_tokens = item[\"answer\"]\n",
        "        elif isinstance(item, torch_geometric.data.Data):\n",
        "            ref_tokens = item.y.tolist()  # Convert tensor to list\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported data type in dataset: {type(item)}\")\n",
        "\n",
        "        # Handle potential issues with missing or invalid 'answer'\n",
        "        if ref_tokens is not None:\n",
        "            for token_list in ref_tokens:\n",
        "                decoded_ref = tokenizer.decode(token_list, skip_special_tokens=True)\n",
        "                references.append([decoded_ref])\n",
        "        else:\n",
        "            print(\"Warning: Skipping example with missing or invalid reference.\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_data in tqdm(loader, desc=\"Evaluating\"):\n",
        "            if batch_data is None:\n",
        "                continue\n",
        "\n",
        "            data, questions, schemas, answers = batch_data\n",
        "\n",
        "            graph_embeddings = []\n",
        "            if data is not None:\n",
        "                data = data.to(device)\n",
        "                try:\n",
        "                    graph_embeddings = model(data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error generating graph embeddings: {e}\")\n",
        "                    continue  # Skip this batch if there's an error\n",
        "\n",
        "            # If the model doesn't generate any embeddings, use an empty tensor\n",
        "            if graph_embeddings is None or len(graph_embeddings) == 0:\n",
        "                graph_embeddings = torch.tensor([])\n",
        "\n",
        "            # Check if any graph embedding is non-zero\n",
        "            if graph_embeddings.any():\n",
        "                mistral_inputs = [f\"Question: {q}\\nSchema: {s}\\nGraph Embedding: {g}\"\n",
        "                                  for q, s, g in zip(questions, schemas, graph_embeddings)]\n",
        "\n",
        "                # Tokenize and generate SQL using Mistral\n",
        "                tokenized_inputs = tokenizer(mistral_inputs, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "                output_sequences = mistral_model.generate(**tokenized_inputs, max_new_tokens=128)\n",
        "                generated_sql = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
        "\n",
        "                predictions.extend(generated_sql)\n",
        "\n",
        "                # Print generated and reference SQL (for debugging purposes)\n",
        "                print(f\"Generated SQL: {generated_sql}\")\n",
        "                print(f\"Reference SQL: {answers}\")\n",
        "\n",
        "    # Ensure both lists have the same length\n",
        "    min_len = min(len(predictions), len(references))\n",
        "    predictions = predictions[:min_len]\n",
        "    references = references[:min_len]\n",
        "\n",
        "    # ... (rest of the evaluate function)\n",
        "\n",
        "    if predictions and references:\n",
        "        try:\n",
        "            bleu_scores = bleu.compute(predictions=predictions, references=references)  # Get the dictionary of BLEU scores\n",
        "            bleu_score = bleu_scores[\"bleu\"]  # Extract the overall BLEU score\n",
        "            print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "        except ZeroDivisionError:\n",
        "            print(\"ZeroDivisionError during BLEU calculation. Likely no matching n-grams.\")\n",
        "    else:\n",
        "        print(\"No valid predictions or references found for BLEU calculation.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BhsBnMAse-Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-aTGJcnhXFN"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "model = SQLGNN(input_dim, hidden_dim, output_dim)  # Recreate the model architecture\n",
        "model.load_state_dict(torch.load(f\"/content/gnn_model_epoch_{epoch+1}.pth\"))  # Load the saved parameters\n",
        "model.to(device)  # Move the model to the desired device (e.g., GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERUY1XBEjNxt"
      },
      "outputs": [],
      "source": [
        "# Final Evaluation (using the best model)\n",
        "evaluate(model, mistral_model, val_loader, bleu)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO1oRilfXIESzobkfy87y2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7b260d70674c4d8e9d7ef83b16da36fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc56bef1db544ad286cc5b0519ed9842",
              "IPY_MODEL_81fc606e67884bf098d524e60c25c699",
              "IPY_MODEL_f80be09b632a41ec85a96ec7f4034c91"
            ],
            "layout": "IPY_MODEL_439d8335790d4d15816222ac450bb1e2"
          }
        },
        "fc56bef1db544ad286cc5b0519ed9842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ace35da014645be80ca0ad86037ba40",
            "placeholder": "​",
            "style": "IPY_MODEL_100285823fdf41b387dfe7c31a74cc30",
            "value": "Downloading readme: 100%"
          }
        },
        "81fc606e67884bf098d524e60c25c699": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_960b83d5b71d4ff4941a346e6f2c7cbe",
            "max": 4432,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ace2e9b73c424dcfb7efafb5c59c04ab",
            "value": 4432
          }
        },
        "f80be09b632a41ec85a96ec7f4034c91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2bb7db13f7e415889071584680e20c0",
            "placeholder": "​",
            "style": "IPY_MODEL_ece39be10050463999ae6218b70fb2d3",
            "value": " 4.43k/4.43k [00:00&lt;00:00, 349kB/s]"
          }
        },
        "439d8335790d4d15816222ac450bb1e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ace35da014645be80ca0ad86037ba40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "100285823fdf41b387dfe7c31a74cc30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "960b83d5b71d4ff4941a346e6f2c7cbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ace2e9b73c424dcfb7efafb5c59c04ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f2bb7db13f7e415889071584680e20c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ece39be10050463999ae6218b70fb2d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25172d0e67794cbfbb732b0bb8c33ed4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_efb01fa3315c4e6aaf98946b318e31f8",
              "IPY_MODEL_bcdd42d596d8477c876111dcf41d2f34",
              "IPY_MODEL_8e7a2ddf806e4818b7d330b5a5c08030"
            ],
            "layout": "IPY_MODEL_f7fd2e631f27455ba49839326d0a604b"
          }
        },
        "efb01fa3315c4e6aaf98946b318e31f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d402df6d317645af97918690b795199c",
            "placeholder": "​",
            "style": "IPY_MODEL_4df44b38994b426aa596992bc59544de",
            "value": "Downloading data: 100%"
          }
        },
        "bcdd42d596d8477c876111dcf41d2f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17d1a0ffdd954add933ac980d1a058de",
            "max": 21752418,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5d03ebc380048abb5e7778b5e9ea3bc",
            "value": 21752418
          }
        },
        "8e7a2ddf806e4818b7d330b5a5c08030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02d8e5a17c46491ab99ec115e43565f2",
            "placeholder": "​",
            "style": "IPY_MODEL_b74b625d236b4c9cb7ba2579b70e9464",
            "value": " 21.8M/21.8M [00:00&lt;00:00, 14.8MB/s]"
          }
        },
        "f7fd2e631f27455ba49839326d0a604b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d402df6d317645af97918690b795199c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4df44b38994b426aa596992bc59544de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17d1a0ffdd954add933ac980d1a058de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5d03ebc380048abb5e7778b5e9ea3bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "02d8e5a17c46491ab99ec115e43565f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b74b625d236b4c9cb7ba2579b70e9464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "296a6ed6b70944b18fb1ea3eddeecfd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6264a0ef828b478d89bb388b0f2445ed",
              "IPY_MODEL_4f9f07e27e1f4db5bf682f42b63482c4",
              "IPY_MODEL_f9d1d0186c814bac912b6e03996a5d90"
            ],
            "layout": "IPY_MODEL_5763fe17d41f4e72a776dcaf3ec85302"
          }
        },
        "6264a0ef828b478d89bb388b0f2445ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a43c2001aea4eefae9f3f33cdc72307",
            "placeholder": "​",
            "style": "IPY_MODEL_4d3f85eba5da486084eb6fbdfa117a7e",
            "value": "Generating train split: 100%"
          }
        },
        "4f9f07e27e1f4db5bf682f42b63482c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2beda49790f0442ab68ace94b33dba20",
            "max": 78577,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9003c162c7b94e2cb0d60eb7a0e4392f",
            "value": 78577
          }
        },
        "f9d1d0186c814bac912b6e03996a5d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80a45a1e1f214f55bbb4e91d0937caef",
            "placeholder": "​",
            "style": "IPY_MODEL_437cff1a37da44668fa6f99808dbb69a",
            "value": " 78577/78577 [00:00&lt;00:00, 167701.90 examples/s]"
          }
        },
        "5763fe17d41f4e72a776dcaf3ec85302": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a43c2001aea4eefae9f3f33cdc72307": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d3f85eba5da486084eb6fbdfa117a7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2beda49790f0442ab68ace94b33dba20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9003c162c7b94e2cb0d60eb7a0e4392f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "80a45a1e1f214f55bbb4e91d0937caef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "437cff1a37da44668fa6f99808dbb69a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}