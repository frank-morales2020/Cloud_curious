{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOqjFnLntdvf2c/nZnKr2Uy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07be2e4b1e444ec594d799605c1703cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fcb36d4313564ffd83022bfb6f04cb3c",
              "IPY_MODEL_3c9bdf25608c45258221720c86c44e1a",
              "IPY_MODEL_aabd677475774bda9604da3e94d64848"
            ],
            "layout": "IPY_MODEL_8036aeaaa4e04be5874e5ff451ee63df"
          }
        },
        "fcb36d4313564ffd83022bfb6f04cb3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc27068a1c0f4a2d938898d5a4247c49",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c53d234134134b81ba8e4d94c6f7a3cf",
            "value": "Loading‚Äáweights:‚Äá100%"
          }
        },
        "3c9bdf25608c45258221720c86c44e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e66bb3e61846ae82e90eb1cec80a42",
            "max": 711,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_915e86a50cff4b06926fe569ec2a534e",
            "value": 711
          }
        },
        "aabd677475774bda9604da3e94d64848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4168e53ac637439da0519b1ddd8a418e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6e3aef31b5584431a780ca7c1e441189",
            "value": "‚Äá711/711‚Äá[00:02&lt;00:00,‚Äá177.92it/s,‚ÄáMaterializing‚Äáparam=multi_modal_projector.linear_2.weight]"
          }
        },
        "8036aeaaa4e04be5874e5ff451ee63df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc27068a1c0f4a2d938898d5a4247c49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c53d234134134b81ba8e4d94c6f7a3cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0e66bb3e61846ae82e90eb1cec80a42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "915e86a50cff4b06926fe569ec2a534e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4168e53ac637439da0519b1ddd8a418e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e3aef31b5584431a780ca7c1e441189": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9721c44aade640179c317276fd57a0e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ff57b9b6abd4482be5b95e73ffc5349",
              "IPY_MODEL_4432cf977c5641cbafdd55ddb9f0276b",
              "IPY_MODEL_06e741d2f436471db092fcc122cedc72"
            ],
            "layout": "IPY_MODEL_47afc1c10b0f4f8ab90bb61a8b566ef9"
          }
        },
        "3ff57b9b6abd4482be5b95e73ffc5349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_890c6a06a29c45afadf253ea60ef8e6c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fd926d6141f04af1b1698c38c92c9485",
            "value": "Loading‚Äáweights:‚Äá100%"
          }
        },
        "4432cf977c5641cbafdd55ddb9f0276b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d36207bc542456cb489060495afb1e6",
            "max": 711,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a78a34bb73b34c4d84fae50fad86f7bc",
            "value": 711
          }
        },
        "06e741d2f436471db092fcc122cedc72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d74e7713e5041dabea36686ee37dbbc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_24348d15d5524659b07500f0239980d7",
            "value": "‚Äá711/711‚Äá[00:02&lt;00:00,‚Äá182.38it/s,‚ÄáMaterializing‚Äáparam=multi_modal_projector.linear_2.weight]"
          }
        },
        "47afc1c10b0f4f8ab90bb61a8b566ef9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "890c6a06a29c45afadf253ea60ef8e6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd926d6141f04af1b1698c38c92c9485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d36207bc542456cb489060495afb1e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78a34bb73b34c4d84fae50fad86f7bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d74e7713e5041dabea36686ee37dbbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24348d15d5524659b07500f0239980d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/FT_V2TXT_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q trl"
      ],
      "metadata": {
        "id": "SeI_D57yiIm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vDACJdh_h9ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. INSTALL REQUIRED DEPENDENCIES\n",
        "!pip install -q mistral-common"
      ],
      "metadata": {
        "id": "oYKdvJt-lwlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers -q\n",
        "!pip install -U bitsandbytes>=0.46.1 -q"
      ],
      "metadata": {
        "id": "AcWkcwAzjLXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "NVPVS_6uqTTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original duration: 936.28s for barackobama2004dncARXE.mp3\n",
        "\n",
        "Original duration: 210.68s for barackobamatransitionaddress1.mp3\n",
        "\n",
        "Original duration: 183.02s for brad_pitt_sag_2020.mp3\n",
        "\n",
        "Original duration: 2415.33s for mandela_davos_1999.mp3\n",
        "\n",
        "Original duration: 1801.37s for mark_carney_davos_2026.mp3\n",
        "\n",
        "Original duration: 2585.63s for mlk_mountaintop_1968.mp3"
      ],
      "metadata": {
        "id": "tUyqNe1rlGwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINE TUNING"
      ],
      "metadata": {
        "id": "P0EeUVyJvVpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUPPRESS WARNINGS - Add this at the very top\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Some matrices hidden dimension is not a multiple of 64\")\n",
        "warnings.filterwarnings(\"ignore\", module=\"bitsandbytes\")\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "from datasets import Dataset\n",
        "from dataclasses import dataclass\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForSpeechSeq2Seq,\n",
        "    BitsAndBytesConfig,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import transformers.models.voxtral_realtime.modeling_voxtral_realtime as vox_mod\n",
        "\n",
        "from warnings import simplefilter\n",
        "simplefilter(action=\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "MODEL_ID = \"mistralai/Voxtral-Mini-4B-Realtime-2602\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/data/H2E_Challenge/Voxtral_FineTune\"\n",
        "AUDIT_PATH = \"/content/drive/MyDrive/data/H2E_Challenge/H2E_Final_Performance_Audit.csv\"\n",
        "TARGET_SR = 16000\n",
        "CHUNK_LENGTH_SEC = 30.0\n",
        "MAX_TEXT_LENGTH = 448\n",
        "\n",
        "# ==================== 1. DATASET PREPARATION ====================\n",
        "df = pd.read_csv(AUDIT_PATH)\n",
        "paths = [f\"/content/drive/MyDrive/data/{f}\" for f in df['File']]\n",
        "texts = df['Transcript'].astype(str).tolist()\n",
        "\n",
        "chunked_audios, chunked_texts = [], []\n",
        "for path, text in zip(paths, texts):\n",
        "    try:\n",
        "        array, _ = librosa.load(path, sr=TARGET_SR, mono=True)\n",
        "        max_samples = int(CHUNK_LENGTH_SEC * TARGET_SR)\n",
        "        if len(array) > max_samples: array = array[:max_samples]\n",
        "        chunked_audios.append({\"array\": array.astype(np.float32).tolist(), \"sampling_rate\": TARGET_SR})\n",
        "        chunked_texts.append(text)\n",
        "    except Exception as e: print(f\"‚úó Failed {path}: {e}\")\n",
        "\n",
        "dataset = Dataset.from_dict({\"audio\": chunked_audios, \"text\": chunked_texts})\n",
        "\n",
        "# ==================== 2. MODEL SETUP ====================\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "\n",
        "# Get model dimensions\n",
        "lm_hidden_size = model.config.hidden_size\n",
        "print(f\"Language model hidden size: {lm_hidden_size}\")\n",
        "\n",
        "# Audio feature dimension for Voxtral\n",
        "audio_feature_dim = 1280\n",
        "print(f\"Audio feature dimension: {audio_feature_dim}\")\n",
        "print(f\"Setting up adapter: {audio_feature_dim} ‚Üí {lm_hidden_size}\")\n",
        "\n",
        "# Create adapter that projects audio features to match language model hidden size\n",
        "class AudioFeatureAdapter(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(in_dim, out_dim)\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "# Initialize adapter with correct dimensions\n",
        "model.audio_adapter = AudioFeatureAdapter(audio_feature_dim, lm_hidden_size).to(model.device).to(torch.bfloat16)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    audio_arrays = []\n",
        "    for x in examples[\"audio\"]:\n",
        "        arr = np.array(x[\"array\"], dtype=np.float32)\n",
        "        audio_arrays.append(arr)\n",
        "\n",
        "    audio_inputs = processor.feature_extractor(\n",
        "        audio_arrays,\n",
        "        sampling_rate=TARGET_SR, return_tensors=\"np\", padding=True\n",
        "    )\n",
        "\n",
        "    input_features = torch.tensor(audio_inputs[\"input_features\"], dtype=torch.bfloat16)\n",
        "\n",
        "    text_inputs = processor.tokenizer(\n",
        "        examples[\"text\"], return_tensors=\"np\", padding=\"max_length\",\n",
        "        truncation=True, max_length=MAX_TEXT_LENGTH\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"input_features\": input_features,\n",
        "        \"input_ids\": torch.tensor(text_inputs[\"input_ids\"], dtype=torch.long),\n",
        "        \"labels\": torch.tensor(text_inputs[\"input_ids\"], dtype=torch.long),\n",
        "        \"attention_mask\": torch.tensor(text_inputs[\"attention_mask\"], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(preprocess_function, batched=True, batch_size=2, remove_columns=dataset.column_names)\n",
        "dataset.set_format(\"torch\")\n",
        "\n",
        "# Prepare for k-bit training but disable gradient checkpointing\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.gradient_checkpointing_disable()  # Explicitly disable gradient checkpointing\n",
        "\n",
        "model = get_peft_model(model, LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        "))\n",
        "\n",
        "# ==================== 3. SIMPLIFIED PATCHED FORWARD ====================\n",
        "def patched_forward(\n",
        "    self, input_ids=None, input_features=None, attention_mask=None, position_ids=None,\n",
        "    past_key_values=None, encoder_past_key_values=None, padding_cache=None,\n",
        "    inputs_embeds=None, encoder_inputs_embeds=None, labels=None, use_cache=None,\n",
        "    cache_position=None, logits_to_keep=0, num_delay_tokens=None, **kwargs\n",
        "):\n",
        "    # Get text embeddings\n",
        "    if inputs_embeds is None:\n",
        "        inputs_embeds = self.get_input_embeddings()(input_ids)\n",
        "\n",
        "    batch_size = inputs_embeds.shape[0]\n",
        "    device = inputs_embeds.device\n",
        "\n",
        "    # Process audio if provided\n",
        "    if input_features is not None:\n",
        "        # Get audio features\n",
        "        audio_outputs = self.audio_tower(\n",
        "            input_features.to(device, dtype=torch.bfloat16),\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # Project audio features\n",
        "        audio_features = audio_outputs.last_hidden_state\n",
        "        audio_projected = self.audio_adapter(audio_features)\n",
        "\n",
        "        # Simple concatenation\n",
        "        inputs_embeds = torch.cat([audio_projected, inputs_embeds], dim=1)\n",
        "\n",
        "        # Update attention mask\n",
        "        if attention_mask is not None:\n",
        "            audio_mask = torch.ones(batch_size, audio_projected.shape[1],\n",
        "                                   device=device, dtype=attention_mask.dtype)\n",
        "            attention_mask = torch.cat([audio_mask, attention_mask], dim=1)\n",
        "\n",
        "    # Time conditioning - FIXED: properly handle dimensions\n",
        "    if num_delay_tokens is None:\n",
        "        num_delay_tokens = self.config.default_num_delay_tokens\n",
        "\n",
        "    # Create time tensor with correct shape for t_cond\n",
        "    time_tensor = torch.full((batch_size,), num_delay_tokens, device=device, dtype=torch.long)\n",
        "    t_cond = self.time_embedding(time_tensor)  # Shape: [batch_size, hidden_size]\n",
        "\n",
        "    # Call language model\n",
        "    lm_outputs = self.language_model(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        attention_mask=attention_mask,\n",
        "        position_ids=position_ids,\n",
        "        past_key_values=past_key_values,\n",
        "        use_cache=False,  # Disable cache to avoid issues\n",
        "        t_cond=t_cond,  # Pass t_cond directly\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    logits = lm_outputs.logits\n",
        "    loss = None\n",
        "\n",
        "    if labels is not None:\n",
        "        # Create labels that ignore audio tokens\n",
        "        if input_features is not None:\n",
        "            audio_len = audio_projected.shape[1]\n",
        "            new_labels = torch.full((batch_size, audio_len + labels.shape[1]), -100,\n",
        "                                   device=device, dtype=labels.dtype)\n",
        "            new_labels[:, audio_len:] = labels\n",
        "            labels = new_labels\n",
        "\n",
        "        # Shift for next token prediction\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "        loss = nn.functional.cross_entropy(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)),\n",
        "            shift_labels.view(-1),\n",
        "            ignore_index=-100\n",
        "        )\n",
        "\n",
        "    return vox_mod.VoxtralRealtimeCausalLMOutputWithPast(\n",
        "        loss=loss,\n",
        "        logits=logits,\n",
        "        past_key_values=lm_outputs.past_key_values\n",
        "    )\n",
        "\n",
        "# Apply the patch\n",
        "vox_mod.VoxtralRealtimeForConditionalGeneration.forward = patched_forward\n",
        "\n",
        "# ==================== 4. TRAINING ====================\n",
        "@dataclass\n",
        "class SimpleCollator:\n",
        "    def __call__(self, features):\n",
        "        batch = {}\n",
        "        for key in [\"input_features\", \"input_ids\", \"labels\", \"attention_mask\"]:\n",
        "            if key in features[0]:\n",
        "                tensors = [f[key] for f in features]\n",
        "                if key == \"input_features\":\n",
        "                    batch[key] = torch.stack(tensors).to(torch.bfloat16)\n",
        "                else:\n",
        "                    batch[key] = torch.stack(tensors)\n",
        "        return batch\n",
        "\n",
        "# Disable all caching and checkpointing in training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-4,\n",
        "    max_steps=500,\n",
        "    bf16=True,\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy=\"no\",\n",
        "    dataloader_drop_last=False,\n",
        "    gradient_checkpointing=False,  # Explicitly disable\n",
        "    use_cache=False,  # Disable cache\n",
        "    ddp_find_unused_parameters=False,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=SimpleCollator(),\n",
        ")\n"
      ],
      "metadata": {
        "id": "eI0VZRf40guU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üöÄ Starting Training with Fixed Forward Pass...\")\n",
        "print(f\"Model device: {model.device}\")\n",
        "print(f\"Hidden size: {lm_hidden_size}\")\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üíæ Saving fine-tuned model...\")\n",
        "\n",
        "# Create the adapter directory\n",
        "final_adapter_path = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
        "os.makedirs(final_adapter_path, exist_ok=True)\n",
        "\n",
        "# Save the model and processor\n",
        "model.save_pretrained(final_adapter_path)\n",
        "processor.save_pretrained(final_adapter_path)\n",
        "\n",
        "print(f\"‚úÖ Model saved to: {final_adapter_path}\")\n",
        "\n",
        "# Verify the save\n",
        "print(\"\\nüìÅ Saved files:\")\n",
        "if os.path.exists(final_adapter_path):\n",
        "    for file in os.listdir(final_adapter_path):\n",
        "        file_path = os.path.join(final_adapter_path, file)\n",
        "        size = os.path.getsize(file_path) / 1024  # Size in KB\n",
        "        print(f\"  - {file} ({size:.2f} KB)\")\n",
        "else:\n",
        "    print(\"‚ùå Save failed - directory not found!\")\n",
        "\n",
        "print(\"\\nüéâ Training complete! You can now use the model for inference.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LVvKF4hjv1e2",
        "outputId": "6cb0bad1-23bd-485d-8e32-d15245c39176"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Training with Fixed Forward Pass...\n",
            "Model device: cuda:0\n",
            "Hidden size: 3072\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 19:49, Epoch 250/250]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>38.537854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>24.828914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>20.967784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>20.354013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>20.307916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>20.243488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>20.211427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>20.114462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>20.052942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>20.009079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>19.981792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>19.981122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>19.963904</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>19.954860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>19.940640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>19.881779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>19.841589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>19.818936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>19.810368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>19.804620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>19.789769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>19.786928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>19.771242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>19.771228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>19.761391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>19.756253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>19.748557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>19.742229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>19.733078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>19.727573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>19.723840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>19.724997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>19.712881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>19.705983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>19.704347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>19.697221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>19.707396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>19.700316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>19.699283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>19.699385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>19.708916</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>19.697469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>19.699226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>19.694067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>19.690590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>19.695134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>19.693703</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>19.693277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>19.688846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>19.689198</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "üíæ Saving fine-tuned model...\n",
            "‚úÖ Model saved to: /content/drive/MyDrive/data/H2E_Challenge/Voxtral_FineTune/final_adapter\n",
            "\n",
            "üìÅ Saved files:\n",
            "  - tekken.json (14560.89 KB)\n",
            "  - README.md (5.11 KB)\n",
            "  - adapter_config.json (0.97 KB)\n",
            "  - processor_config.json (0.38 KB)\n",
            "  - adapter_model.safetensors (15840.23 KB)\n",
            "\n",
            "üéâ Training complete! You can now use the model for inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DEBUG-INFERENCE-CODE"
      ],
      "metadata": {
        "id": "SZxVRLhJX3Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/data/*.mp3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aP78119mKte",
        "outputId": "3cddefae-6bdf-48b4-de6a-9b9d8ae8dbb5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data/barackobama2004dncARXE.mp3\n",
            "/content/drive/MyDrive/data/barackobamatransitionaddress1.mp3\n",
            "/content/drive/MyDrive/data/brad_pitt_sag_2020.mp3\n",
            "/content/drive/MyDrive/data/mandela_davos_1999.mp3\n",
            "/content/drive/MyDrive/data/mark_carney_davos_2026.mp3\n",
            "/content/drive/MyDrive/data/mlk_mountaintop_1968.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltha /content/drive/MyDrive/data/H2E_Challenge/Voxtral_FineTune/final_adapter/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV5BOmv-sBYc",
        "outputId": "860a7074-4061-4fb8-fc63-3d1e03426796"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 30M\n",
            "-rw------- 1 root root  384 Feb 23 05:25 processor_config.json\n",
            "-rw------- 1 root root  15M Feb 23 05:25 tekken.json\n",
            "-rw------- 1 root root  991 Feb 23 05:25 adapter_config.json\n",
            "-rw------- 1 root root  16M Feb 23 05:25 adapter_model.safetensors\n",
            "-rw------- 1 root root 5.2K Feb 23 05:25 README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "from peft import PeftModel\n",
        "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "MODEL_ID = \"mistralai/Voxtral-Mini-4B-Realtime-2602\"\n",
        "ADAPTER_PATH = \"/content/drive/MyDrive/data/H2E_Challenge/Voxtral_FineTune/final_adapter\"\n",
        "AUDIO_FILES = [\n",
        "    \"/content/drive/MyDrive/data/barackobama2004dncARXE.mp3\",\n",
        "    \"/content/drive/MyDrive/data/barackobamatransitionaddress1.mp3\",\n",
        "    \"/content/drive/MyDrive/data/brad_pitt_sag_2020.mp3\",\n",
        "    \"/content/drive/MyDrive/data/mandela_davos_1999.mp3\",\n",
        "    \"/content/drive/MyDrive/data/mark_carney_davos_2026.mp3\",\n",
        "    \"/content/drive/MyDrive/data/mlk_mountaintop_1968.mp3\"\n",
        "]\n",
        "\n",
        "# ==================== 1. SETUP ====================\n",
        "print(\"üîß Setting up tokenizer and processor...\")\n",
        "\n",
        "mistral_tokenizer = MistralTokenizer.v3(is_tekken=True)\n",
        "backend_tokenizer = mistral_tokenizer.instruct_tokenizer.tokenizer\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "# ==================== 2. LOAD MODEL ====================\n",
        "print(\"üîÑ Loading base model...\")\n",
        "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "print(\"üîÑ Loading and merging LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "model = model.merge_and_unload()\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "lQkN8blrAqfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 3. DEBUGGING FUNCTIONS ====================\n",
        "\n",
        "def analyze_audio_debug(audio_path):\n",
        "    \"\"\"Detailed audio analysis\"\"\"\n",
        "    print(f\"\\n  üìä AUDIO ANALYSIS for {os.path.basename(audio_path)}:\")\n",
        "\n",
        "    # Load audio\n",
        "    speech, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Basic stats\n",
        "    duration = len(speech) / sr\n",
        "    print(f\"     Duration: {duration:.2f} seconds\")\n",
        "    print(f\"     Sample rate: {sr} Hz\")\n",
        "    print(f\"     Samples: {len(speech)}\")\n",
        "\n",
        "    # Amplitude stats\n",
        "    print(f\"     Mean amplitude: {np.mean(np.abs(speech)):.6f}\")\n",
        "    print(f\"     Max amplitude: {np.max(np.abs(speech)):.6f}\")\n",
        "    print(f\"     RMS energy: {np.sqrt(np.mean(speech**2)):.6f}\")\n",
        "\n",
        "    # Check for silence/audio issues\n",
        "    silence_threshold = 0.01\n",
        "    silent_samples = np.sum(np.abs(speech) < silence_threshold)\n",
        "    silence_ratio = silent_samples / len(speech)\n",
        "    print(f\"     Silence ratio (<{silence_threshold}): {silence_ratio:.2%}\")\n",
        "\n",
        "    # Check if audio might be problematic\n",
        "    if np.max(np.abs(speech)) < 0.01:\n",
        "        print(\"  ‚ö†Ô∏è  WARNING: Audio amplitude is very low!\")\n",
        "    if silence_ratio > 0.8:\n",
        "        print(\"  ‚ö†Ô∏è  WARNING: Audio is mostly silence!\")\n",
        "\n",
        "    # Save a small segment for inspection (optional)\n",
        "    debug_dir = \"debug_audio\"\n",
        "    os.makedirs(debug_dir, exist_ok=True)\n",
        "    debug_path = os.path.join(debug_dir, f\"debug_{os.path.basename(audio_path)}.wav\")\n",
        "    sf.write(debug_path, speech, sr)\n",
        "    print(f\"     Saved debug copy to: {debug_path}\")\n",
        "\n",
        "    return speech, sr\n",
        "\n",
        "def test_different_segments(audio_path, segment_duration=10):\n",
        "    \"\"\"Test different segments of the audio file\"\"\"\n",
        "    print(f\"\\n  üîç TESTING DIFFERENT SEGMENTS:\")\n",
        "\n",
        "    speech, sr = librosa.load(audio_path, sr=16000)\n",
        "    total_duration = len(speech) / sr\n",
        "\n",
        "    # Test first 10s, middle 10s, and last 10s (if enough length)\n",
        "    segments = []\n",
        "\n",
        "    # First segment\n",
        "    if total_duration >= segment_duration:\n",
        "        segments.append((\"first\", speech[:int(segment_duration * sr)]))\n",
        "\n",
        "    # Middle segment (if longer than 2x segment_duration)\n",
        "    if total_duration >= segment_duration * 2:\n",
        "        mid_start = int((total_duration/2 - segment_duration/2) * sr)\n",
        "        segments.append((\"middle\", speech[mid_start:mid_start + int(segment_duration * sr)]))\n",
        "\n",
        "    # Last segment\n",
        "    if total_duration >= segment_duration:\n",
        "        segments.append((\"last\", speech[-int(segment_duration * sr):]))\n",
        "\n",
        "    results = {}\n",
        "    for seg_name, seg_audio in segments:\n",
        "        print(f\"     Testing {seg_name} {len(seg_audio)/sr:.1f}s segment...\")\n",
        "\n",
        "        # Normalize\n",
        "        seg_audio = seg_audio / (np.max(np.abs(seg_audio)) + 1e-8)\n",
        "\n",
        "        # Process\n",
        "        inputs = processor(audio=seg_audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "        input_features = inputs.input_features.to(\"cuda\", dtype=torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                input_features=input_features,\n",
        "                max_new_tokens=128,\n",
        "                do_sample=False,\n",
        "                num_beams=1,\n",
        "                use_cache=True\n",
        "            )\n",
        "\n",
        "        transcription = processor.batch_decode(\n",
        "            generated_ids,\n",
        "            skip_special_tokens=True\n",
        "        )[0].strip()\n",
        "\n",
        "        results[seg_name] = transcription\n",
        "        print(f\"       ‚Üí '{transcription if transcription else '[EMPTY]'}'\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def transcribe_with_different_params(audio_segment):\n",
        "    \"\"\"Try different generation parameters\"\"\"\n",
        "    print(f\"\\n  üéõÔ∏è  TESTING DIFFERENT PARAMETERS:\")\n",
        "\n",
        "    param_sets = [\n",
        "        {\"name\": \"Greedy\", \"do_sample\": False, \"num_beams\": 1, \"temperature\": 1.0},\n",
        "        {\"name\": \"Beam 3\", \"do_sample\": False, \"num_beams\": 3, \"temperature\": 1.0},\n",
        "        {\"name\": \"Sampling (temp=0.3)\", \"do_sample\": True, \"num_beams\": 1, \"temperature\": 0.3},\n",
        "        {\"name\": \"Sampling (temp=0.7)\", \"do_sample\": True, \"num_beams\": 1, \"temperature\": 0.7},\n",
        "        {\"name\": \"Beam + Sampling\", \"do_sample\": True, \"num_beams\": 2, \"temperature\": 0.5},\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "    for params in param_sets:\n",
        "        print(f\"     Trying {params['name']}...\")\n",
        "\n",
        "        # Process\n",
        "        inputs = processor(audio=audio_segment, sampling_rate=16000, return_tensors=\"pt\")\n",
        "        input_features = inputs.input_features.to(\"cuda\", dtype=torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                input_features=input_features,\n",
        "                max_new_tokens=128,\n",
        "                use_cache=True,\n",
        "                do_sample=params[\"do_sample\"],\n",
        "                num_beams=params[\"num_beams\"],\n",
        "                temperature=params[\"temperature\"]\n",
        "            )\n",
        "\n",
        "        transcription = processor.batch_decode(\n",
        "            generated_ids,\n",
        "            skip_special_tokens=True\n",
        "        )[0].strip()\n",
        "\n",
        "        results[params[\"name\"]] = transcription\n",
        "        print(f\"       ‚Üí '{transcription if transcription else '[EMPTY]'}'\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ==================== 4. RUN DEBUGGING INFERENCE ====================\n",
        "print(f\"üöÄ Processing {len(AUDIO_FILES)} files with debugging...\\n\")\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for i, path in enumerate(AUDIO_FILES, 1):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"‚ö†Ô∏è [{i}/{len(AUDIO_FILES)}] File not found: {path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"üìÅ [{i}/{len(AUDIO_FILES)}] Processing: {os.path.basename(path)}\")\n",
        "    print('='*60)\n",
        "\n",
        "    try:\n",
        "        # Step 1: Analyze audio\n",
        "        speech, sr = analyze_audio_debug(path)\n",
        "\n",
        "        # Step 2: Try different segments\n",
        "        segment_results = test_different_segments(path)\n",
        "\n",
        "        # Step 3: If first segment failed, try different parameters on first 10s\n",
        "        first_10s = speech[:int(10 * sr)]\n",
        "        first_10s = first_10s / (np.max(np.abs(first_10s)) + 1e-8)\n",
        "\n",
        "        param_results = transcribe_with_different_params(first_10s)\n",
        "\n",
        "        # Store results\n",
        "        all_results[os.path.basename(path)] = {\n",
        "            'segment_tests': segment_results,\n",
        "            'param_tests': param_results\n",
        "        }\n",
        "\n",
        "        # Summary\n",
        "        print(f\"\\n  üìã SUMMARY for {os.path.basename(path)}:\")\n",
        "        successful = [v for v in segment_results.values() if v]\n",
        "        if successful:\n",
        "            print(f\"     ‚úÖ Best result: '{max(successful, key=len)}'\")\n",
        "        else:\n",
        "            print(f\"     ‚ùå All transcriptions empty!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error processing {path}: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print('='*60)\n",
        "\n",
        "# ==================== 5. FINAL SUMMARY ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä FINAL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for filename, results in all_results.items():\n",
        "    print(f\"\\n{filename}:\")\n",
        "\n",
        "    # Check if any successful transcriptions\n",
        "    all_transcriptions = []\n",
        "    all_transcriptions.extend(results['segment_tests'].values())\n",
        "    all_transcriptions.extend(results['param_tests'].values())\n",
        "\n",
        "    successful = [t for t in all_transcriptions if t]\n",
        "\n",
        "    if successful:\n",
        "        print(f\"  ‚úÖ SUCCESS - Found {len(successful)} non-empty transcriptions\")\n",
        "        print(f\"     Best: '{max(successful, key=len)}'\")\n",
        "    else:\n",
        "        print(f\"  ‚ùå FAILED - All transcriptions empty\")\n",
        "\n",
        "print(\"\\n‚úÖ Debugging complete! Check the output to identify issues.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89gqiD6HX6tR",
        "outputId": "becc954a-4adb-4985-a785-6b6d438daadb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Processing 6 files with debugging...\n",
            "\n",
            "\n",
            "============================================================\n",
            "üìÅ [1/6] Processing: barackobama2004dncARXE.mp3\n",
            "============================================================\n",
            "\n",
            "  üìä AUDIO ANALYSIS for barackobama2004dncARXE.mp3:\n",
            "     Duration: 936.28 seconds\n",
            "     Sample rate: 16000 Hz\n",
            "     Samples: 14980494\n",
            "     Mean amplitude: 0.086645\n",
            "     Max amplitude: 0.668423\n",
            "     RMS energy: 0.130867\n",
            "     Silence ratio (<0.01): 17.98%\n",
            "     Saved debug copy to: debug_audio/debug_barackobama2004dncARXE.mp3.wav\n",
            "\n",
            "  üîç TESTING DIFFERENT SEGMENTS:\n",
            "     Testing first 10.0s segment...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`num_delay_tokens` was not provided. Falling back to `config.default_num_delay_tokens=6`. Consider preparing inputs with [`~VoxtralRealtimeProcessor.__call__`] which automatically sets this parameter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       ‚Üí 'Thank you so much. Thank you so much. Thank you. Thank you.'\n",
            "     Testing middle 10.0s segment...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INFERENCE-GOOD"
      ],
      "metadata": {
        "id": "KZuRwMin1qCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "from peft import PeftModel\n",
        "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "MODEL_ID = \"mistralai/Voxtral-Mini-4B-Realtime-2602\"\n",
        "ADAPTER_PATH = \"/content/drive/MyDrive/data/H2E_Challenge/Voxtral_FineTune/final_adapter\"\n",
        "AUDIO_FILES = [\n",
        "    \"/content/drive/MyDrive/data/barackobama2004dncARXE.mp3\",\n",
        "    \"/content/drive/MyDrive/data/barackobamatransitionaddress1.mp3\",\n",
        "    \"/content/drive/MyDrive/data/brad_pitt_sag_2020.mp3\",\n",
        "    \"/content/drive/MyDrive/data/mandela_davos_1999.mp3\",\n",
        "    \"/content/drive/MyDrive/data/mark_carney_davos_2026.mp3\",\n",
        "    \"/content/drive/MyDrive/data/mlk_mountaintop_1968.mp3\"\n",
        "]\n",
        "\n",
        "# ==================== 1. SETUP ====================\n",
        "print(\"üîß Setting up tokenizer and processor...\")\n",
        "\n",
        "mistral_tokenizer = MistralTokenizer.v3(is_tekken=True)\n",
        "backend_tokenizer = mistral_tokenizer.instruct_tokenizer.tokenizer\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "# ==================== 2. LOAD MODEL ====================\n",
        "print(\"üîÑ Loading base model...\")\n",
        "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "print(\"üîÑ Loading and merging LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "model = model.merge_and_unload()\n",
        "model.eval()\n",
        "\n",
        "# ==================== 3. ENHANCED FUNCTIONS ====================\n",
        "\n",
        "def find_best_speech_segment(audio, sr=16000, segment_duration=20, min_energy=0.01):\n",
        "    \"\"\"Find the segment with highest speech energy\"\"\"\n",
        "\n",
        "    segment_samples = int(segment_duration * sr)\n",
        "    hop_samples = int(2 * sr)\n",
        "\n",
        "    best_energy = 0\n",
        "    best_segment = None\n",
        "    best_start = 0\n",
        "    all_segments = []\n",
        "\n",
        "    for start in range(0, max(1, len(audio) - segment_samples), hop_samples):\n",
        "        end = min(start + segment_samples, len(audio))\n",
        "        segment = audio[start:end]\n",
        "        energy = np.sqrt(np.mean(segment**2))\n",
        "\n",
        "        all_segments.append((energy, start, segment))\n",
        "\n",
        "        if energy > best_energy and energy > min_energy:\n",
        "            best_energy = energy\n",
        "            best_segment = segment\n",
        "            best_start = start\n",
        "\n",
        "    # Sort all segments by energy\n",
        "    all_segments.sort(reverse=True)\n",
        "\n",
        "    return best_segment, best_energy, best_start, all_segments[:5]  # Return top 5\n",
        "\n",
        "def transcribe_segment(segment, sr=16000, segment_duration=20):\n",
        "    \"\"\"Transcribe a specific audio segment\"\"\"\n",
        "\n",
        "    # Normalize\n",
        "    if np.max(np.abs(segment)) > 0:\n",
        "        segment = segment / np.max(np.abs(segment))\n",
        "\n",
        "    # Prepare inputs\n",
        "    inputs = processor(audio=segment, sampling_rate=sr, return_tensors=\"pt\")\n",
        "    input_features = inputs.input_features.to(\"cuda\", dtype=torch.bfloat16)\n",
        "\n",
        "    # Try multiple generation strategies\n",
        "    strategies = [\n",
        "        {\"name\": \"Greedy\", \"params\": {\"do_sample\": False, \"num_beams\": 1}},\n",
        "        {\"name\": \"Beam 3\", \"params\": {\"do_sample\": False, \"num_beams\": 3}},\n",
        "        {\"name\": \"Sampling (temp=0.3)\", \"params\": {\"do_sample\": True, \"temperature\": 0.3, \"num_beams\": 1}},\n",
        "        {\"name\": \"Sampling (temp=0.5)\", \"params\": {\"do_sample\": True, \"temperature\": 0.5, \"num_beams\": 1}},\n",
        "        {\"name\": \"Beam+Sample\", \"params\": {\"do_sample\": True, \"temperature\": 0.4, \"num_beams\": 2}},\n",
        "    ]\n",
        "\n",
        "    best_transcription = \"\"\n",
        "\n",
        "    for strategy in strategies:\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                input_features=input_features,\n",
        "                max_new_tokens=128,\n",
        "                use_cache=True,\n",
        "                **strategy[\"params\"]\n",
        "            )\n",
        "\n",
        "        transcription = processor.batch_decode(\n",
        "            generated_ids,\n",
        "            skip_special_tokens=True\n",
        "        )[0].strip()\n",
        "\n",
        "        if transcription and len(transcription) > len(best_transcription):\n",
        "            best_transcription = transcription\n",
        "            print(f\"     ‚úì {strategy['name']}: {transcription[:50]}...\")\n",
        "\n",
        "    return best_transcription\n",
        "\n",
        "def transcribe_audio_enhanced(audio_path, segment_duration=15):\n",
        "    \"\"\"Enhanced transcription with fallback strategies\"\"\"\n",
        "\n",
        "    print(f\"  üìÇ Loading: {os.path.basename(audio_path)}\")\n",
        "    speech, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Special handling for Mandela\n",
        "    if \"mandela\" in audio_path.lower():\n",
        "        print(\"  üîç Using enhanced Mandela mode...\")\n",
        "\n",
        "        # Try different segment durations\n",
        "        durations = [10, 15, 20, 25, 30]\n",
        "\n",
        "        for dur in durations:\n",
        "            print(f\"  \\n  üìè Trying {dur}s segments...\")\n",
        "\n",
        "            # Find best segments\n",
        "            _, _, _, top_segments = find_best_speech_segment(\n",
        "                speech, sr, segment_duration=dur, min_energy=0.01\n",
        "            )\n",
        "\n",
        "            # Try top 3 segments\n",
        "            for i, (energy, start, segment) in enumerate(top_segments[:3]):\n",
        "                print(f\"    Segment {i+1} at {start/sr:.1f}s (energy: {energy:.4f})\")\n",
        "\n",
        "                # Try transcribing with multiple strategies\n",
        "                transcription = transcribe_segment(segment, sr, dur)\n",
        "\n",
        "                if transcription:\n",
        "                    return transcription\n",
        "\n",
        "        # If still no transcription, try the exact segment that worked before\n",
        "        print(\"\\n  üéØ Trying known working segment (1778s)...\")\n",
        "        working_start = 1778  # The segment that worked in debugging\n",
        "        working_end = working_start + 15\n",
        "        working_segment = speech[int(working_start * sr):int(working_end * sr)]\n",
        "        transcription = transcribe_segment(working_segment, sr)\n",
        "\n",
        "        if transcription:\n",
        "            return transcription\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    else:\n",
        "        # Regular handling for other files\n",
        "        best_segment, energy, start_time, _ = find_best_speech_segment(\n",
        "            speech, sr, segment_duration=segment_duration\n",
        "        )\n",
        "\n",
        "        if best_segment is not None:\n",
        "            print(f\"  ‚úÖ Found speech at {start_time/sr:.1f}s (energy: {energy:.4f})\")\n",
        "\n",
        "            # Normalize\n",
        "            if np.max(np.abs(best_segment)) > 0:\n",
        "                best_segment = best_segment / np.max(np.abs(best_segment))\n",
        "\n",
        "            # Prepare inputs\n",
        "            inputs = processor(\n",
        "                audio=best_segment,\n",
        "                sampling_rate=16000,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            input_features = inputs.input_features.to(\"cuda\", dtype=torch.bfloat16)\n",
        "\n",
        "            # Generate\n",
        "            with torch.no_grad():\n",
        "                generated_ids = model.generate(\n",
        "                    input_features=input_features,\n",
        "                    max_new_tokens=128,\n",
        "                    use_cache=True,\n",
        "                    do_sample=False,\n",
        "                    num_beams=1\n",
        "                )\n",
        "\n",
        "            transcription = processor.batch_decode(\n",
        "                generated_ids,\n",
        "                skip_special_tokens=True\n",
        "            )[0].strip()\n",
        "\n",
        "            return transcription\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è No clear speech found\")\n",
        "            return \"\"\n",
        "\n",
        "# ==================== 4. BATCH PROCESSING ====================\n",
        "print(f\"üöÄ Processing {len(AUDIO_FILES)} files...\\n\")\n",
        "\n",
        "results = {}\n",
        "successful = 0\n",
        "\n",
        "for i, path in enumerate(AUDIO_FILES, 1):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"‚ö†Ô∏è [{i}/{len(AUDIO_FILES)}] File not found: {path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÅ [{i}/{len(AUDIO_FILES)}] {os.path.basename(path)}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    try:\n",
        "        transcription = transcribe_audio_enhanced(path)\n",
        "        results[os.path.basename(path)] = transcription\n",
        "\n",
        "        if transcription:\n",
        "            successful += 1\n",
        "            print(f\"\\n‚úÖ {transcription}\")\n",
        "        else:\n",
        "            print(f\"\\n‚ùå No transcription\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# ==================== 5. SAVE RESULTS ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for filename, transcription in results.items():\n",
        "    if transcription:\n",
        "        print(f\"\\n‚úÖ {filename}:\")\n",
        "        print(f\"   {transcription}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå {filename}: No transcription\")\n",
        "\n",
        "# Save to file with timestamp\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "output_file = f\"transcriptions_{timestamp}.json\"\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "# Also save as readable text\n",
        "text_file = f\"transcriptions_{timestamp}.txt\"\n",
        "with open(text_file, 'w') as f:\n",
        "    for filename, transcription in results.items():\n",
        "        f.write(f\"{filename}:\\n{transcription}\\n\\n\")\n",
        "\n",
        "print(f\"\\nüìù Results saved to: {output_file} and {text_file}\")\n",
        "print(f\"\\nüìä Summary: {successful}/{len(AUDIO_FILES)} successful\")\n",
        "\n",
        "# Special note for Mandela\n",
        "if not results.get(\"mandela_davos_1999.mp3\"):\n",
        "    print(\"\\nüîç MANDELA DEBUG INFO:\")\n",
        "    print(\"   The model found speech but returned empty.\")\n",
        "    print(\"   This could be due to:\")\n",
        "    print(\"   - Strong accent challenging the model\")\n",
        "    print(\"   - Background noise/interference\")\n",
        "    print(\"   - Very short phrases\")\n",
        "    print(\"\\n   Try extracting a longer segment manually:\")\n",
        "    print(\"   import soundfile as sf\")\n",
        "    print(\"   speech, sr = librosac.load('mandela_davos_1999.mp3', sr=16000)\")\n",
        "    print(\"   segment = speech[1778*16000:(1778+30)*16000]  # 30s from 1778s\")\n",
        "    print(\"   sf.write('mandela_segment.wav', segment, 16000)\")\n",
        "\n",
        "print(\"\\n‚úÖ Inference complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "07be2e4b1e444ec594d799605c1703cc",
            "fcb36d4313564ffd83022bfb6f04cb3c",
            "3c9bdf25608c45258221720c86c44e1a",
            "aabd677475774bda9604da3e94d64848",
            "8036aeaaa4e04be5874e5ff451ee63df",
            "fc27068a1c0f4a2d938898d5a4247c49",
            "c53d234134134b81ba8e4d94c6f7a3cf",
            "c0e66bb3e61846ae82e90eb1cec80a42",
            "915e86a50cff4b06926fe569ec2a534e",
            "4168e53ac637439da0519b1ddd8a418e",
            "6e3aef31b5584431a780ca7c1e441189"
          ]
        },
        "id": "7S73KLiu0glQ",
        "outputId": "6e397324-6131-4a24-bed2-dea5e8e3b9f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Setting up tokenizer and processor...\n",
            "üîÑ Loading base model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/711 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07be2e4b1e444ec594d799605c1703cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading and merging LoRA adapter...\n",
            "üöÄ Processing 6 files...\n",
            "\n",
            "\n",
            "üìÅ [1/6] barackobama2004dncARXE.mp3\n",
            "------------------------------------------------------------\n",
            "  üìÇ Loading: barackobama2004dncARXE.mp3\n",
            "  ‚úÖ Found speech at 838.0s (energy: 0.1753)\n",
            "\n",
            "‚úÖ who believes that America has a place for him too.\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÅ [2/6] barackobamatransitionaddress1.mp3\n",
            "------------------------------------------------------------\n",
            "  üìÇ Loading: barackobamatransitionaddress1.mp3\n",
            "  ‚úÖ Found speech at 148.0s (energy: 0.2811)\n",
            "\n",
            "‚úÖ impact of the financial crisis on other sectors of our economy. And ensure that the rescue plan that passed Congress is working to\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÅ [3/6] brad_pitt_sag_2020.mp3\n",
            "------------------------------------------------------------\n",
            "  üìÇ Loading: brad_pitt_sag_2020.mp3\n",
            "  ‚úÖ Found speech at 124.0s (energy: 0.0901)\n",
            "\n",
            "‚úÖ I love our communities. I love our communities so much. It's been amazing to me. I've met so many\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÅ [4/6] mandela_davos_1999.mp3\n",
            "------------------------------------------------------------\n",
            "  üìÇ Loading: mandela_davos_1999.mp3\n",
            "  üîç Using enhanced Mandela mode...\n",
            "  \n",
            "  üìè Trying 10s segments...\n",
            "    Segment 1 at 8.0s (energy: 0.1119)\n",
            "    Segment 2 at 6.0s (energy: 0.1113)\n",
            "    Segment 3 at 1778.0s (energy: 0.1113)\n",
            "     ‚úì Greedy: So that I can intimidate you with my height....\n",
            "\n",
            "‚úÖ So that I can intimidate you with my height.\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÅ [5/6] mark_carney_davos_2026.mp3\n",
            "------------------------------------------------------------\n",
            "  üìÇ Loading: mark_carney_davos_2026.mp3\n",
            "  ‚úÖ Found speech at 226.0s (energy: 0.0611)\n",
            "\n",
            "‚úÖ For decades, countries like Canada prospered under what we called the rules-based international order. We joined as\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÅ [6/6] mlk_mountaintop_1968.mp3\n",
            "------------------------------------------------------------\n",
            "  üìÇ Loading: mlk_mountaintop_1968.mp3\n",
            "  ‚úÖ Found speech at 746.0s (energy: 0.1450)\n",
            "\n",
            "‚úÖ is not being fair to them. And that male loaf is in dire\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "üìä FINAL RESULTS\n",
            "============================================================\n",
            "\n",
            "‚úÖ barackobama2004dncARXE.mp3:\n",
            "   who believes that America has a place for him too.\n",
            "\n",
            "‚úÖ barackobamatransitionaddress1.mp3:\n",
            "   impact of the financial crisis on other sectors of our economy. And ensure that the rescue plan that passed Congress is working to\n",
            "\n",
            "‚úÖ brad_pitt_sag_2020.mp3:\n",
            "   I love our communities. I love our communities so much. It's been amazing to me. I've met so many\n",
            "\n",
            "‚úÖ mandela_davos_1999.mp3:\n",
            "   So that I can intimidate you with my height.\n",
            "\n",
            "‚úÖ mark_carney_davos_2026.mp3:\n",
            "   For decades, countries like Canada prospered under what we called the rules-based international order. We joined as\n",
            "\n",
            "‚úÖ mlk_mountaintop_1968.mp3:\n",
            "   is not being fair to them. And that male loaf is in dire\n",
            "\n",
            "üìù Results saved to: transcriptions_20260223_045040.json and transcriptions_20260223_045040.txt\n",
            "\n",
            "üìä Summary: 6/6 successful\n",
            "\n",
            "‚úÖ Inference complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "from peft import PeftModel\n",
        "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "MODEL_ID = \"mistralai/Voxtral-Mini-4B-Realtime-2602\"\n",
        "ADAPTER_PATH = \"/content/drive/MyDrive/data/H2E_Challenge/Voxtral_FineTune/final_adapter\"\n",
        "AUDIO_FILES = [\n",
        "    \"/content/drive/MyDrive/data/barackobama2004dncARXE.mp3\",\n",
        "    \"/content/drive/MyDrive/data/barackobamatransitionaddress1.mp3\",\n",
        "    \"/content/drive/MyDrive/data/brad_pitt_sag_2020.mp3\",\n",
        "    \"/content/drive/MyDrive/data/mandela_davos_1999.mp3\",\n",
        "    \"/content/drive/MyDrive/data/mark_carney_davos_2026.mp3\",\n",
        "    \"/content/drive/MyDrive/data/mlk_mountaintop_1968.mp3\"\n",
        "]\n",
        "\n",
        "# ==================== 1. SETUP ====================\n",
        "print(\"üîß Setting up tokenizer and processor...\")\n",
        "\n",
        "mistral_tokenizer = MistralTokenizer.v3(is_tekken=True)\n",
        "backend_tokenizer = mistral_tokenizer.instruct_tokenizer.tokenizer\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "# ==================== 2. LOAD MODEL ====================\n",
        "print(\"üîÑ Loading base model...\")\n",
        "base_model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "print(\"üîÑ Loading and merging LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)\n",
        "model = model.merge_and_unload()\n",
        "model.eval()\n",
        "\n",
        "# ==================== 3. ROBUST TRANSCRIPTION FUNCTION ====================\n",
        "\n",
        "def find_speech_segments(audio, sr=16000, segment_duration=15, min_energy=0.01, top_k=5):\n",
        "    \"\"\"Find top k speech segments by energy\"\"\"\n",
        "\n",
        "    segment_samples = int(segment_duration * sr)\n",
        "    hop_samples = int(2 * sr)\n",
        "\n",
        "    segments = []\n",
        "\n",
        "    for start in range(0, max(1, len(audio) - segment_samples), hop_samples):\n",
        "        end = min(start + segment_samples, len(audio))\n",
        "        segment = audio[start:end]\n",
        "        energy = np.sqrt(np.mean(segment**2))\n",
        "\n",
        "        if energy > min_energy:\n",
        "            segments.append((energy, start, segment))\n",
        "\n",
        "    # Sort by energy and return top k\n",
        "    segments.sort(reverse=True)\n",
        "    return segments[:top_k]\n",
        "\n",
        "def transcribe_segment(segment, sr=16000):\n",
        "    \"\"\"Transcribe a single audio segment\"\"\"\n",
        "\n",
        "    # Normalize\n",
        "    if np.max(np.abs(segment)) > 0:\n",
        "        segment = segment / np.max(np.abs(segment))\n",
        "\n",
        "    # Prepare inputs\n",
        "    inputs = processor(audio=segment, sampling_rate=sr, return_tensors=\"pt\")\n",
        "    input_features = inputs.input_features.to(\"cuda\", dtype=torch.bfloat16)\n",
        "\n",
        "    # Generate with greedy decoding (fastest)\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            input_features=input_features,\n",
        "            max_new_tokens=128,\n",
        "            use_cache=True,\n",
        "            do_sample=False,\n",
        "            num_beams=1\n",
        "        )\n",
        "\n",
        "    transcription = processor.batch_decode(\n",
        "        generated_ids,\n",
        "        skip_special_tokens=True\n",
        "    )[0].strip()\n",
        "\n",
        "    return transcription\n",
        "\n",
        "def transcribe_audio_robust(audio_path, segment_duration=15):\n",
        "    \"\"\"Robust transcription that handles various audio types\"\"\"\n",
        "\n",
        "    print(f\"  üìÇ Loading: {os.path.basename(audio_path)}\")\n",
        "    speech, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "    # Special handling for Mandela (known to have speech later in the file)\n",
        "    if \"mandela\" in audio_path.lower():\n",
        "        print(\"  üîç Using enhanced mode for Mandela speech...\")\n",
        "\n",
        "        # Try different segment durations\n",
        "        for dur in [10, 15, 20]:\n",
        "            segments = find_speech_segments(speech, sr, segment_duration=dur, top_k=3)\n",
        "\n",
        "            for energy, start, segment in segments:\n",
        "                print(f\"    Trying {dur}s segment at {start/sr:.1f}s (energy: {energy:.4f})\")\n",
        "                transcription = transcribe_segment(segment, sr)\n",
        "\n",
        "                if transcription:\n",
        "                    return transcription\n",
        "\n",
        "        # If still no transcription, try the known working segment\n",
        "        print(\"    Trying known working segment at 1778s...\")\n",
        "        working_start = 1778\n",
        "        working_segment = speech[int(working_start * sr):int((working_start + 15) * sr)]\n",
        "        transcription = transcribe_segment(working_segment, sr)\n",
        "\n",
        "        return transcription\n",
        "\n",
        "    else:\n",
        "        # Regular handling for other files\n",
        "        segments = find_speech_segments(speech, sr, segment_duration=segment_duration, top_k=1)\n",
        "\n",
        "        if segments:\n",
        "            energy, start, best_segment = segments[0]\n",
        "            print(f\"  ‚úÖ Found speech at {start/sr:.1f}s (energy: {energy:.4f})\")\n",
        "            return transcribe_segment(best_segment, sr)\n",
        "        else:\n",
        "            print(f\"  ‚ö†Ô∏è No clear speech found\")\n",
        "            return \"\"\n",
        "\n",
        "# ==================== 4. BATCH PROCESSING ====================\n",
        "print(f\"üöÄ Processing {len(AUDIO_FILES)} files...\\n\")\n",
        "\n",
        "results = {}\n",
        "successful = 0\n",
        "\n",
        "for i, path in enumerate(AUDIO_FILES, 1):\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"‚ö†Ô∏è [{i}/{len(AUDIO_FILES)}] File not found: {path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nüìÅ [{i}/{len(AUDIO_FILES)}] {os.path.basename(path)}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    try:\n",
        "        transcription = transcribe_audio_robust(path)\n",
        "        results[os.path.basename(path)] = transcription\n",
        "\n",
        "        if transcription:\n",
        "            successful += 1\n",
        "            print(f\"\\n‚úÖ {transcription}\")\n",
        "        else:\n",
        "            print(f\"\\n‚ùå No transcription\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "# ==================== 5. SAVE RESULTS ====================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for filename, transcription in results.items():\n",
        "    if transcription:\n",
        "        print(f\"\\n‚úÖ {filename}:\")\n",
        "        print(f\"   {transcription}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ùå {filename}: No transcription\")\n",
        "\n",
        "# Save with timestamp\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "output_file = f\"transcriptions_{timestamp}.json\"\n",
        "text_file = f\"transcriptions_{timestamp}.txt\"\n",
        "\n",
        "with open(output_file, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "with open(text_file, 'w') as f:\n",
        "    for filename, transcription in results.items():\n",
        "        f.write(f\"{filename}:\\n{transcription}\\n\\n\")\n",
        "\n",
        "print(f\"\\nüìù Results saved to: {output_file} and {text_file}\")\n",
        "print(f\"\\nüìä Summary: {successful}/{len(AUDIO_FILES)} successful\")\n",
        "print(\"\\n‚úÖ Inference complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9721c44aade640179c317276fd57a0e3",
            "3ff57b9b6abd4482be5b95e73ffc5349",
            "4432cf977c5641cbafdd55ddb9f0276b",
            "06e741d2f436471db092fcc122cedc72",
            "47afc1c10b0f4f8ab90bb61a8b566ef9",
            "890c6a06a29c45afadf253ea60ef8e6c",
            "fd926d6141f04af1b1698c38c92c9485",
            "4d36207bc542456cb489060495afb1e6",
            "a78a34bb73b34c4d84fae50fad86f7bc",
            "6d74e7713e5041dabea36686ee37dbbc",
            "24348d15d5524659b07500f0239980d7"
          ]
        },
        "id": "6axUjhOL2VzU",
        "outputId": "3a54a931-8f3a-4c15-bbc8-4ba49511aa17"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Setting up tokenizer and processor...\n",
            "üîÑ Loading base model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/711 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9721c44aade640179c317276fd57a0e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading and merging LoRA adapter...\n",
            "üöÄ Processing 6 files...\n",
            "\n",
            "\n",
            "üìÅ [1/6] barackobama2004dncARXE.mp3\n",
            "------------------------------------------------------------\n",
            "  üìÇ Loading: barackobama2004dncARXE.mp3\n",
            "  ‚úÖ Found speech at 838.0s (energy: 0.1753)\n",
            "\n",
            "‚úÖ who believes that America has a place for him too.\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÅ [2/6] barackobamatransitionaddress1.mp3\n",
            "------------------------------------------------------------\n",
            "  üìÇ Loading: barackobamatransitionaddress1.mp3\n",
            "  ‚úÖ Found speech at 148.0s (energy: 0.2811)\n",
            "\n",
            "‚úÖ impact of the financial crisis on other sectors of our economy. And ensure that the rescue plan that passed Congress is working to\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÅ [3/6] brad_pitt_sag_2020.mp3\n",
            "------------------------------------------------------------\n",
            "  üìÇ Loading: brad_pitt_sag_2020.mp3\n",
            "  ‚úÖ Found speech at 124.0s (energy: 0.0901)\n",
            "\n",
            "‚úÖ I love our communities. I love our communities so much. It's been amazing to me. I've met so many\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÅ [4/6] mandela_davos_1999.mp3\n",
            "------------------------------------------------------------\n",
            "  üìÇ Loading: mandela_davos_1999.mp3\n",
            "  üîç Using enhanced mode for Mandela speech...\n",
            "    Trying 10s segment at 8.0s (energy: 0.1119)\n",
            "    Trying 10s segment at 6.0s (energy: 0.1113)\n",
            "    Trying 10s segment at 1778.0s (energy: 0.1113)\n",
            "\n",
            "‚úÖ So that I can intimidate you with my height.\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÅ [5/6] mark_carney_davos_2026.mp3\n",
            "------------------------------------------------------------\n",
            "  üìÇ Loading: mark_carney_davos_2026.mp3\n",
            "  ‚úÖ Found speech at 226.0s (energy: 0.0611)\n",
            "\n",
            "‚úÖ For decades, countries like Canada prospered under what we called the rules-based international order. We joined as\n",
            "------------------------------------------------------------\n",
            "\n",
            "üìÅ [6/6] mlk_mountaintop_1968.mp3\n",
            "------------------------------------------------------------\n",
            "  üìÇ Loading: mlk_mountaintop_1968.mp3\n",
            "  ‚úÖ Found speech at 746.0s (energy: 0.1450)\n",
            "\n",
            "‚úÖ is not being fair to them. And that male loaf is in dire\n",
            "------------------------------------------------------------\n",
            "\n",
            "============================================================\n",
            "üìä FINAL RESULTS\n",
            "============================================================\n",
            "\n",
            "‚úÖ barackobama2004dncARXE.mp3:\n",
            "   who believes that America has a place for him too.\n",
            "\n",
            "‚úÖ barackobamatransitionaddress1.mp3:\n",
            "   impact of the financial crisis on other sectors of our economy. And ensure that the rescue plan that passed Congress is working to\n",
            "\n",
            "‚úÖ brad_pitt_sag_2020.mp3:\n",
            "   I love our communities. I love our communities so much. It's been amazing to me. I've met so many\n",
            "\n",
            "‚úÖ mandela_davos_1999.mp3:\n",
            "   So that I can intimidate you with my height.\n",
            "\n",
            "‚úÖ mark_carney_davos_2026.mp3:\n",
            "   For decades, countries like Canada prospered under what we called the rules-based international order. We joined as\n",
            "\n",
            "‚úÖ mlk_mountaintop_1968.mp3:\n",
            "   is not being fair to them. And that male loaf is in dire\n",
            "\n",
            "üìù Results saved to: transcriptions_20260223_045638.json and transcriptions_20260223_045638.txt\n",
            "\n",
            "üìä Summary: 6/6 successful\n",
            "\n",
            "‚úÖ Inference complete!\n"
          ]
        }
      ]
    }
  ]
}