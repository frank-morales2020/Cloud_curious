{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPIIPOpb1Da6oYumivALTNH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "275618762775440f8dafce8f2bb346b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d89d0ec608f34b2990c4a9088e9b3fd9",
              "IPY_MODEL_434c81650c884522af1b40d55a7c5282",
              "IPY_MODEL_2266141f974642c79cc86dc455e00289"
            ],
            "layout": "IPY_MODEL_4746e5c8cb1d451ba3fb70c973ddeac6"
          }
        },
        "d89d0ec608f34b2990c4a9088e9b3fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a535e29dd2db4e7286842b33cedba999",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7bddeb4111c14a6f8a3e715b4fa29515",
            "value": "Loading‚Äáweights:‚Äá100%"
          }
        },
        "434c81650c884522af1b40d55a7c5282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e684547fd474448cbd0d8aeb7336f2de",
            "max": 711,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57adc4afe89b4d8e96541e282a6119fb",
            "value": 711
          }
        },
        "2266141f974642c79cc86dc455e00289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53b7ca4bc00f4c58a9f2064d2d8ef722",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e16c59ee3b5c4a6b8ec2eb105f823357",
            "value": "‚Äá711/711‚Äá[00:03&lt;00:00,‚Äá571.84it/s,‚ÄáMaterializing‚Äáparam=multi_modal_projector.linear_2.weight]"
          }
        },
        "4746e5c8cb1d451ba3fb70c973ddeac6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a535e29dd2db4e7286842b33cedba999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bddeb4111c14a6f8a3e715b4fa29515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e684547fd474448cbd0d8aeb7336f2de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57adc4afe89b4d8e96541e282a6119fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53b7ca4bc00f4c58a9f2064d2d8ef722": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e16c59ee3b5c4a6b8ec2eb105f823357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be724795132a4961adcca7563242f3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_972ce361b02d4236b4c49d11c543345d",
              "IPY_MODEL_b0aabf7ff1624072a869b137fab45812",
              "IPY_MODEL_dba47a8b31ba48ea919c65d7f264cabe"
            ],
            "layout": "IPY_MODEL_31597a4feabc4e798993e3d7a37e3842"
          }
        },
        "972ce361b02d4236b4c49d11c543345d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a02db8e56a84cd180e8b6fa7d5b9ffb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7a21e2fcfff04fbb9b3abf9a2d961058",
            "value": "Map:‚Äá100%"
          }
        },
        "b0aabf7ff1624072a869b137fab45812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52ae4a9e32af41b29e59456f833383a4",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d2cb610903fe4f84841a7d1b3aaaace4",
            "value": 6
          }
        },
        "dba47a8b31ba48ea919c65d7f264cabe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f00f544cec849c68f546a755bba6d54",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_12a795766e0f4a4ca6eb53c8c5674a3a",
            "value": "‚Äá6/6‚Äá[00:01&lt;00:00,‚Äá‚Äá3.73‚Äáexamples/s]"
          }
        },
        "31597a4feabc4e798993e3d7a37e3842": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a02db8e56a84cd180e8b6fa7d5b9ffb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a21e2fcfff04fbb9b3abf9a2d961058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52ae4a9e32af41b29e59456f833383a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2cb610903fe4f84841a7d1b3aaaace4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f00f544cec849c68f546a755bba6d54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12a795766e0f4a4ca6eb53c8c5674a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/FT_V2TXT_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeI_D57yiIm2",
        "outputId": "a8e3aab6-1be8-49df-c8d6-16205c6441c7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/540.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDACJdh_h9ik",
        "outputId": "f714452d-d751-4b11-d767-999362809df6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. INSTALL REQUIRED DEPENDENCIES\n",
        "!pip install -q mistral-common"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYKdvJt-lwlX",
        "outputId": "2d6d7ab0-9aab-4160-9b25-93605dbe9c00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/6.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m245.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m138.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/74.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/8.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m298.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m160.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers -q\n",
        "!pip install -U bitsandbytes>=0.46.1 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcWkcwAzjLXu",
        "outputId": "4ba60337-6f7c-4311-bf86-50ea5d27df97"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/10.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.0/10.4 MB\u001b[0m \u001b[31m211.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m235.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m141.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "NVPVS_6uqTTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original duration: 936.28s for barackobama2004dncARXE.mp3\n",
        "\n",
        "Original duration: 210.68s for barackobamatransitionaddress1.mp3\n",
        "\n",
        "Original duration: 183.02s for brad_pitt_sag_2020.mp3\n",
        "\n",
        "Original duration: 2415.33s for mandela_davos_1999.mp3\n",
        "\n",
        "Original duration: 1801.37s for mark_carney_davos_2026.mp3\n",
        "\n",
        "Original duration: 2585.63s for mlk_mountaintop_1968.mp3"
      ],
      "metadata": {
        "id": "tUyqNe1rlGwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "IBaEDkpcu5fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SUPPRESS WARNINGS - Add this at the very top\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Some matrices hidden dimension is not a multiple of 64\")\n",
        "warnings.filterwarnings(\"ignore\", module=\"bitsandbytes\")\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import librosa\n",
        "from datasets import Dataset\n",
        "from dataclasses import dataclass\n",
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    AutoModelForSpeechSeq2Seq,\n",
        "    BitsAndBytesConfig,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import transformers.models.voxtral_realtime.modeling_voxtral_realtime as vox_mod\n",
        "\n",
        "from warnings import simplefilter\n",
        "simplefilter(action=\"ignore\", category=FutureWarning)\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "MODEL_ID = \"mistralai/Voxtral-Mini-4B-Realtime-2602\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/data/H2E_Challenge/Voxtral_FineTune\"\n",
        "AUDIT_PATH = \"/content/drive/MyDrive/data/H2E_Challenge/H2E_Final_Performance_Audit.csv\"\n",
        "TARGET_SR = 16000\n",
        "CHUNK_LENGTH_SEC = 30.0\n",
        "MAX_TEXT_LENGTH = 448\n",
        "\n",
        "# ==================== 1. DATASET PREPARATION ====================\n",
        "df = pd.read_csv(AUDIT_PATH)\n",
        "paths = [f\"/content/drive/MyDrive/data/{f}\" for f in df['File']]\n",
        "texts = df['Transcript'].astype(str).tolist()\n",
        "\n",
        "chunked_audios, chunked_texts = [], []\n",
        "for path, text in zip(paths, texts):\n",
        "    try:\n",
        "        array, _ = librosa.load(path, sr=TARGET_SR, mono=True)\n",
        "        max_samples = int(CHUNK_LENGTH_SEC * TARGET_SR)\n",
        "        if len(array) > max_samples: array = array[:max_samples]\n",
        "        chunked_audios.append({\"array\": array.astype(np.float32).tolist(), \"sampling_rate\": TARGET_SR})\n",
        "        chunked_texts.append(text)\n",
        "    except Exception as e: print(f\"‚úó Failed {path}: {e}\")\n",
        "\n",
        "dataset = Dataset.from_dict({\"audio\": chunked_audios, \"text\": chunked_texts})\n",
        "\n",
        "# ==================== 2. MODEL SETUP ====================\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"eager\"\n",
        ")\n",
        "\n",
        "# Get model dimensions\n",
        "lm_hidden_size = model.config.hidden_size\n",
        "print(f\"Language model hidden size: {lm_hidden_size}\")\n",
        "\n",
        "# Audio feature dimension for Voxtral\n",
        "audio_feature_dim = 1280\n",
        "print(f\"Audio feature dimension: {audio_feature_dim}\")\n",
        "print(f\"Setting up adapter: {audio_feature_dim} ‚Üí {lm_hidden_size}\")\n",
        "\n",
        "# Create adapter that projects audio features to match language model hidden size\n",
        "class AudioFeatureAdapter(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(in_dim, out_dim)\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "# Initialize adapter with correct dimensions\n",
        "model.audio_adapter = AudioFeatureAdapter(audio_feature_dim, lm_hidden_size).to(model.device).to(torch.bfloat16)\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    audio_arrays = []\n",
        "    for x in examples[\"audio\"]:\n",
        "        arr = np.array(x[\"array\"], dtype=np.float32)\n",
        "        audio_arrays.append(arr)\n",
        "\n",
        "    audio_inputs = processor.feature_extractor(\n",
        "        audio_arrays,\n",
        "        sampling_rate=TARGET_SR, return_tensors=\"np\", padding=True\n",
        "    )\n",
        "\n",
        "    input_features = torch.tensor(audio_inputs[\"input_features\"], dtype=torch.bfloat16)\n",
        "\n",
        "    text_inputs = processor.tokenizer(\n",
        "        examples[\"text\"], return_tensors=\"np\", padding=\"max_length\",\n",
        "        truncation=True, max_length=MAX_TEXT_LENGTH\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"input_features\": input_features,\n",
        "        \"input_ids\": torch.tensor(text_inputs[\"input_ids\"], dtype=torch.long),\n",
        "        \"labels\": torch.tensor(text_inputs[\"input_ids\"], dtype=torch.long),\n",
        "        \"attention_mask\": torch.tensor(text_inputs[\"attention_mask\"], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(preprocess_function, batched=True, batch_size=2, remove_columns=dataset.column_names)\n",
        "dataset.set_format(\"torch\")\n",
        "\n",
        "# Prepare for k-bit training but disable gradient checkpointing\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.gradient_checkpointing_disable()  # Explicitly disable gradient checkpointing\n",
        "\n",
        "model = get_peft_model(model, LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    task_type=\"CAUSAL_LM\"\n",
        "))\n",
        "\n",
        "# ==================== 3. SIMPLIFIED PATCHED FORWARD ====================\n",
        "def patched_forward(\n",
        "    self, input_ids=None, input_features=None, attention_mask=None, position_ids=None,\n",
        "    past_key_values=None, encoder_past_key_values=None, padding_cache=None,\n",
        "    inputs_embeds=None, encoder_inputs_embeds=None, labels=None, use_cache=None,\n",
        "    cache_position=None, logits_to_keep=0, num_delay_tokens=None, **kwargs\n",
        "):\n",
        "    # Get text embeddings\n",
        "    if inputs_embeds is None:\n",
        "        inputs_embeds = self.get_input_embeddings()(input_ids)\n",
        "\n",
        "    batch_size = inputs_embeds.shape[0]\n",
        "    device = inputs_embeds.device\n",
        "\n",
        "    # Process audio if provided\n",
        "    if input_features is not None:\n",
        "        # Get audio features\n",
        "        audio_outputs = self.audio_tower(\n",
        "            input_features.to(device, dtype=torch.bfloat16),\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # Project audio features\n",
        "        audio_features = audio_outputs.last_hidden_state\n",
        "        audio_projected = self.audio_adapter(audio_features)\n",
        "\n",
        "        # Simple concatenation\n",
        "        inputs_embeds = torch.cat([audio_projected, inputs_embeds], dim=1)\n",
        "\n",
        "        # Update attention mask\n",
        "        if attention_mask is not None:\n",
        "            audio_mask = torch.ones(batch_size, audio_projected.shape[1],\n",
        "                                   device=device, dtype=attention_mask.dtype)\n",
        "            attention_mask = torch.cat([audio_mask, attention_mask], dim=1)\n",
        "\n",
        "    # Time conditioning - FIXED: properly handle dimensions\n",
        "    if num_delay_tokens is None:\n",
        "        num_delay_tokens = self.config.default_num_delay_tokens\n",
        "\n",
        "    # Create time tensor with correct shape for t_cond\n",
        "    time_tensor = torch.full((batch_size,), num_delay_tokens, device=device, dtype=torch.long)\n",
        "    t_cond = self.time_embedding(time_tensor)  # Shape: [batch_size, hidden_size]\n",
        "\n",
        "    # Call language model\n",
        "    lm_outputs = self.language_model(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        attention_mask=attention_mask,\n",
        "        position_ids=position_ids,\n",
        "        past_key_values=past_key_values,\n",
        "        use_cache=False,  # Disable cache to avoid issues\n",
        "        t_cond=t_cond,  # Pass t_cond directly\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    logits = lm_outputs.logits\n",
        "    loss = None\n",
        "\n",
        "    if labels is not None:\n",
        "        # Create labels that ignore audio tokens\n",
        "        if input_features is not None:\n",
        "            audio_len = audio_projected.shape[1]\n",
        "            new_labels = torch.full((batch_size, audio_len + labels.shape[1]), -100,\n",
        "                                   device=device, dtype=labels.dtype)\n",
        "            new_labels[:, audio_len:] = labels\n",
        "            labels = new_labels\n",
        "\n",
        "        # Shift for next token prediction\n",
        "        shift_logits = logits[..., :-1, :].contiguous()\n",
        "        shift_labels = labels[..., 1:].contiguous()\n",
        "        loss = nn.functional.cross_entropy(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)),\n",
        "            shift_labels.view(-1),\n",
        "            ignore_index=-100\n",
        "        )\n",
        "\n",
        "    return vox_mod.VoxtralRealtimeCausalLMOutputWithPast(\n",
        "        loss=loss,\n",
        "        logits=logits,\n",
        "        past_key_values=lm_outputs.past_key_values\n",
        "    )\n",
        "\n",
        "# Apply the patch\n",
        "vox_mod.VoxtralRealtimeForConditionalGeneration.forward = patched_forward\n",
        "\n",
        "# ==================== 4. TRAINING ====================\n",
        "@dataclass\n",
        "class SimpleCollator:\n",
        "    def __call__(self, features):\n",
        "        batch = {}\n",
        "        for key in [\"input_features\", \"input_ids\", \"labels\", \"attention_mask\"]:\n",
        "            if key in features[0]:\n",
        "                tensors = [f[key] for f in features]\n",
        "                if key == \"input_features\":\n",
        "                    batch[key] = torch.stack(tensors).to(torch.bfloat16)\n",
        "                else:\n",
        "                    batch[key] = torch.stack(tensors)\n",
        "        return batch\n",
        "\n",
        "# Disable all caching and checkpointing in training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-4,\n",
        "    max_steps=500,\n",
        "    bf16=True,\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        "    remove_unused_columns=False,\n",
        "    save_strategy=\"no\",\n",
        "    dataloader_drop_last=False,\n",
        "    gradient_checkpointing=False,  # Explicitly disable\n",
        "    use_cache=False,  # Disable cache\n",
        "    ddp_find_unused_parameters=False,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=SimpleCollator(),\n",
        ")\n",
        "\n",
        "print(\"üöÄ Starting Training with Fixed Forward Pass...\")\n",
        "print(f\"Model device: {model.device}\")\n",
        "print(f\"Hidden size: {lm_hidden_size}\")\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üíæ Saving fine-tuned model...\")\n",
        "\n",
        "# Create the adapter directory\n",
        "final_adapter_path = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
        "os.makedirs(final_adapter_path, exist_ok=True)\n",
        "\n",
        "# Save the model and processor\n",
        "model.save_pretrained(final_adapter_path)\n",
        "processor.save_pretrained(final_adapter_path)\n",
        "\n",
        "print(f\"‚úÖ Model saved to: {final_adapter_path}\")\n",
        "\n",
        "# Verify the save\n",
        "print(\"\\nüìÅ Saved files:\")\n",
        "if os.path.exists(final_adapter_path):\n",
        "    for file in os.listdir(final_adapter_path):\n",
        "        file_path = os.path.join(final_adapter_path, file)\n",
        "        size = os.path.getsize(file_path) / 1024  # Size in KB\n",
        "        print(f\"  - {file} ({size:.2f} KB)\")\n",
        "else:\n",
        "    print(\"‚ùå Save failed - directory not found!\")\n",
        "\n",
        "print(\"\\nüéâ Training complete! You can now use the model for inference.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306,
          "referenced_widgets": [
            "275618762775440f8dafce8f2bb346b8",
            "d89d0ec608f34b2990c4a9088e9b3fd9",
            "434c81650c884522af1b40d55a7c5282",
            "2266141f974642c79cc86dc455e00289",
            "4746e5c8cb1d451ba3fb70c973ddeac6",
            "a535e29dd2db4e7286842b33cedba999",
            "7bddeb4111c14a6f8a3e715b4fa29515",
            "e684547fd474448cbd0d8aeb7336f2de",
            "57adc4afe89b4d8e96541e282a6119fb",
            "53b7ca4bc00f4c58a9f2064d2d8ef722",
            "e16c59ee3b5c4a6b8ec2eb105f823357",
            "be724795132a4961adcca7563242f3ba",
            "972ce361b02d4236b4c49d11c543345d",
            "b0aabf7ff1624072a869b137fab45812",
            "dba47a8b31ba48ea919c65d7f264cabe",
            "31597a4feabc4e798993e3d7a37e3842",
            "0a02db8e56a84cd180e8b6fa7d5b9ffb",
            "7a21e2fcfff04fbb9b3abf9a2d961058",
            "52ae4a9e32af41b29e59456f833383a4",
            "d2cb610903fe4f84841a7d1b3aaaace4",
            "6f00f544cec849c68f546a755bba6d54",
            "12a795766e0f4a4ca6eb53c8c5674a3a"
          ]
        },
        "id": "eI0VZRf40guU",
        "outputId": "f3d3fa04-8fd6-4da4-efa4-b25c22ccb8f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/711 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "275618762775440f8dafce8f2bb346b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language model hidden size: 3072\n",
            "Audio feature dimension: 1280\n",
            "Setting up adapter: 1280 ‚Üí 3072\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be724795132a4961adcca7563242f3ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Training with Fixed Forward Pass...\n",
            "Model device: cuda:0\n",
            "Hidden size: 3072\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='23' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 23/500 00:49 < 18:37, 0.43 it/s, Epoch 11/250]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>41.992667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>27.299902</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## INFERENCE-CODE"
      ],
      "metadata": {
        "id": "SZxVRLhJX3Yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inference_fixed.py - Corrected version with proper function ordering\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import librosa\n",
        "import numpy as np\n",
        "from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "from peft import PeftModel\n",
        "import os\n",
        "\n",
        "# ==================== CONFIG ====================\n",
        "MODEL_ID = \"mistralai/Voxtral-Mini-4B-Realtime-2602\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/data/H2E_Challenge/Voxtral_FineTune\"\n",
        "ADAPTER_PATH = os.path.join(OUTPUT_DIR, \"final_adapter\")\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "TARGET_SR = 16000\n",
        "\n",
        "# ==================== LOAD MODEL ====================\n",
        "print(\"üöÄ Loading Voxtral Model...\")\n",
        "print(f\"Adapter path: {ADAPTER_PATH}\")\n",
        "print(f\"Device: {DEVICE}\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Load processor\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
        "\n",
        "# Load base model\n",
        "print(\"\\nüì¶ Loading base model...\")\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load your fine-tuned adapter\n",
        "print(f\"\\nüîß Loading fine-tuned adapter from: {ADAPTER_PATH}\")\n",
        "model = PeftModel.from_pretrained(model, ADAPTER_PATH)\n",
        "model.eval()\n",
        "print(\"‚úì Adapter loaded successfully!\")\n",
        "\n",
        "# ==================== AUDIO ADAPTER DEFINITION ====================\n",
        "class AudioFeatureAdapter(nn.Module):\n",
        "    def __init__(self, in_dim=1280, out_dim=3072):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(in_dim, out_dim)\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "# Add adapter to model\n",
        "if not hasattr(model, 'audio_adapter'):\n",
        "    model.audio_adapter = AudioFeatureAdapter().to(model.device).to(torch.bfloat16)\n",
        "    print(\"‚úì Audio adapter added to model\")\n",
        "\n",
        "# ==================== FIXED PATCHED FORWARD FOR INFERENCE ====================\n",
        "import transformers.models.voxtral_realtime.modeling_voxtral_realtime as vox_mod\n",
        "\n",
        "# First, save the original prepare_inputs_for_generation method\n",
        "original_prepare_inputs = vox_mod.VoxtralRealtimeForConditionalGeneration.prepare_inputs_for_generation\n",
        "\n",
        "# Define patched functions BEFORE applying them\n",
        "def patched_prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
        "    # If we have input_features, we don't need input_ids\n",
        "    if kwargs.get('input_features') is not None:\n",
        "        # Create a dummy input_ids to satisfy the method signature\n",
        "        # but it won't be used because we'll override with inputs_embeds\n",
        "        dummy_input_ids = torch.ones((input_ids.shape[0], 1), dtype=torch.long, device=input_ids.device)\n",
        "        kwargs['inputs_embeds'] = None  # Will be created in forward\n",
        "        return {\n",
        "            'input_ids': dummy_input_ids,\n",
        "            'input_features': kwargs.get('input_features'),\n",
        "            'attention_mask': kwargs.get('attention_mask'),\n",
        "            'use_cache': kwargs.get('use_cache', True),\n",
        "        }\n",
        "    return original_prepare_inputs(self, input_ids, **kwargs)\n",
        "\n",
        "def patched_forward(\n",
        "    self, input_ids=None, input_features=None, attention_mask=None, position_ids=None,\n",
        "    past_key_values=None, encoder_past_key_values=None, padding_cache=None,\n",
        "    inputs_embeds=None, encoder_inputs_embeds=None, labels=None, use_cache=None,\n",
        "    cache_position=None, logits_to_keep=0, num_delay_tokens=None, **kwargs\n",
        "):\n",
        "    # Process audio features\n",
        "    if input_features is not None:\n",
        "        # Get audio features from audio_tower\n",
        "        audio_outputs = self.audio_tower(\n",
        "            input_features.to(self.device, dtype=torch.bfloat16),\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # Project audio features\n",
        "        audio_features = audio_outputs.last_hidden_state\n",
        "        inputs_embeds = self.audio_adapter(audio_features)\n",
        "\n",
        "        # Create attention mask for audio\n",
        "        batch_size = inputs_embeds.shape[0]\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(batch_size, inputs_embeds.shape[1],\n",
        "                                       device=self.device, dtype=torch.long)\n",
        "    else:\n",
        "        # Fallback to text-only mode\n",
        "        if inputs_embeds is None and input_ids is not None:\n",
        "            inputs_embeds = self.get_input_embeddings()(input_ids)\n",
        "\n",
        "    # Time conditioning\n",
        "    if num_delay_tokens is None:\n",
        "        num_delay_tokens = self.config.default_num_delay_tokens\n",
        "\n",
        "    batch_size = inputs_embeds.shape[0] if inputs_embeds is not None else input_ids.shape[0]\n",
        "    time_tensor = torch.full((batch_size,), num_delay_tokens, device=self.device, dtype=torch.long)\n",
        "    t_cond = self.time_embedding(time_tensor)\n",
        "\n",
        "    # Call language model with inputs_embeds\n",
        "    lm_outputs = self.language_model(\n",
        "        inputs_embeds=inputs_embeds,\n",
        "        attention_mask=attention_mask,\n",
        "        position_ids=position_ids,\n",
        "        past_key_values=past_key_values,\n",
        "        use_cache=use_cache if use_cache is not None else True,\n",
        "        t_cond=t_cond,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "    logits = lm_outputs.logits\n",
        "\n",
        "    return vox_mod.VoxtralRealtimeCausalLMOutputWithPast(\n",
        "        loss=None,\n",
        "        logits=logits,\n",
        "        past_key_values=lm_outputs.past_key_values\n",
        "    )\n",
        "\n",
        "# Apply the patches\n",
        "vox_mod.VoxtralRealtimeForConditionalGeneration.forward = patched_forward\n",
        "vox_mod.VoxtralRealtimeForConditionalGeneration.prepare_inputs_for_generation = patched_prepare_inputs\n",
        "print(\"‚úì Forward patches applied\")\n",
        "\n",
        "# ==================== INFERENCE FUNCTIONS ====================\n",
        "\n",
        "def transcribe_audio(audio_path, max_new_tokens=200):\n",
        "    \"\"\"\n",
        "    Transcribe a single audio file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"\\nüìÇ Processing: {os.path.basename(audio_path)}\")\n",
        "\n",
        "        # Load audio\n",
        "        audio, _ = librosa.load(audio_path, sr=TARGET_SR, mono=True)\n",
        "\n",
        "        # Prepare inputs - only audio features, no text\n",
        "        inputs = processor(\n",
        "            audio,\n",
        "            sampling_rate=TARGET_SR,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Move to device\n",
        "        input_features = inputs[\"input_features\"].to(DEVICE).to(torch.bfloat16)\n",
        "\n",
        "        # Generate - only pass input_features\n",
        "        print(\"‚öôÔ∏è Generating transcription...\")\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                input_features=input_features,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                num_beams=1,\n",
        "            )\n",
        "\n",
        "        # Decode\n",
        "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        # Print first 100 chars or full if shorter\n",
        "        preview = transcription[:100] + \"...\" if len(transcription) > 100 else transcription\n",
        "        print(f\"üìù Transcription: {preview}\")\n",
        "        return transcription\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def save_transcription(transcription, audio_path):\n",
        "    \"\"\"\n",
        "    Save transcription to a text file\n",
        "    \"\"\"\n",
        "    transcripts_dir = os.path.join(OUTPUT_DIR, \"transcripts\")\n",
        "    os.makedirs(transcripts_dir, exist_ok=True)\n",
        "\n",
        "    base_name = os.path.basename(audio_path).split('.')[0]\n",
        "    output_path = os.path.join(transcripts_dir, f\"{base_name}_transcript.txt\")\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        f.write(transcription)\n",
        "\n",
        "    print(f\"üíæ Saved to: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "def batch_transcribe(audio_files):\n",
        "    \"\"\"\n",
        "    Transcribe multiple audio files\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    successful = 0\n",
        "    failed = 0\n",
        "\n",
        "    for i, audio_path in enumerate(audio_files):\n",
        "        print(f\"\\n[{i+1}/{len(audio_files)}] \", end=\"\")\n",
        "        transcription = transcribe_audio(audio_path)\n",
        "        if transcription:\n",
        "            results[audio_path] = transcription\n",
        "            save_transcription(transcription, audio_path)\n",
        "            successful += 1\n",
        "        else:\n",
        "            failed += 1\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    print(f\"\\n‚úÖ Complete: {successful} successful, {failed} failed\")\n",
        "    return results\n",
        "\n",
        "# ==================== TEST THE MODEL ====================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéØ Testing the model on your audio files\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Get all audio files\n",
        "data_dir = \"/content/drive/MyDrive/data\"\n",
        "if os.path.exists(data_dir):\n",
        "    audio_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)\n",
        "                  if f.endswith(('.mp3', '.wav', '.m4a', '.flac'))]\n",
        "\n",
        "    if audio_files:\n",
        "        print(f\"\\nFound {len(audio_files)} audio files\")\n",
        "\n",
        "        # Test first file only first to verify it works\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"üî¨ Testing first file to verify fix...\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        test_file = audio_files[0]\n",
        "        print(f\"Test file: {os.path.basename(test_file)}\")\n",
        "        transcription = transcribe_audio(test_file)\n",
        "\n",
        "        if transcription:\n",
        "            print(\"\\n‚úÖ First file succeeded! Saving result...\")\n",
        "            save_transcription(transcription, test_file)\n",
        "\n",
        "            # Now ask user what to do\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"Select option:\")\n",
        "            print(\"1. Transcribe all remaining files\")\n",
        "            print(\"2. Transcribe a specific file\")\n",
        "            print(\"3. Exit\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            choice = input(\"\\nEnter choice (1-3): \").strip()\n",
        "\n",
        "            if choice == \"1\":\n",
        "                print(\"\\nüîÑ Transcribing remaining files...\")\n",
        "                if len(audio_files) > 1:\n",
        "                    batch_transcribe(audio_files[1:])\n",
        "                else:\n",
        "                    print(\"No remaining files to transcribe.\")\n",
        "\n",
        "            elif choice == \"2\":\n",
        "                print(\"\\nSelect file number:\")\n",
        "                for i, file in enumerate(audio_files):\n",
        "                    status = \"‚úì\" if i == 0 else \" \"\n",
        "                    print(f\"  {status} {i+1}. {os.path.basename(file)}\")\n",
        "\n",
        "                try:\n",
        "                    file_num = int(input(\"\\nEnter file number: \")) - 1\n",
        "                    if 0 <= file_num < len(audio_files):\n",
        "                        if file_num == 0:\n",
        "                            print(\"File 1 already transcribed. Transcribing again...\")\n",
        "                        transcription = transcribe_audio(audio_files[file_num])\n",
        "                        if transcription:\n",
        "                            save = input(\"\\nSave transcription? (y/n): \").lower()\n",
        "                            if save == 'y':\n",
        "                                save_transcription(transcription, audio_files[file_num])\n",
        "                    else:\n",
        "                        print(\"‚ùå Invalid file number\")\n",
        "                except ValueError:\n",
        "                    print(\"‚ùå Please enter a valid number\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå First file failed. Error details above.\")\n",
        "\n",
        "            # Offer to try a different file\n",
        "            print(\"\\nTry a different file? (Enter file number or 'n' to exit)\")\n",
        "            for i, file in enumerate(audio_files):\n",
        "                print(f\"  {i+1}. {os.path.basename(file)}\")\n",
        "\n",
        "            try:\n",
        "                file_num = int(input(\"\\nEnter file number: \")) - 1\n",
        "                if 0 <= file_num < len(audio_files):\n",
        "                    transcription = transcribe_audio(audio_files[file_num])\n",
        "                    if transcription:\n",
        "                        save = input(\"\\nSave transcription? (y/n): \").lower()\n",
        "                        if save == 'y':\n",
        "                            save_transcription(transcription, audio_files[file_num])\n",
        "            except ValueError:\n",
        "                print(\"Exiting...\")\n",
        "    else:\n",
        "        print(f\"No audio files found in {data_dir}\")\n",
        "else:\n",
        "    print(f\"Data directory not found: {data_dir}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úÖ Inference complete!\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ==================== SIMPLE FUNCTION FOR QUICK USE ====================\n",
        "def quick_transcribe(audio_path):\n",
        "    \"\"\"\n",
        "    Quick one-liner transcription function\n",
        "    Usage: text = quick_transcribe('path/to/audio.mp3')\n",
        "    \"\"\"\n",
        "    return transcribe_audio(audio_path)\n",
        "\n",
        "print(\"\\nüìå Quick transcription function available:\")\n",
        "print(\"   text = quick_transcribe('path/to/audio.mp3')\")\n",
        "print(\"\\nüìÇ Transcripts will be saved to: {}/transcripts/\".format(OUTPUT_DIR))"
      ],
      "metadata": {
        "id": "89gqiD6HX6tR"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}