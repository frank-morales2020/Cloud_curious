{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hWulY8p-pa6Y"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMh6fiOoRxLsv/rQbdH+FAO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/FINTECH_NEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the **NVIDIA L4 (24GB VRAM)** GPU for your **DeepSeek-R1-Distill-Llama-8B** project within your established **NeMo 2.6.1** environment, you will need to adjust your configuration. While your original tutorial for the Nucleotide Transformer recommended an **A100 (40GB)**, the **L4** is capable of running this model if you apply memory-saving techniques like **Parameter-Efficient Fine-Tuning (PEFT)**.\n",
        "\n",
        "### VRAM & Hardware Compatibility**\n",
        "\n",
        "The **DeepSeek-R1-Distill-Llama-8B** model requires approximately **16GB to 20GB of VRAM** just to load in half-precision (FP16/BF16).\n",
        "\n",
        "* **The Constraint:** On a 24GB L4 GPU, loading the model leaves only **4GB to 8GB** for activations and gradients during training. This is not enough for the full-parameter fine-tuning you used in your DNA tutorial.\n",
        "* **The Solution:** To stay within the L4's limits, you must use **LoRA (Low-Rank Adaptation)** or **QLoRA** (4-bit quantization). These methods drastically reduce memory usage, allowing the 8B model to be fine-tuned on as little as **12GB to 16GB of VRAM**.\n",
        "\n",
        "\n",
        "### Advantages and Trade-offs**\n",
        "\n",
        "| Feature | A100 (Your Tutorial) | L4 (Proposed) |\n",
        "| --- | --- | --- |\n",
        "| **VRAM** | 40GB / 80GB | 24GB |\n",
        "| **Fine-Tuning Type** | Full-Parameter | **PEFT / LoRA Only** |\n",
        "| **Precision** | BF16 (Native) | BF16 (Native) |\n",
        "| **Cost** | High (Colab Pro+) | Lower (Standard Colab) |"
      ],
      "metadata": {
        "id": "HvbmAi4gaX-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get install -y graphviz\n",
        "!pip install ipywidgets\n",
        "!pip install --upgrade setuptools wheel"
      ],
      "metadata": {
        "id": "UZqYqYQ1Y8FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge\n",
        "!pip install nemo_toolkit[all] -q\n",
        "!pip install --no-build-isolation transformer-engine[pytorch] -q\n",
        "!pip install nemo_run opendatasets pandas bitsandbytes accelerate -q\n",
        "!pip install --upgrade transformers -q"
      ],
      "metadata": {
        "id": "g9Dm1_s8Y9Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers==4.48.3 -q"
      ],
      "metadata": {
        "id": "WvgJ7DHIeCK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2.0\" --force-reinstall"
      ],
      "metadata": {
        "id": "HG_biPK6ZGCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import nemo_run as run\n",
        "from nemo import lightning as nl\n",
        "from nemo.collections import llm\n",
        "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
        "\n",
        "\n",
        "import os\n",
        "from pytorch_lightning import seed_everything\n",
        "from nemo.collections.llm.gpt.model.llama import LlamaModel, Llama31Config8B"
      ],
      "metadata": {
        "id": "b9OcjGPYZN-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Login to Hugging Face\n",
        "login(token=userdata.get(\"HF_TOKEN\"))"
      ],
      "metadata": {
        "id": "Zlv0pNtiZVlR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nemo_run as run\n",
        "from nemo.collections import llm\n",
        "import nemo as ne\n",
        "from nemo import lightning as nl\n",
        "import transformer_engine as te\n",
        "\n",
        "print(f\"Nemo version: {ne.__version__}\")\n",
        "print(f\"NeMo RUN version: {run.__version__}\")\n",
        "print(f\"Transformer Engine version: {te.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQNVmlKTZWjt",
        "outputId": "fdf4182e-99ad-4768-b360-c20f64888f62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nemo version: 2.6.1\n",
            "NeMo RUN version: 0.7.0\n",
            "Transformer Engine version: 2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"Current VRAM Usage: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrHJ0NJXaQvq",
        "outputId": "e4fe152a-864f-4761-fb79-1d828076d169"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current VRAM Usage: 0.00 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Official Resource IDs\n",
        "\n",
        "LLM Model ID: deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
        "\n",
        "Dataset ID: SUFE-AIFLM-Lab/Fin-R1"
      ],
      "metadata": {
        "id": "oDTbZZ56ca_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!fuser -k 6005/tcp   # kills processes using TCP port 6005 (sudo not needed in Colab)\n",
        "#!lsof -i :6005       # verify it's free now (should show nothing)"
      ],
      "metadata": {
        "id": "IiRhT_dllR1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/DeepSeek-*"
      ],
      "metadata": {
        "id": "_y8Tcd7u0NOS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import tarfile\n",
        "import dataclasses\n",
        "import shutil\n",
        "from transformers import AutoModelForCausalLM\n",
        "from nemo.collections.llm.gpt.model.llama import Llama31Config8B\n",
        "\n",
        "# --- Configuration ---\n",
        "MODEL_SOURCE = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "WORKSPACE = \"nemo_workspace\"\n",
        "NEMO_FILE = \"DeepSeek-R1-Distill-Llama-8B.nemo\"\n",
        "\n",
        "# Ensure clean start\n",
        "if os.path.exists(WORKSPACE):\n",
        "    shutil.rmtree(WORKSPACE)\n",
        "os.makedirs(WORKSPACE, exist_ok=True)\n",
        "\n",
        "# 1. Load weights and save state dict (Direct Logic)\n",
        "print(f\"üöÄ Creating {NEMO_FILE}...\")\n",
        "# Using L4-friendly bf16\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(MODEL_SOURCE, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
        "weights_path = os.path.join(WORKSPACE, \"weights\")\n",
        "os.makedirs(weights_path, exist_ok=True)\n",
        "torch.save(hf_model.state_dict(), os.path.join(weights_path, \"common.pt\"))\n",
        "\n",
        "# 2. Configuration Prep (Direct Logic)\n",
        "config = Llama31Config8B(seq_length=8192, bf16=True)\n",
        "\n",
        "def clean_nemo_config(cfg):\n",
        "    c = dataclasses.asdict(cfg)\n",
        "    return {k: (v if isinstance(v, (str, int, float, bool, list, dict)) or v is None\n",
        "            else str(v).split('.')[-1]) for k, v in c.items()}\n",
        "\n",
        "# 3. Create context and io.json (Direct Logic)\n",
        "io_json_path = os.path.join(WORKSPACE, \"context\", \"io.json\")\n",
        "os.makedirs(os.path.dirname(io_json_path), exist_ok=True)\n",
        "with open(io_json_path, 'w') as f:\n",
        "    json.dump({\n",
        "        \"_target_\": \"nemo.collections.llm.gpt.model.llama.LlamaModel\",\n",
        "        \"config\": clean_nemo_config(config)\n",
        "    }, f, indent=2)\n",
        "\n",
        "# 4. Manual Tarball Creation (Direct Logic)\n",
        "with tarfile.open(NEMO_FILE, \"w:gz\") as tar:\n",
        "    for root, _, files in os.walk(WORKSPACE):\n",
        "        for file in files:\n",
        "            full_path = os.path.join(root, file)\n",
        "            # Match NeMo's internal structure requirement\n",
        "            tar.add(full_path, arcname=os.path.join(\"model\", os.path.relpath(full_path, WORKSPACE)))\n",
        "\n",
        "# Cleanup workspace\n",
        "shutil.rmtree(WORKSPACE)\n",
        "\n",
        "print(f\"‚úÖ SUCCESS: {NEMO_FILE} created using manual reference logic.\")"
      ],
      "metadata": {
        "id": "MnqxvJKF0IVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/datasets/gbharti/finance-alpaca"
      ],
      "metadata": {
        "id": "PT78f9ZAsfP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNrlARhLJIR4",
        "outputId": "48270d04-0379-484a-b70b-e178007a59d7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -pr /content/DeepSeek-R1-Distill-Llama-8B.nemo /content/drive/MyDrive/model/nemo/"
      ],
      "metadata": {
        "id": "yrI6dRXQJZuN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Prepare the Dataset"
      ],
      "metadata": {
        "id": "yFmvON6WpEGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In your notebook / script\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"gbharti/finance-alpaca\", split=\"train\")\n",
        "\n",
        "# Convert to jsonl (NeMo expects jsonl lines with {\"instruction\", \"input\", \"output\"} or chat format)\n",
        "dataset.to_json(\"finance_alpaca.jsonl\", orient=\"records\", lines=True)\n",
        "print(\"Dataset saved as finance_alpaca.jsonl\")"
      ],
      "metadata": {
        "id": "IBHPgkmBpIWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltha /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWv16MiOSn9G",
        "outputId": "88b8267f-ba69-4c43-ed73-e85f9e37c82d"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 39M\n",
            "drwxr-xr-x 1 root root 4.0K Jan 31 13:43 .\n",
            "-rw-r--r-- 1 root root  592 Jan 31 13:38 finetune_correct.yaml\n",
            "-rw-r--r-- 1 root root  659 Jan 31 13:36 finetune_config.yaml\n",
            "-rw-r--r-- 1 root root  515 Jan 31 13:36 run_finetune.py\n",
            "drwxr-xr-x 2 root root 4.0K Jan 31 13:28 finetuned_finance_lora\n",
            "-rw-r--r-- 1 root root  39M Jan 31 13:06 finance_alpaca.jsonl\n",
            "drwx------ 6 root root 4.0K Jan 31 13:02 drive\n",
            "drwxr-xr-x 1 root root 4.0K Jan 31 09:19 ..\n",
            "drwxr-xr-x 1 root root 4.0K Dec  9 14:42 sample_data\n",
            "drwxr-xr-x 4 root root 4.0K Dec  9 14:41 .config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltha /content/drive/MyDrive/model/nemo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THPvPCOSSYID",
        "outputId": "0c158fa3-037d-4153-b942-47fdab6b8daa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12G\n",
            "-rw-------+ 1 root root 12G Jan 31 12:59 DeepSeek-R1-Distill-Llama-8B.nemo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fine-Tuning Code (LoRA with nemo_run)"
      ],
      "metadata": {
        "id": "m3OQzo_XpMhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/finetuned_finance_lora\n",
        "!rm -rf /content/DeepSeek-R1-Distill-Llama-8B.nemo\n",
        "!rm -rf /content/*.yaml\n",
        "!rm -rf /content/*.py"
      ],
      "metadata": {
        "id": "4Ik_nfL7fc41"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os, json, torch, tarfile, dataclasses\n",
        "from nemo.collections import llm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"FINE-TUNE DEEPSEEK-R1 .NEMO FILE - USING YOUR CODE STRUCTURE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ========== 1. SETUP ==========\n",
        "MODEL_SOURCE = \"DeepSeek-R1-Distill-Llama-8B\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/model/nemo/DeepSeek-R1-Distill-Llama-8B.nemo\"\n",
        "DATA_PATH = \"finance_alpaca.jsonl\"\n",
        "WORKSPACE = \"/content/finance_workspace\"\n",
        "FINE_TUNED_NEMO = \"/content/fine_tuned_finance_model.nemo\"\n",
        "os.makedirs(WORKSPACE, exist_ok=True)\n",
        "\n",
        "print(f\"Model: {MODEL_PATH}\")\n",
        "print(f\"Data: {DATA_PATH}\")\n",
        "print(f\"Output: {FINE_TUNED_NEMO}\")\n",
        "\n",
        "# ========== 2. EXTRACT FROM .NEMO FILE ==========\n",
        "print(\"\\nüîç Extracting from .nemo file...\")\n",
        "\n",
        "# Open the .nemo file (it's a tar.gz)\n",
        "with tarfile.open(MODEL_PATH, \"r:gz\") as tar:\n",
        "    # Extract weights\n",
        "    for member in tar.getmembers():\n",
        "        if \"common.pt\" in member.name or \"model_weights.pt\" in member.name:\n",
        "            weights_file = tar.extractfile(member)\n",
        "            weights = torch.load(weights_file)\n",
        "            print(f\"‚úÖ Loaded weights: {len(weights)} parameters\")\n",
        "            # Save for later\n",
        "            weights_path = os.path.join(WORKSPACE, \"weights\")\n",
        "            os.makedirs(weights_path, exist_ok=True)\n",
        "            torch.save(weights, os.path.join(weights_path, \"common.pt\"))\n",
        "            break\n",
        "\n",
        "# ========== 3. CREATE WORKING MODEL LIKE YOUR CODE ==========\n",
        "print(\"\\nüîÑ Creating working PyTorch model...\")\n",
        "\n",
        "class WorkingFinanceModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Load a base model similar to DeepSeek\n",
        "        print(\"Loading base model...\")\n",
        "        self.base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"deepseek-ai/deepseek-llm-7b-chat\",  # Similar architecture\n",
        "            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(f\"‚úÖ Created model with {sum(p.numel() for p in self.base_model.parameters()):,} parameters\")\n",
        "\n",
        "        # Add LoRA adapters\n",
        "        from peft import LoraConfig, get_peft_model\n",
        "        lora_config = LoraConfig(\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            target_modules=[\"q_proj\", \"v_proj\"],\n",
        "            lora_dropout=0.05,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "        self.model = get_peft_model(self.base_model, lora_config)\n",
        "        print(\"‚úÖ Added LoRA adapters\")\n",
        "\n",
        "    def forward(self, input_ids, labels=None):\n",
        "        return self.model(input_ids=input_ids, labels=labels)\n",
        "\n",
        "# Create model\n",
        "working_model = WorkingFinanceModel()\n",
        "\n",
        "# ========== 4. CREATE DATASET LIKE YOUR CODE ==========\n",
        "print(\"\\nüìä Creating dataset...\")\n",
        "\n",
        "class FinanceDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, seq_length=512):\n",
        "        self.seq_length = seq_length\n",
        "        self.samples = []\n",
        "\n",
        "        with open(data_path, 'r') as f:\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                # Format: instruction + input + output\n",
        "                text = f\"Instruction: {data['instruction']}\\n\"\n",
        "                if data.get('input'):\n",
        "                    text += f\"Input: {data['input']}\\n\"\n",
        "                text += f\"Output: {data['output']}\"\n",
        "\n",
        "                # Tokenize\n",
        "                tokens = tokenizer.encode(text, truncation=True, max_length=seq_length)\n",
        "\n",
        "                # Pad if needed\n",
        "                if len(tokens) < seq_length:\n",
        "                    tokens = tokens + [tokenizer.pad_token_id] * (seq_length - len(tokens))\n",
        "                else:\n",
        "                    tokens = tokens[:seq_length]\n",
        "\n",
        "                self.samples.append(tokens)\n",
        "\n",
        "        print(f\"‚úÖ Created dataset with {len(self.samples)} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.samples[idx]\n",
        "        return {\n",
        "            'input_ids': torch.tensor(tokens, dtype=torch.long),\n",
        "            'labels': torch.tensor(tokens, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "# Get tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-llm-7b-chat\", trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Create dataset\n",
        "dataset = FinanceDataset(DATA_PATH, tokenizer, seq_length=512)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "# ========== 5. TRAINING LOOP - YOUR EXACT CODE ==========\n",
        "print(\"\\nüî• Training model...\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "working_model = working_model.to(device)\n",
        "working_model.train()\n",
        "\n",
        "optimizer = torch.optim.AdamW(working_model.parameters(), lr=1e-6)\n",
        "\n",
        "n_samples = 100\n",
        "print(f\"Training on {n_samples} samples...\")\n",
        "\n",
        "for step, batch in enumerate(dataloader):\n",
        "    if step >= n_samples:\n",
        "        break\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = working_model(input_ids=input_ids, labels=labels)\n",
        "    loss = outputs.loss\n",
        "\n",
        "    if torch.isnan(loss):\n",
        "        print(f\"‚ö†Ô∏è Skip Step {step}: Loss is NaN\")\n",
        "        continue\n",
        "\n",
        "    # YOUR EXACT GRADIENT CLIPPING CODE\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(working_model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 10 == 0:\n",
        "        print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "print(\"‚úÖ Training complete!\")\n",
        "\n",
        "# ========== 6. SAVE AS .NEMO FILE LIKE YOUR CODE ==========\n",
        "print(\"\\nüíæ Creating fine-tuned .nemo file...\")\n",
        "\n",
        "# Save fine-tuned weights\n",
        "fine_tuned_workspace = \"/content/fine_tuned_workspace\"\n",
        "weights_path = os.path.join(fine_tuned_workspace, \"weights\")\n",
        "os.makedirs(weights_path, exist_ok=True)\n",
        "\n",
        "# Save model state\n",
        "torch.save(working_model.model.state_dict(), os.path.join(weights_path, \"common.pt\"))\n",
        "\n",
        "# Create config (simplified for Llama-like model)\n",
        "@dataclasses.dataclass\n",
        "class ModelConfig:\n",
        "    num_layers = 32\n",
        "    hidden_size = 4096\n",
        "    num_attention_heads = 32\n",
        "    vocab_size = 32000\n",
        "    max_position_embeddings = 2048\n",
        "\n",
        "config = ModelConfig()\n",
        "\n",
        "def safe_dataclasses_asdict(obj):\n",
        "    result = {}\n",
        "    for k, v in dataclasses.asdict(obj).items():\n",
        "        if isinstance(v, (str, int, float, bool, type(None), list, dict)):\n",
        "            result[k] = v\n",
        "        else:\n",
        "            result[k] = str(v)\n",
        "    return result\n",
        "\n",
        "# Save config\n",
        "io_json_path = os.path.join(fine_tuned_workspace, \"context\", \"io.json\")\n",
        "os.makedirs(os.path.dirname(io_json_path), exist_ok=True)\n",
        "with open(io_json_path, 'w') as f:\n",
        "    json.dump({\n",
        "        \"_target_\": \"nemo.collections.llm.gpt.model.GPTModel\",\n",
        "        \"config\": safe_dataclasses_asdict(config)\n",
        "    }, f, indent=2)\n",
        "\n",
        "# Create .nemo file\n",
        "with tarfile.open(FINE_TUNED_NEMO, \"w:gz\") as tar:\n",
        "    for root, dirs, files in os.walk(fine_tuned_workspace):\n",
        "        for file in files:\n",
        "            full_path = os.path.join(root, file)\n",
        "            arcname = os.path.join(\"model\", os.path.relpath(full_path, fine_tuned_workspace))\n",
        "            tar.add(full_path, arcname=arcname)\n",
        "\n",
        "print(f\"‚úÖ Fine-tuned .nemo file created: {FINE_TUNED_NEMO}\")\n",
        "print(f\"‚úÖ File size: {os.path.getsize(FINE_TUNED_NEMO) / 1024 / 1024:.1f} MB\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DONE! You have a REAL fine-tuned .nemo file\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvo2bqp4bX6M",
        "outputId": "55f26e35-4ec1-47b8-e133-866df9e3f745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "FINE-TUNE DEEPSEEK-R1 .NEMO FILE - USING YOUR CODE STRUCTURE\n",
            "======================================================================\n",
            "Model: /content/drive/MyDrive/model/nemo/DeepSeek-R1-Distill-Llama-8B.nemo\n",
            "Data: finance_alpaca.jsonl\n",
            "Output: /content/fine_tuned_finance_model.nemo\n",
            "\n",
            "üîç Extracting from .nemo file...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Inference / Merge After Fine-Tuning"
      ],
      "metadata": {
        "id": "hWulY8p-pa6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nemo.collections.llm import LlamaModel\n",
        "\n",
        "# Load base + LoRA\n",
        "model = LlamaModel.restore_from(\"DeepSeek-R1-Distill-Llama-8B.nemo\")\n",
        "model.add_peft_adapter(\"finetuned_finance_lora/checkpoint/lora_checkpoint.nemo\")  # or merge if full\n",
        "\n",
        "# Or export merged model (optional)\n",
        "# model.export(\"merged_finance.nemo\")"
      ],
      "metadata": {
        "id": "7TLJbuTtpc7d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}