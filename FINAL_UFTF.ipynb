{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNn/EA09XhM4gk82d6+6E7T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ddc54396f8254ecc8f5d2469bcfa94d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d1f5855dfde4b688fd36d531ceb1621",
              "IPY_MODEL_fa56d18a19cc4871a6484cfcc5e41902",
              "IPY_MODEL_5cc81324ea644955a93e154b59fe8bd6"
            ],
            "layout": "IPY_MODEL_7200c008c0974ead9b0554d9cb1e2821"
          }
        },
        "3d1f5855dfde4b688fd36d531ceb1621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aebbb63b24744fe9bb69ac716bc79231",
            "placeholder": "​",
            "style": "IPY_MODEL_9992b29134c64f9b91690a81c571235b",
            "value": "Map: 100%"
          }
        },
        "fa56d18a19cc4871a6484cfcc5e41902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f510b282e2c4cfd80d0750b8b3673e7",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14b713e7eb9d46a1a01b08d9102cd73c",
            "value": 100
          }
        },
        "5cc81324ea644955a93e154b59fe8bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_324ab11ddbc6428aa9499537e4424fac",
            "placeholder": "​",
            "style": "IPY_MODEL_035d3e4befbf47d19d49ee76500ebf60",
            "value": " 100/100 [00:00&lt;00:00, 2837.32 examples/s]"
          }
        },
        "7200c008c0974ead9b0554d9cb1e2821": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aebbb63b24744fe9bb69ac716bc79231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9992b29134c64f9b91690a81c571235b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f510b282e2c4cfd80d0750b8b3673e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14b713e7eb9d46a1a01b08d9102cd73c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "324ab11ddbc6428aa9499537e4424fac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "035d3e4befbf47d19d49ee76500ebf60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c36cea7e632749e7bd413974fdf6516f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_143193f260af4bf781ea7d457bc9c516",
              "IPY_MODEL_f8a4aeb41b8147a29bee05fba66ba1cc",
              "IPY_MODEL_0a63a52928f44286967f6fefcb308ccd"
            ],
            "layout": "IPY_MODEL_fd490c640c5549ef813ef3ac19805447"
          }
        },
        "143193f260af4bf781ea7d457bc9c516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e0fbfc74bf0471aaf81757b8d854ee6",
            "placeholder": "​",
            "style": "IPY_MODEL_7a5b7bb6013e44098b46a2857c5c56c6",
            "value": "Map: 100%"
          }
        },
        "f8a4aeb41b8147a29bee05fba66ba1cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_51cb50b87e7842b591145b17aa1846cd",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12401a5586c040c99e2800ed71741c09",
            "value": 25
          }
        },
        "0a63a52928f44286967f6fefcb308ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e849ecdf63b64963bd448d4179df380f",
            "placeholder": "​",
            "style": "IPY_MODEL_69da1dea211d42bbbecf540658df9f85",
            "value": " 25/25 [00:00&lt;00:00, 1001.73 examples/s]"
          }
        },
        "fd490c640c5549ef813ef3ac19805447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e0fbfc74bf0471aaf81757b8d854ee6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a5b7bb6013e44098b46a2857c5c56c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51cb50b87e7842b591145b17aa1846cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12401a5586c040c99e2800ed71741c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e849ecdf63b64963bd448d4179df380f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69da1dea211d42bbbecf540658df9f85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1dccdd94eb64e579f3d359523bdf43b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0df347f8b8404fd2b63bc4546e1760db",
              "IPY_MODEL_51660be59ea54a19abbf8e11fc5687ed",
              "IPY_MODEL_08a34ab2b6364e709924137f99309b6a"
            ],
            "layout": "IPY_MODEL_325775bdb5d84c689f6d1282eff23173"
          }
        },
        "0df347f8b8404fd2b63bc4546e1760db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_980a5d1d30bb404d964b7de802fe22c8",
            "placeholder": "​",
            "style": "IPY_MODEL_44df5d018ca94b1aa37da6d4713e722a",
            "value": "model.safetensors:  79%"
          }
        },
        "51660be59ea54a19abbf8e11fc5687ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76e09508de8347b1b301acbe9e03db0a",
            "max": 4138270819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4377588be604b3785c864adcb527e98",
            "value": 3282042880
          }
        },
        "08a34ab2b6364e709924137f99309b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f302a389816c44ce8967d902cdbc1049",
            "placeholder": "​",
            "style": "IPY_MODEL_4237f29ccedd46f681af5dc3dff09e7e",
            "value": " 3.27G/4.14G [01:17&lt;00:20, 43.0MB/s]"
          }
        },
        "325775bdb5d84c689f6d1282eff23173": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "980a5d1d30bb404d964b7de802fe22c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44df5d018ca94b1aa37da6d4713e722a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76e09508de8347b1b301acbe9e03db0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4377588be604b3785c864adcb527e98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f302a389816c44ce8967d902cdbc1049": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4237f29ccedd46f681af5dc3dff09e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/FINAL_UFTF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# Install necessary modules\n",
        "!pip install transformers accelerate trl bitsandbytes datasets peft --quiet\n",
        "!pip install -U bitsandbytes -q\n",
        "!pip install unsloth --quiet\n",
        "!pip install wandb --quiet\n",
        "\n",
        "#!nvidia-smi\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import warnings\n",
        "import gc\n",
        "from transformers import (\n",
        "    TrainingArguments,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoModelForCausalLM,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import Trainer\n",
        "import accelerate\n",
        "from trl import DPOTrainer\n",
        "import copy\n",
        "\n",
        "# Import from Unsloth\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from unsloth.kernels import cross_entropy_loss\n",
        "\n",
        "# Initialize the Accelerator\n",
        "accelerator = accelerate.Accelerator()\n",
        "\n",
        "# Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
        "#os.environ[\"WANDB_MODE\"] = \"offline\"\n",
        "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Environment variable num_items_in_batch not found.\")\n",
        "\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clears GPU memory and performs garbage collection.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "\n",
        "class FineTuningAgent:\n",
        "    \"\"\"\n",
        "    A class for fine-tuning language models using the OODA loop.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_id, dataset_name, config=None):\n",
        "        \"\"\"\n",
        "        Initializes the FineTuningAgent.\n",
        "\n",
        "        Args:\n",
        "            model_id (str): The ID of the pre-trained model.\n",
        "            dataset_name (str): The name of the dataset to use.\n",
        "            config (dict, optional): Configuration parameters. Defaults to None.\n",
        "        \"\"\"\n",
        "        self.model_id = model_id\n",
        "        self.dataset_name = dataset_name\n",
        "        self.config = config if config is not None else {}\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.trainer = None\n",
        "        self.training_args = None\n",
        "        self.peft_config = None\n",
        "        self.dataset = None\n",
        "        self.counter = 0\n",
        "        self.data_collator = None\n",
        "        self.model_type = None\n",
        "        print(\"Agent dictionary(inside __init__):\")\n",
        "        print(self.config)\n",
        "\n",
        "    def _observe(self):\n",
        "        \"\"\"\n",
        "        Loads the model, tokenizer, and dataset.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"Starting Observe ...\")\n",
        "\n",
        "        clear_memory()\n",
        "\n",
        "        # Check if Unsloth should be used.\n",
        "        use_unsloth = self.config.get(\"use_unsloth\", False)\n",
        "\n",
        "        if use_unsloth:\n",
        "            print(\"Unsloth will be used.\")\n",
        "\n",
        "        quantization_config = None\n",
        "        if self.config.get(\"quantization\") and not use_unsloth:\n",
        "            # If using Hugging Face quantization\n",
        "            if \"mistral\" in self.model_id.lower():\n",
        "                print(\"Mistral model detected. Using 4-bit quantization.\")\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                )\n",
        "            else:\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_use_double_quant=False,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.float32,\n",
        "                )\n",
        "\n",
        "        model_downloaded = False\n",
        "        max_retries = 3\n",
        "        retry_count = 0\n",
        "        while not model_downloaded and retry_count < max_retries:\n",
        "            try:\n",
        "                # Determine the correct model class based on architecture\n",
        "                if \"bert\" in self.model_id.lower():\n",
        "                    print(\"BERT model detected.\")\n",
        "                    self.model_type = \"encoder-only\"\n",
        "                    if use_unsloth:\n",
        "                        # Load the model with unsloth\n",
        "                        print(\"Loading BERT with Unsloth\")\n",
        "                        # This is the correct model ID to use with Unsloth\n",
        "                        # Corrected Model ID.\n",
        "                        unsloth_model_id = self.config.get(\n",
        "                            \"unsloth_model_id\", \"bert-base-uncased\"\n",
        "                        )\n",
        "                        max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "                        dtype = self.config.get(\"dtype\", None)\n",
        "                        load_in_4bit = self.config.get(\"load_in_4bit\", True)\n",
        "                        access_token = self.config.get(\"access_token\", None)\n",
        "                        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                            model_name=unsloth_model_id,\n",
        "                            max_seq_length=max_seq_length,\n",
        "                            dtype=dtype,\n",
        "                            load_in_4bit=load_in_4bit,\n",
        "                            token=access_token,\n",
        "                        )\n",
        "                    else:\n",
        "                        # Load the model with Hugging Face\n",
        "                        print(\"Loading BERT with Hugging Face\")\n",
        "                        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                            self.model_id,\n",
        "                            num_labels=2,\n",
        "                            quantization_config=quantization_config,\n",
        "                            trust_remote_code=True,\n",
        "                        )\n",
        "                        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                            self.model_id, trust_remote_code=True\n",
        "                        )\n",
        "\n",
        "                elif \"mistral\" in self.model_id.lower() or \"deepseek\" in self.model_id.lower():\n",
        "                    print(\"Decoder-only model detected.\")\n",
        "                    self.model_type = \"decoder-only\"\n",
        "                    if use_unsloth:\n",
        "                        # Load the model with unsloth\n",
        "                        print(\"Loading Decoder-only with Unsloth\")\n",
        "                        unsloth_model_id = self.config.get(\n",
        "                            \"unsloth_model_id\", \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
        "                        )\n",
        "                        max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "                        dtype = self.config.get(\"dtype\", None)\n",
        "                        load_in_4bit = self.config.get(\"load_in_4bit\", True)\n",
        "                        access_token = self.config.get(\"access_token\", None)\n",
        "                        self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                            model_name=unsloth_model_id,\n",
        "                            max_seq_length=max_seq_length,\n",
        "                            dtype=dtype,\n",
        "                            load_in_4bit=load_in_4bit,\n",
        "                            token=access_token,\n",
        "                        )\n",
        "                    else:\n",
        "                        # Load the model with Hugging Face\n",
        "                        print(\"Loading Decoder-only with Hugging Face\")\n",
        "                        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                            self.model_id,\n",
        "                            quantization_config=quantization_config,\n",
        "                            trust_remote_code=True,\n",
        "                        )\n",
        "                        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                            self.model_id, trust_remote_code=True\n",
        "                        )\n",
        "                # unsloth model\n",
        "                elif \"unsloth\" in self.model_id.lower():\n",
        "                    print(\"Unsloth model detected.\")\n",
        "                    # Load the model with unsloth\n",
        "                    print(\"Loading Unsloth model\")\n",
        "                    # Correct model name: unsloth/mistral-7b-instruct-v0.3-bnb-4bit\n",
        "                    unsloth_model_id = self.config.get(\n",
        "                        \"unsloth_model_id\", \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "                    )\n",
        "                    max_seq_length = self.config.get(\"max_seq_length\", 2048)\n",
        "                    dtype = self.config.get(\"dtype\", None)\n",
        "                    load_in_4bit = self.config.get(\"load_in_4bit\", True)\n",
        "                    access_token = self.config.get(\"access_token\", None)\n",
        "                    self.model, self.tokenizer = FastLanguageModel.from_pretrained(\n",
        "                        model_name=unsloth_model_id,\n",
        "                        max_seq_length=max_seq_length,\n",
        "                        dtype=dtype,\n",
        "                        load_in_4bit=load_in_4bit,\n",
        "                        token=access_token,\n",
        "                    )\n",
        "                    self.model_type = \"decoder-only\"\n",
        "                else:\n",
        "                    print(f\"Model {self.model_id} not supported.\")\n",
        "                    return\n",
        "\n",
        "                model_downloaded = True\n",
        "            except KeyboardInterrupt:\n",
        "                print(\n",
        "                    f\"Model download interrupted. Retrying... (Attempt {retry_count + 1}/{max_retries})\"\n",
        "                )\n",
        "                retry_count += 1\n",
        "                # Clear GPU memory to avoid potential issues\n",
        "                clear_memory()\n",
        "                if retry_count == max_retries:\n",
        "                    print(\"Max retry reached, skipping model download.\")\n",
        "                    return\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during model download: {e}\")\n",
        "                retry_count += 1\n",
        "                # Clear GPU memory to avoid potential issues\n",
        "                clear_memory()\n",
        "\n",
        "                if retry_count == max_retries:\n",
        "                    print(\"Max retry reached, skipping model download.\")\n",
        "                    return\n",
        "        # Add padding token if it does not exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        if not use_unsloth and not \"unsloth\" in self.model_id.lower():\n",
        "            # Move model to device\n",
        "            self.model.to(self.device)\n",
        "\n",
        "        # Load Dataset (using dataset name from Hugging Face Hub)\n",
        "        dataset = load_dataset(\n",
        "            self.dataset_name, split=\"train\", num_proc=self.config.get(\"dataset_num_proc\", 2)\n",
        "        )\n",
        "        self.dataset = dataset.shuffle().select(\n",
        "            range(self.config.get(\"dataset_size\", 125))\n",
        "        )\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Observe finished.\")\n",
        "\n",
        "    def _orient(self):\n",
        "        \"\"\"\n",
        "        Orients the agent by formatting the dataset and preparing training arguments.\n",
        "        \"\"\"\n",
        "        print(\"\\n\")\n",
        "        self.counter += 1\n",
        "        print(\"Starting Orient ...\")\n",
        "        if self.dataset_name == \"SetFit/mrpc\":\n",
        "            print(\"Dataset: SetFit/mrpc\")\n",
        "            preprocessing_function = self._preprocess_function_mrpc\n",
        "        elif self.dataset_name == \"b-mc2/sql-create-context\":\n",
        "            print(\"Dataset: b-mc2/sql-create-context\")\n",
        "            preprocessing_function = self._preprocess_function_sql_create_context\n",
        "        elif self.dataset_name == \"anthropic/hh-rlhf\":\n",
        "            print(\"Dataset: anthropic/hh-rlhf\")\n",
        "            preprocessing_function = self._preprocess_function_anthropic_hh_rlhf\n",
        "        elif self.dataset_name == \"imdb\":\n",
        "            print(\"Dataset: imdb\")\n",
        "            preprocessing_function = self._preprocess_function_imdb\n",
        "        else:\n",
        "            print(f\"Dataset: {self.dataset_name} not supported.\")\n",
        "            return\n",
        "\n",
        "        # Set the train/test split.\n",
        "        test_size_percentage = self.config.get(\"test_split_percentage\", 0.2)\n",
        "        self.dataset = self.dataset.train_test_split(\n",
        "            test_size=test_size_percentage\n",
        "        )\n",
        "\n",
        "        self.dataset = self.dataset.map(\n",
        "            preprocessing_function,\n",
        "            batched=True,\n",
        "            remove_columns=self.dataset[\"train\"].column_names,\n",
        "        )\n",
        "\n",
        "        # 3. Prepare Training Arguments\n",
        "        # Import is_bfloat16_supported function.\n",
        "\n",
        "\n",
        "        # Create TrainingArguments with the desired parameters\n",
        "        training_args_config = self.config.get(\"training_args\", {})\n",
        "        self.training_args = TrainingArguments(\n",
        "            output_dir=training_args_config.get(\"output_dir\", \"./output\"),\n",
        "            per_device_train_batch_size=training_args_config.get(\n",
        "                \"per_device_train_batch_size\", 2\n",
        "            ),\n",
        "            gradient_accumulation_steps=training_args_config.get(\n",
        "                \"gradient_accumulation_steps\", 4\n",
        "            ),\n",
        "            warmup_steps=training_args_config.get(\"warmup_steps\", 5),\n",
        "            max_steps=training_args_config.get(\"max_steps\", 60),\n",
        "            learning_rate=training_args_config.get(\"learning_rate\", 2e-4),\n",
        "            fp16=training_args_config.get(\"fp16\", not is_bfloat16_supported()),\n",
        "            bf16=training_args_config.get(\"bf16\", is_bfloat16_supported()),\n",
        "            logging_steps=training_args_config.get(\"logging_steps\", 10),\n",
        "            optim=training_args_config.get(\"optim\", \"adamw_8bit\"),\n",
        "            weight_decay=training_args_config.get(\"weight_decay\", 0.01),\n",
        "            lr_scheduler_type=training_args_config.get(\"lr_scheduler_type\", \"linear\"),\n",
        "            seed=training_args_config.get(\"seed\", 3407),\n",
        "            evaluation_strategy=training_args_config.get(\n",
        "                \"evaluation_strategy\", \"steps\"\n",
        "            ),  # we need this\n",
        "            eval_steps=training_args_config.get(\"eval_steps\", 20),\n",
        "            save_strategy=training_args_config.get(\"save_strategy\", \"steps\"),\n",
        "            save_steps=training_args_config.get(\"save_steps\", 20),\n",
        "            report_to=training_args_config.get(\"report_to\", \"wandb\"),\n",
        "            remove_unused_columns=False # we need this\n",
        "        )\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Orient Dataset: {self.dataset}\")\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Orient finished.\")\n",
        "\n",
        "    def _decide(self):\n",
        "        \"\"\"\n",
        "        Decides on the fine-tuning strategy, including LoRA configuration.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(\"Starting Decide ...\")\n",
        "        clear_memory()\n",
        "        # PEFT Configuration (LoRA)\n",
        "        if self.config.get(\"lora\"):\n",
        "            self.model = prepare_model_for_kbit_training(self.model)\n",
        "            if \"bert\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=16,  # You can tune this.\n",
        "                    lora_dropout=0.1,  # You can tune this.\n",
        "                    r=64,  # You can tune this.\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"query\", \"key\", \"value\", \"dense\"],  # Correct target modules for BERT\n",
        "                    task_type=\"SEQ_CLS\",  # correct task type\n",
        "                )\n",
        "            elif \"mistral\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=128,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=256,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "            elif \"deepseek\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=128,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=256,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "            elif \"unsloth\" in self.model_id.lower():\n",
        "                peft_config = LoraConfig(\n",
        "                    lora_alpha=128,\n",
        "                    lora_dropout=0.05,\n",
        "                    r=256,\n",
        "                    bias=\"none\",\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    task_type=\"CAUSAL_LM\",\n",
        "                )\n",
        "                print(\"\\n\")\n",
        "                print(f\"LORA: {peft_config}\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Model {self.model_id} not supported.\")\n",
        "                return\n",
        "\n",
        "            self.peft_config = peft_config\n",
        "            self.model = get_peft_model(self.model, peft_config)\n",
        "\n",
        "            self.model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "        print('\\n')\n",
        "        print(\"Decide finished.\")\n",
        "\n",
        "    def _act(self):\n",
        "        \"\"\"\n",
        "        Acts by preprocessing the dataset and initializing the training loop.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(\"Starting Act ...\")\n",
        "        clear_memory()\n",
        "\n",
        "        try:\n",
        "            if \"train\" not in self.dataset or \"test\" not in self.dataset:\n",
        "                print(f\"Missing train or test split for {self.dataset_name}\")\n",
        "                return\n",
        "\n",
        "            print(\"Dataset preprocessed successfully.\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # Unsloth's Data Collator (Hypothetical)\n",
        "            if self.config.get(\"use_unsloth\", False) or \"unsloth\" in self.model_id.lower():\n",
        "                # Replace with actual Unsloth data collator creation if needed\n",
        "                # This is where we would add logic to use Unsloth's data collator\n",
        "                # if it exists.\n",
        "                # Example of a hypothetical Unsloth data collator\n",
        "                #self.data_collator = UnslothDataCollator()\n",
        "                print(\"Unsloth data collator used.\")\n",
        "                # Set collator\n",
        "                self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "            else:\n",
        "                # Hugging Face Data Collator\n",
        "                self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n",
        "                print(\"Hugging Face data collator used.\")\n",
        "\n",
        "            # Initialize Trainer\n",
        "            print(\"Initializing Trainer...\")\n",
        "            # Use the Trainer class instead of SFTTrainer\n",
        "            self.trainer = Trainer(\n",
        "                model=self.model,\n",
        "                args=self.training_args,\n",
        "                train_dataset=self.dataset[\"train\"],\n",
        "                eval_dataset=self.dataset[\"test\"],\n",
        "                data_collator=self.data_collator,\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred in _act(): {e}\")\n",
        "            raise\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(\"Act finished.\")\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Executes the OODA loop and fine-tunes the language model.\n",
        "        \"\"\"\n",
        "        self.counter += 1\n",
        "        print(\"\\n\")\n",
        "        print(\"Starting Run ...\")\n",
        "        clear_memory()\n",
        "        self._observe()\n",
        "        if self.model is None:\n",
        "            print(\"Model loading failed, skipping _orient, _decide and _act\")\n",
        "            return\n",
        "        self._orient()\n",
        "        self._decide()\n",
        "        self._act()\n",
        "\n",
        "        print(\"\\n\")\n",
        "        print(f\"Run Dataset: {self.dataset}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if self.trainer is not None:\n",
        "            try:\n",
        "                # Train the model\n",
        "                self.trainer.train()\n",
        "                print(\"\\n\")\n",
        "                print(\"Evaluation:\")\n",
        "                eval_results = self.evaluate()\n",
        "                print(\"\\n\")\n",
        "                print(eval_results)\n",
        "                print(\"\\n\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during training or evaluation: {e}\")\n",
        "                raise\n",
        "        else:\n",
        "            print(\"Trainer is None. Skipping training and evaluation.\")\n",
        "\n",
        "        print(\"Run  finished.\")\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Evaluates the fine-tuned language model.\n",
        "        \"\"\"\n",
        "        return self.trainer.evaluate()\n",
        "\n",
        "    def _preprocess_function_mrpc(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the SetFit/mrpc dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: SetFit/mrpc\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 128)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            inputs = self.tokenizer(\n",
        "                examples[\"text1\"],\n",
        "                examples[\"text2\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            inputs[\"labels\"] = examples[\"label\"]\n",
        "        elif self.model_type == \"decoder-only\":\n",
        "            # Mistral, DeepSeek, and other decoder-only models\n",
        "            # Adapt input format as needed for text generation tasks\n",
        "            # We need the code of the DeepSeek tokenizer and preprocessing\n",
        "            print(\"Decoder-only models not implemented yet in MRPC Dataset.\")\n",
        "            return\n",
        "            # inputs = self.tokenizer(...)  # Adapt for generation\n",
        "            # inputs[\"labels\"] = ...        # Adapt for generation\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def _preprocess_function_sql_create_context(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the b-mc2/sql-create-context dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: b-mc2/sql-create-context\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            # Mistral, DeepSeek, and other decoder-only models\n",
        "            # Tokenize inputs and labels\n",
        "            inputs = [f\"### Question: {q} ### Context: {c}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"answer\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Assign labels to model_inputs\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        elif self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            # Tokenize inputs and labels\n",
        "            inputs = [f\"### Question: {q} ### Context: {c}\" for q, c in zip(examples[\"question\"], examples[\"context\"])]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"answer\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Assign labels to model_inputs\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    def _preprocess_function_anthropic_hh_rlhf(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the anthropic/hh-rlhf dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: anthropic/hh-rlhf\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"decoder-only\":\n",
        "            # Mistral, DeepSeek, and other decoder-only models\n",
        "            inputs = examples[\"chosen\"]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"chosen\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        elif self.model_type == \"encoder-only\":\n",
        "            # BERT and other encoder-only models\n",
        "            inputs = examples[\"chosen\"]\n",
        "            model_inputs = self.tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            # Tokenize labels\n",
        "            labels_tokenized = self.tokenizer(examples[\"chosen\"], max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "            model_inputs[\"labels\"] = labels_tokenized[\"input_ids\"]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    def _preprocess_function_imdb(self, examples):\n",
        "        \"\"\"\n",
        "        Preprocesses the data for the imdb dataset.\n",
        "        Handles different model types and sequence lengths.\n",
        "        \"\"\"\n",
        "        print(\"Preprocess Dataset: imdb\")\n",
        "\n",
        "        max_length = self.config.get(\"max_length\", 1024)  # Get max_length from config\n",
        "\n",
        "        if self.model_type == \"encoder-only\":\n",
        "             # BERT and other encoder-only models\n",
        "            inputs = self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            inputs[\"labels\"] = examples[\"label\"]\n",
        "            return inputs\n",
        "        elif self.model_type == \"decoder-only\":\n",
        "            # Decoder-only models (Mistral, DeepSeek, etc.)\n",
        "            model_inputs = self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                max_length=max_length,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            # Copy input_ids to labels for causal LM training\n",
        "            model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "\n",
        "            return model_inputs\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "# %%\n",
        "\n",
        "#TODO\n",
        "\n",
        "#\"unsloth/mistral-7b-bnb-4bit\",\n",
        "#\"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "#\"unsloth/llama-2-7b-bnb-4bit\",\n",
        "#\"unsloth/llama-2-13b-bnb-4bit\",\n",
        "#\"unsloth/codellama-34b-bnb-4bit\",\n",
        "#\"unsloth/tinyllama-bnb-4bit\",\n",
        "#\"unsloth/gemma-7b-bnb-4bit\", # New Google 6 trillion tokens model 2.5x faster!\n",
        "#\"unsloth/gemma-2b-bnb-4bit\",\n",
        "#    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "#    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "#    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "#    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "#    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "#    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "#    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "#    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "#    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
        "#    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "#    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "#    \"unsloth/gemma-2-27b-bnb-4bit\",\n",
        "\n",
        "\n",
        "\n",
        "RL_PAIRSNO = {'experiment_name': 'google-bert_bert-base-uncased_SetFit_mrpc_base_config',\n",
        " 'model_id': 'google-bert/bert-base-uncased',\n",
        " 'dataset_name': 'SetFit/mrpc',\n",
        " 'dataset_size': 125,\n",
        " 'test_split_percentage': 0.2,\n",
        " 'quantization': True,\n",
        " 'lora': True,\n",
        " 'use_unsloth': False,\n",
        " 'max_seq_length': 2048,\n",
        " 'dtype': None,\n",
        " 'load_in_4bit': True,\n",
        " 'output_dir': './google-bert_bert-base-uncased_SetFit_mrpc_base_config_output',\n",
        " 'per_device_train_batch_size': 1,\n",
        " 'gradient_accumulation_steps': 2,\n",
        " 'report_to': 'none',\n",
        " 'gradient_checkpointing': True,\n",
        " 'optim': 'adamw_torch_fused',\n",
        " 'logging_steps': 5,\n",
        " 'save_strategy': 'epoch',\n",
        " 'learning_rate': 0.0002,\n",
        " 'bf16': True,\n",
        " 'tf32': True,\n",
        " 'max_grad_norm': 0.3,\n",
        " 'warmup_ratio': 0.03,\n",
        " 'lr_scheduler_type': 'constant',\n",
        " 'num_train_epochs': 1}\n",
        "\n",
        "# %%\n",
        "# Configuration for experiments\n",
        "RL_PAIRSSI = [\n",
        "        {\n",
        "    \"model_id\": \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
        "        \"dataset_name\": \"imdb\",\n",
        "        \"config\": {\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"use_unsloth\": True,  # Indicate that Unsloth should be used for this model\n",
        "            \"max_seq_length\": 2048,\n",
        "            \"dtype\": None,\n",
        "            \"load_in_4bit\": True,\n",
        "            \"training_args\": {\n",
        "                \"report_to\": \"none\",\n",
        "                \"output_dir\": \"./imdb_output\",\n",
        "                \"per_device_train_batch_size\": 1,\n",
        "                \"gradient_accumulation_steps\": 2,\n",
        "                \"report_to\": \"none\",\n",
        "                \"gradient_checkpointing\": True,\n",
        "                \"optim\": \"adamw_torch_fused\",\n",
        "                \"logging_steps\": 5,\n",
        "                \"report_to\": \"none\",\n",
        "                \"save_strategy\": \"epoch\",\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"bf16\": True,\n",
        "                \"tf32\": True,\n",
        "                \"max_grad_norm\": 0.3,\n",
        "                \"warmup_ratio\": 0.03,\n",
        "                \"lr_scheduler_type\": \"constant\",\n",
        "                \"num_train_epochs\":1,\n",
        "                \"remove_unused_columns\": False,\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "\n",
        "    {\n",
        "        \"model_id\": \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "        \"dataset_name\": \"b-mc2/sql-create-context\",\n",
        "        \"config\": {\n",
        "            \"unsloth_model_id\": \"deepseek-ai/deepseek-coder-1.3b-base\",  # Corrected model ID for Unsloth\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"use_unsloth\": True,\n",
        "            \"max_seq_length\": 2048,\n",
        "            \"dtype\": None,\n",
        "            \"load_in_4bit\": True,\n",
        "            \"access_token\": None,  # No needed a token\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"training_args\": {\n",
        "                \"report_to\": \"none\",\n",
        "                \"output_dir\": \"./unsloth_sql_create_context_output\",\n",
        "                \"per_device_train_batch_size\": 2,\n",
        "                \"gradient_accumulation_steps\": 4,\n",
        "                \"report_to\": \"none\",\n",
        "                \"gradient_checkpointing\": True,\n",
        "                \"optim\": \"adamw_8bit\",\n",
        "                \"logging_steps\": 10,\n",
        "                \"save_strategy\": \"epoch\",\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"bf16\": True,\n",
        "                \"fp16\": False,\n",
        "                \"max_grad_norm\": 0.3,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"lr_scheduler_type\": \"linear\",\n",
        "                \"num_train_epochs\": 1,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"max_steps\": 60,\n",
        "                \"seed\": 3407,\n",
        "                \"evaluation_strategy\": \"steps\", # We need this\n",
        "                \"eval_steps\": 20, # We need this\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "\n",
        "\n",
        "    {\n",
        "        \"model_id\": \"google-bert/bert-base-uncased\",  # Corrected model ID\n",
        "        \"dataset_name\": \"SetFit/mrpc\",\n",
        "        \"config\": {\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"training_args\": {\n",
        "                \"report_to\": \"none\",\n",
        "                \"output_dir\": \"./mrpc_output\",\n",
        "                \"per_device_train_batch_size\": 1,  # reduce batch size\n",
        "                \"gradient_accumulation_steps\": 2,\n",
        "                \"report_to\": \"wandb\",\n",
        "                \"gradient_checkpointing\": True,\n",
        "                \"optim\": \"adamw_torch_fused\",\n",
        "                \"logging_steps\": 5,\n",
        "                \"save_strategy\": \"epoch\",\n",
        "                \"report_to\": \"none\",\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"bf16\": True,\n",
        "                \"tf32\": True,\n",
        "                \"max_grad_norm\": 0.3,\n",
        "                \"warmup_ratio\": 0.03,\n",
        "                \"lr_scheduler_type\": \"constant\",\n",
        "                \"num_train_epochs\": 1, # Set the correct epoch\n",
        "                \"remove_unused_columns\": False,\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"model_id\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "        \"dataset_name\": \"b-mc2/sql-create-context\",\n",
        "        \"config\": {\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"training_args\": {\n",
        "                \"report_to\": \"none\",\n",
        "                \"output_dir\": \"./sql_create_context_output\",\n",
        "                \"per_device_train_batch_size\": 1,\n",
        "                \"gradient_accumulation_steps\": 2,\n",
        "                \"report_to\": \"none\",\n",
        "                \"gradient_checkpointing\": True,\n",
        "                \"optim\": \"adamw_torch_fused\",\n",
        "                \"logging_steps\": 5,\n",
        "                \"save_strategy\": \"epoch\",\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"bf16\": True,\n",
        "                \"tf32\": True,\n",
        "                \"max_grad_norm\": 0.3,\n",
        "                \"warmup_ratio\": 0.03,\n",
        "                \"lr_scheduler_type\": \"constant\",\n",
        "                \"num_train_epochs\":1, # Set the correct epoch\n",
        "                \"remove_unused_columns\": False,\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"model_id\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "        \"dataset_name\": \"anthropic/hh-rlhf\",\n",
        "        \"config\": {\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"training_args\": {\n",
        "                \"report_to\": \"none\",\n",
        "                \"output_dir\": \"./hh_rlhf_output\",\n",
        "                \"per_device_train_batch_size\": 1,\n",
        "                \"gradient_accumulation_steps\": 2,\n",
        "                \"gradient_checkpointing\": True,\n",
        "                \"optim\": \"adamw_torch_fused\",\n",
        "                \"logging_steps\": 5,\n",
        "                \"report_to\": None,\n",
        "                \"save_strategy\": \"epoch\",\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"bf16\": True,\n",
        "                \"tf32\": True,\n",
        "                \"max_grad_norm\": 0.3,\n",
        "                \"warmup_ratio\": 0.03,\n",
        "                \"lr_scheduler_type\": \"constant\",\n",
        "                \"num_train_epochs\":1, # Set the correct epoch\n",
        "                \"remove_unused_columns\": False,\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"model_id\": \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "        \"dataset_name\": \"b-mc2/sql-create-context\",\n",
        "        \"config\": {\n",
        "            \"unsloth_model_id\": \"deepseek-ai/deepseek-coder-1.3b-base\",  # Corrected model ID for Unsloth\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"use_unsloth\": True,\n",
        "            \"max_seq_length\": 2048,\n",
        "            \"dtype\": None,\n",
        "            \"load_in_4bit\": True,\n",
        "            \"access_token\": None,  # No needed a token\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"training_args\": {\n",
        "                \"report_to\": \"none\",\n",
        "                \"output_dir\": \"./unsloth_sql_create_context_output\",\n",
        "                \"per_device_train_batch_size\": 2,\n",
        "                \"gradient_accumulation_steps\": 4,\n",
        "                \"gradient_checkpointing\": True,\n",
        "                \"optim\": \"adamw_8bit\",\n",
        "                \"logging_steps\": 10,\n",
        "                \"save_strategy\": \"epoch\",\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"bf16\": True,\n",
        "                \"fp16\": False,\n",
        "                \"max_grad_norm\": 0.3,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"lr_scheduler_type\": \"linear\",\n",
        "                \"num_train_epochs\": 1,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"max_steps\": 60,\n",
        "                \"seed\": 3407,\n",
        "                \"evaluation_strategy\": \"steps\", # We need this\n",
        "                \"eval_steps\": 20, # We need this\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "RL_PAIRS2 = [\n",
        "  {\n",
        "        \"model_id\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "        \"dataset_name\": \"b-mc2/sql-create-context\",\n",
        "        \"config\": {\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./sql_create_context_output\",\n",
        "                \"per_device_train_batch_size\": 1,\n",
        "                \"gradient_accumulation_steps\": 2,\n",
        "                \"report_to\": \"wandb\",\n",
        "                \"gradient_checkpointing\": True,\n",
        "                \"optim\": \"adamw_torch_fused\",\n",
        "                \"logging_steps\": 5,\n",
        "                \"report_to\": \"none\",\n",
        "                \"save_strategy\": \"epoch\",\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"bf16\": True,\n",
        "                \"tf32\": True,\n",
        "                \"max_grad_norm\": 0.3,\n",
        "                \"warmup_ratio\": 0.03,\n",
        "                \"lr_scheduler_type\": \"constant\",\n",
        "                \"num_train_epochs\":1, # Set the correct epoch\n",
        "                \"remove_unused_columns\": False,\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"model_id\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "        \"dataset_name\": \"anthropic/hh-rlhf\",\n",
        "        \"config\": {\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./hh_rlhf_output\",\n",
        "                \"per_device_train_batch_size\": 1,\n",
        "                \"gradient_accumulation_steps\": 2,\n",
        "                \"report_to\": \"wandb\",\n",
        "                \"gradient_checkpointing\": True,\n",
        "                \"optim\": \"adamw_torch_fused\",\n",
        "                \"logging_steps\": 5,\n",
        "                \"report_to\": \"none\",\n",
        "                \"save_strategy\": \"epoch\",\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"bf16\": True,\n",
        "                \"tf32\": True,\n",
        "                \"max_grad_norm\": 0.3,\n",
        "                \"warmup_ratio\": 0.03,\n",
        "                \"lr_scheduler_type\": \"constant\",\n",
        "                \"num_train_epochs\":1, # Set the correct epoch\n",
        "                \"remove_unused_columns\": False,\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "    {\n",
        "        \"model_id\": \"unsloth/DeepSeek-R1-Distill-Llama-8B\",\n",
        "        \"dataset_name\": \"b-mc2/sql-create-context\",\n",
        "        \"config\": {\n",
        "            \"unsloth_model_id\": \"deepseek-ai/deepseek-coder-1.3b-base\",  # Corrected model ID for Unsloth\n",
        "            \"dataset_size\": 125,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"quantization\": True,\n",
        "            \"lora\": True,\n",
        "            \"use_unsloth\": True,\n",
        "            \"max_seq_length\": 2048,\n",
        "            \"dtype\": None,\n",
        "            \"load_in_4bit\": True,\n",
        "            \"access_token\": None,  # No needed a token\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./unsloth_sql_create_context_output\",\n",
        "                \"per_device_train_batch_size\": 2,\n",
        "                \"gradient_accumulation_steps\": 4,\n",
        "                \"report_to\": \"none\",\n",
        "                \"gradient_checkpointing\": True,\n",
        "                \"optim\": \"adamw_8bit\",\n",
        "                \"report_to\": \"none\",\n",
        "                \"logging_steps\": 10,\n",
        "                \"save_strategy\": \"epoch\",\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"bf16\": True,\n",
        "                \"fp16\": False,\n",
        "                \"max_grad_norm\": 0.3,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"lr_scheduler_type\": \"linear\",\n",
        "                \"num_train_epochs\": 1,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"max_steps\": 60,\n",
        "                \"seed\": 3407,\n",
        "                \"evaluation_strategy\": \"steps\", # We need this\n",
        "                \"eval_steps\": 20, # We need this\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "# Import necessary modules\n",
        "import itertools\n",
        "import copy\n",
        "import gc\n",
        "import torch\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clears GPU memory and performs garbage collection.\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "def create_rl_pairs():\n",
        "    \"\"\"\n",
        "    Creates a list of all possible combinations of datasets, models,\n",
        "    and configurations for RL experiments.\n",
        "    \"\"\"\n",
        "\n",
        "    datasets = [\n",
        "        \"SetFit/mrpc\",\n",
        "        \"b-mc2/sql-create-context\",\n",
        "        \"anthropic/hh-rlhf\",\n",
        "        \"imdb\",\n",
        "    ]\n",
        "    models = [\n",
        "        \"bert-base-uncased\",\n",
        "        \"mistralai/Mistral-7B-v0.1\",\n",
        "        \"deepseek-ai/deepseek-coder-1.3b-base\",\n",
        "        \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    ]\n",
        "\n",
        "    # Define different configs\n",
        "    configs = [\n",
        "        {\n",
        "            \"max_length\": 128,\n",
        "            \"quantization\": True,\n",
        "            \"use_unsloth\": False,\n",
        "            \"lora\": True,\n",
        "            \"dataset_size\": 125,\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./output\",\n",
        "                \"per_device_train_batch_size\": 4,\n",
        "                \"gradient_accumulation_steps\": 4,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"max_steps\": 60,\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"logging_steps\": 10,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"eval_steps\": 20,\n",
        "                \"report_to\": \"none\",\n",
        "                \"save_steps\": 20,\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"max_length\": 1024,\n",
        "            \"quantization\": True,\n",
        "            \"use_unsloth\": True,\n",
        "            \"lora\": True,\n",
        "            \"dataset_size\": 125,\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./output\",\n",
        "                \"per_device_train_batch_size\": 4,\n",
        "                \"gradient_accumulation_steps\": 4,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"max_steps\": 60,\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"logging_steps\": 10,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"eval_steps\": 20,\n",
        "                \"save_steps\": 20,\n",
        "            },\n",
        "            \"unsloth_model_id\": \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "            \"max_seq_length\": 2048,\n",
        "            \"dtype\": None,\n",
        "            \"load_in_4bit\": True,\n",
        "        },\n",
        "        {\n",
        "            \"max_length\": 1024,\n",
        "            \"quantization\": True,\n",
        "            \"use_unsloth\": True,\n",
        "            \"lora\": True,\n",
        "            \"dataset_size\": 125,\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./output\",\n",
        "                \"per_device_train_batch_size\": 4,\n",
        "                \"gradient_accumulation_steps\": 4,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"max_steps\": 60,\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"logging_steps\": 10,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"eval_steps\": 20,\n",
        "                \"save_steps\": 20,\n",
        "            },\n",
        "            \"unsloth_model_id\": \"bert-base-uncased\",\n",
        "            \"max_seq_length\": 2048,\n",
        "            \"dtype\": None,\n",
        "            \"load_in_4bit\": True,\n",
        "        },\n",
        "        {\n",
        "            \"max_length\": 1024,\n",
        "            \"quantization\": True,\n",
        "            \"use_unsloth\": True,\n",
        "            \"lora\": True,\n",
        "            \"dataset_size\": 125,\n",
        "            \"dataset_num_proc\": 2,\n",
        "            \"test_split_percentage\": 0.2,\n",
        "            \"training_args\": {\n",
        "                \"output_dir\": \"./output\",\n",
        "                \"per_device_train_batch_size\": 4,\n",
        "                \"gradient_accumulation_steps\": 4,\n",
        "                \"warmup_steps\": 5,\n",
        "                \"max_steps\": 60,\n",
        "                \"learning_rate\": 2e-4,\n",
        "                \"logging_steps\": 10,\n",
        "                \"weight_decay\": 0.01,\n",
        "                \"eval_steps\": 20,\n",
        "                \"save_steps\": 20,\n",
        "            },\n",
        "            \"unsloth_model_id\": \"deepseek-ai/deepseek-coder-1.3b-base\",\n",
        "            \"max_seq_length\": 2048,\n",
        "            \"dtype\": None,\n",
        "            \"load_in_4bit\": True,\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    rl_pairs = []\n",
        "    for dataset, model, config in itertools.product(datasets, models, configs):\n",
        "        rl_pairs.append((dataset, model, copy.deepcopy(config))) # Use copy.deepcopy()\n",
        "\n",
        "    return rl_pairs\n",
        "\n",
        "\n",
        "# Create pairs\n",
        "rl_pairs = create_rl_pairs()\n",
        "\n",
        "# Run the experiment\n",
        "for dataset_name, model_id, config in rl_pairs:\n",
        "    clear_memory()\n",
        "    print(\"\\n\")\n",
        "    print(f\"Running experiment with:\")\n",
        "    print(f\"- Dataset: {dataset_name}\")\n",
        "    print(f\"- Model: {model_id}\")\n",
        "    print(f\"- Config: {config}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    try:\n",
        "        agent = FineTuningAgent(model_id, dataset_name, config)\n",
        "        agent.run()\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the experiment: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run the experiments\n",
        "for rl_pair in RL_PAIRS:\n",
        "    print(\"\\n\")\n",
        "\n",
        "    print(\n",
        "        f\"Running experiment with model: {rl_pair['model_id']} and dataset: {rl_pair['dataset_name']}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    agent = FineTuningAgent(\n",
        "        model_id=rl_pair[\"model_id\"],\n",
        "        dataset_name=rl_pair[\"dataset_name\"],\n",
        "        config=rl_pair[\"config\"],\n",
        "    )\n",
        "    # Initiate the OODA loop and fine-tuning process\n",
        "    agent.run()\n",
        "    print(\"\\n\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "2jNJgwulgSL-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ddc54396f8254ecc8f5d2469bcfa94d1",
            "3d1f5855dfde4b688fd36d531ceb1621",
            "fa56d18a19cc4871a6484cfcc5e41902",
            "5cc81324ea644955a93e154b59fe8bd6",
            "7200c008c0974ead9b0554d9cb1e2821",
            "aebbb63b24744fe9bb69ac716bc79231",
            "9992b29134c64f9b91690a81c571235b",
            "6f510b282e2c4cfd80d0750b8b3673e7",
            "14b713e7eb9d46a1a01b08d9102cd73c",
            "324ab11ddbc6428aa9499537e4424fac",
            "035d3e4befbf47d19d49ee76500ebf60",
            "c36cea7e632749e7bd413974fdf6516f",
            "143193f260af4bf781ea7d457bc9c516",
            "f8a4aeb41b8147a29bee05fba66ba1cc",
            "0a63a52928f44286967f6fefcb308ccd",
            "fd490c640c5549ef813ef3ac19805447",
            "1e0fbfc74bf0471aaf81757b8d854ee6",
            "7a5b7bb6013e44098b46a2857c5c56c6",
            "51cb50b87e7842b591145b17aa1846cd",
            "12401a5586c040c99e2800ed71741c09",
            "e849ecdf63b64963bd448d4179df380f",
            "69da1dea211d42bbbecf540658df9f85",
            "b1dccdd94eb64e579f3d359523bdf43b",
            "0df347f8b8404fd2b63bc4546e1760db",
            "51660be59ea54a19abbf8e11fc5687ed",
            "08a34ab2b6364e709924137f99309b6a",
            "325775bdb5d84c689f6d1282eff23173",
            "980a5d1d30bb404d964b7de802fe22c8",
            "44df5d018ca94b1aa37da6d4713e722a",
            "76e09508de8347b1b301acbe9e03db0a",
            "a4377588be604b3785c864adcb527e98",
            "f302a389816c44ce8967d902cdbc1049",
            "4237f29ccedd46f681af5dc3dff09e7e"
          ]
        },
        "outputId": "e9e33d43-9457-4a14-8ebb-674b5920cba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "\n",
            "\n",
            "Running experiment with:\n",
            "- Dataset: SetFit/mrpc\n",
            "- Model: bert-base-uncased\n",
            "- Config: {'max_length': 128, 'quantization': True, 'use_unsloth': False, 'lora': True, 'dataset_size': 125, 'dataset_num_proc': 2, 'test_split_percentage': 0.2, 'training_args': {'output_dir': './output', 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'warmup_steps': 5, 'max_steps': 60, 'learning_rate': 0.0002, 'logging_steps': 10, 'weight_decay': 0.01, 'eval_steps': 20, 'report_to': 'none', 'save_steps': 20}}\n",
            "\n",
            "\n",
            "Agent dictionary(inside __init__):\n",
            "{'max_length': 128, 'quantization': True, 'use_unsloth': False, 'lora': True, 'dataset_size': 125, 'dataset_num_proc': 2, 'test_split_percentage': 0.2, 'training_args': {'output_dir': './output', 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'warmup_steps': 5, 'max_steps': 60, 'learning_rate': 0.0002, 'logging_steps': 10, 'weight_decay': 0.01, 'eval_steps': 20, 'report_to': 'none', 'save_steps': 20}}\n",
            "\n",
            "\n",
            "Starting Run ...\n",
            "Starting Observe ...\n",
            "BERT model detected.\n",
            "Loading BERT with Hugging Face\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Observe finished.\n",
            "\n",
            "\n",
            "Starting Orient ...\n",
            "Dataset: SetFit/mrpc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ddc54396f8254ecc8f5d2469bcfa94d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: SetFit/mrpc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c36cea7e632749e7bd413974fdf6516f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocess Dataset: SetFit/mrpc\n",
            "\n",
            "\n",
            "Orient Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 25\n",
            "    })\n",
            "})\n",
            "\n",
            "\n",
            "Orient finished.\n",
            "\n",
            "\n",
            "Starting Decide ...\n",
            "trainable params: 10,716,674 || all params: 120,200,452 || trainable%: 8.9157\n",
            "\n",
            "\n",
            "Decide finished.\n",
            "\n",
            "\n",
            "Starting Act ...\n",
            "Dataset preprocessed successfully.\n",
            "\n",
            "\n",
            "Hugging Face data collator used.\n",
            "Initializing Trainer...\n",
            "\n",
            "\n",
            "Act finished.\n",
            "\n",
            "\n",
            "Run Dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 100\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 25\n",
            "    })\n",
            "})\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 00:37, Epoch 8/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.475000</td>\n",
              "      <td>0.896030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>2.209500</td>\n",
              "      <td>0.765166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.069000</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Evaluation:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "{'eval_loss': 0.800000011920929, 'eval_runtime': 0.1999, 'eval_samples_per_second': 125.072, 'eval_steps_per_second': 20.011, 'epoch': 8.64}\n",
            "\n",
            "\n",
            "Run  finished.\n",
            "\n",
            "\n",
            "Running experiment with:\n",
            "- Dataset: SetFit/mrpc\n",
            "- Model: bert-base-uncased\n",
            "- Config: {'max_length': 1024, 'quantization': True, 'use_unsloth': True, 'lora': True, 'dataset_size': 125, 'dataset_num_proc': 2, 'test_split_percentage': 0.2, 'training_args': {'output_dir': './output', 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'warmup_steps': 5, 'max_steps': 60, 'learning_rate': 0.0002, 'logging_steps': 10, 'weight_decay': 0.01, 'eval_steps': 20, 'save_steps': 20}, 'unsloth_model_id': 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit', 'max_seq_length': 2048, 'dtype': None, 'load_in_4bit': True}\n",
            "\n",
            "\n",
            "Agent dictionary(inside __init__):\n",
            "{'max_length': 1024, 'quantization': True, 'use_unsloth': True, 'lora': True, 'dataset_size': 125, 'dataset_num_proc': 2, 'test_split_percentage': 0.2, 'training_args': {'output_dir': './output', 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'warmup_steps': 5, 'max_steps': 60, 'learning_rate': 0.0002, 'logging_steps': 10, 'weight_decay': 0.01, 'eval_steps': 20, 'save_steps': 20}, 'unsloth_model_id': 'unsloth/mistral-7b-instruct-v0.3-bnb-4bit', 'max_seq_length': 2048, 'dtype': None, 'load_in_4bit': True}\n",
            "\n",
            "\n",
            "Starting Run ...\n",
            "Starting Observe ...\n",
            "Unsloth will be used.\n",
            "BERT model detected.\n",
            "Loading BERT with Unsloth\n",
            "==((====))==  Unsloth 2025.2.15: Fast Mistral patching. Transformers: 4.47.1.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.14G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1dccdd94eb64e579f3d359523bdf43b"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}