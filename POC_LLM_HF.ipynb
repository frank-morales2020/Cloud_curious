{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/POC_LLM_HF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkKx3xIPuD2d"
      },
      "source": [
        "https://huggingface.co/collections/open-llm-leaderboard/the-big-benchmarks-collection-64faca6335a7fc7d4ffe974a\n",
        "\n",
        "\n",
        "https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPPrd3vrEku7",
        "outputId": "1f1c09cb-da96-4e96-d558-c20ccdf8e71e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr 29 14:15:49 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0              45W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r87UBcYKaPb9"
      },
      "source": [
        "https://www.engineering.com/story/design-and-engineering-performance-and-flexibility-the-nvidia-l4-tensor-core-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gze0JGsUrpOD",
        "outputId": "0e729a0b-22c7-4f69-f78b-bf101770b1e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for colab-env (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install colab-env --quiet\n",
        "!pip install tiktoken -q\n",
        "!pip install accelerate -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of_yMPBIrxbw",
        "outputId": "6dc7b588-aca3-4706-b860-9de4fdb0b339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcqcgKr-izp9"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhORtoG8pVtw",
        "outputId": "9c8da109-6505-4b3e-8629-81a2fac2c2bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtune -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0IpiYjfpaeK",
        "outputId": "ff7ee129-c410-4512-ba27-b25205f745ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: tune [-h] {download,ls,cp,run,validate} ...\n",
            "\n",
            "Welcome to the TorchTune CLI!\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "\n",
            "subcommands:\n",
            "  {download,ls,cp,run,validate}\n",
            "    download            Download a model from the Hugging Face Hub.\n",
            "    ls                  List all built-in recipes and configs\n",
            "    cp                  Copy a built-in recipe or config to a local path.\n",
            "    run                 Run a recipe. For distributed recipes, this supports all torchrun\n",
            "                        arguments.\n",
            "    validate            Validate a config and ensure that it is well-formed.\n"
          ]
        }
      ],
      "source": [
        "!tune -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3Ak8-i7pbg2"
      },
      "outputs": [],
      "source": [
        "!tune download frankmorales2020/torchtune-Llama-2-7b --output-dir /tmp/Llama-2-7b-hf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toSKWSMArPoJ",
        "outputId": "dff18dc4-3a4e-4288-fd6d-754328591e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lrwxrwxrwx 1 root root  151 Apr 29 14:32 /tmp/Llama-2-7b-hf/hf_model_0001_0.pt -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/40be895e082a4f8b9d610326d2f9de309a58ae0219346b8afd608d27b0451c19\n",
            "lrwxrwxrwx 1 root root  151 Apr 29 14:26 /tmp/Llama-2-7b-hf/pytorch_model-00001-of-00002.bin -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/ee62ed2ad7ded505ae47df50bc6c52916860dfb1c009df4715148cc4bfb50d2f\n",
            "lrwxrwxrwx 1 root root  151 Apr 29 14:21 /tmp/Llama-2-7b-hf/pytorch_model-00002-of-00002.bin -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/1fd7762035b3ca4f2d6af6bf10129689a119b7c38058025f9842511532ea02fb\n",
            "lrwxrwxrwx 1 root root  151 Apr 29 14:21 /tmp/Llama-2-7b-hf/hf_model_0002_0.pt -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/f93037a0b82968e3c11e26de864988d642f37154bdfd1f314630c00d06ab4b17\n",
            "-rw-r--r-- 1 root root 489K Apr 29 14:18 /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "-rw-r--r-- 1 root root 1.8M Apr 29 14:18 /tmp/Llama-2-7b-hf/tokenizer.json\n",
            "-rw-r--r-- 1 root root  776 Apr 29 14:18 /tmp/Llama-2-7b-hf/tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  414 Apr 29 14:18 /tmp/Llama-2-7b-hf/special_tokens_map.json\n",
            "-rw-r--r-- 1 root root  27K Apr 29 14:18 /tmp/Llama-2-7b-hf/pytorch_model.bin.index.json\n",
            "-rw-r--r-- 1 root root  27K Apr 29 14:18 /tmp/Llama-2-7b-hf/model.safetensors.index.json\n",
            "lrwxrwxrwx 1 root root  151 Apr 29 14:18 /tmp/Llama-2-7b-hf/adapter_0.pt -> ../../root/.cache/huggingface/hub/models--frankmorales2020--torchtune-Llama-2-7b/blobs/9b209735a82e8963036e00bc4f21c59d7dee6c196f5a588b6711b3f03d30dc99\n",
            "-rw-r--r-- 1 root root 1.2M Apr 29 14:18 /tmp/Llama-2-7b-hf/Responsible-Use-Guide.pdf\n",
            "-rw-r--r-- 1 root root  609 Apr 29 14:18 /tmp/Llama-2-7b-hf/config.json\n",
            "-rw-r--r-- 1 root root  188 Apr 29 14:18 /tmp/Llama-2-7b-hf/generation_config.json\n",
            "-rw-r--r-- 1 root root 4.7K Apr 29 14:18 /tmp/Llama-2-7b-hf/USE_POLICY.md\n",
            "-rw-r--r-- 1 root root  22K Apr 29 14:18 /tmp/Llama-2-7b-hf/README.md\n",
            "-rw-r--r-- 1 root root 6.9K Apr 29 14:18 /tmp/Llama-2-7b-hf/LICENSE.txt\n"
          ]
        }
      ],
      "source": [
        "!ls -ltha /tmp/Llama-2-7b-hf/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYv2HMBVrtfh",
        "outputId": "f3215bfb-fb00-4af1-9e63-d87deb1a61ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied file to /content/custom_generation_config.yaml\n"
          ]
        }
      ],
      "source": [
        "!tune cp generation /content/custom_generation_config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "before fine-tunne"
      ],
      "metadata": {
        "id": "jrXzQoQpTUuS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jzx_4W6AsY_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4b4ae8-a39c-45de-fbfb-30349086ec97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:torchtune.utils.logging:Running InferenceRecipe with resolved config:\n",
            "\n",
            "checkpointer:\n",
            "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
            "  checkpoint_dir: /tmp/Llama-2-7b-hf/\n",
            "  checkpoint_files:\n",
            "  - pytorch_model-00001-of-00002.bin\n",
            "  - pytorch_model-00002-of-00002.bin\n",
            "  model_type: LLAMA2\n",
            "  output_dir: /tmp/Llama-2-7b-hf/\n",
            "device: cuda\n",
            "dtype: bf16\n",
            "max_new_tokens: 300\n",
            "model:\n",
            "  _component_: torchtune.models.llama2.llama2_7b\n",
            "prompt: What are some interesting sites to visit in the Bay Area?\n",
            "quantizer: null\n",
            "seed: 1234\n",
            "temperature: 0.6\n",
            "tokenizer:\n",
            "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
            "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
            "top_k: 300\n",
            "\n",
            "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 1234. Local seed is seed + rank = 1234 + 0\n",
            "INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.\n",
            "INFO:torchtune.utils.logging:What are some interesting sites to visit in the Bay Area?\n",
            "I'm going to be spending the weekend in San Francisco and would love to find some new and interesting places to visit. It's been awhile since I've been to the Bay Area and would love to see some new and interesting sites.\n",
            "I've been to the Golden Gate Bridge, Fisherman's Wharf, Alcatraz, and a bunch of other typical touristy places.\n",
            "I've been to Muir Woods, and want to go back.\n",
            "I've been to Sausalito and Tiburon, but want to go back.\n",
            "I've been to the Exploratorium, but want to go back.\n",
            "I've been to the Palace of Fine Arts, but want to go back.\n",
            "I've been to the Art Institute of California, but want to go back.\n",
            "I've been to the San Francisco Zoo, but want to go back.\n",
            "I've been to the Musee Mechanique, but want to go back.\n",
            "I've been to the Cable Cars, but want to go back.\n",
            "I've been to the De Young Museum, but want to go back.\n",
            "I've been to the California Academy of Sciences, but want to go back.\n",
            "I've been to the Conservatory of Flowers, but want to go back.\n",
            "I've been to the Japanese Tea Garden,\n",
            "INFO:torchtune.utils.logging:Time for inference: 12.46 sec total, 24.07 tokens/sec\n",
            "INFO:torchtune.utils.logging:Bandwidth achieved: 377.66 GB/s\n",
            "INFO:torchtune.utils.logging:Memory used: 15.72 GB\n"
          ]
        }
      ],
      "source": [
        "!tune run generate --config /content/custom_generation_config.yaml prompt=\"What are some interesting sites to visit in the Bay Area?\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "fine-tune model\n",
        "\n",
        "checkpoint_files: [\n",
        "        hf_model_0001_0.pt,\n",
        "        hf_model_0002_0.pt,\n",
        "  ]"
      ],
      "metadata": {
        "id": "N5O7awR2SxM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tune run generate --config /content/custom_generation_config.yaml prompt=\"What are some interesting sites to visit in the Bay Area?\""
      ],
      "metadata": {
        "id": "IltRzp6QSsdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-B4eX9pr4vt"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "#model_id = \"davidkim205/Rhea-72b-v0.5\" # HF AVERAGE = 81.22 #1 APRIL 5TH, 2024\n",
        "\n",
        "model_id = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
        "\n",
        "model_id = \"meta-llama/Llama-2-7b-hf\" #29/04/2024\n",
        "\n",
        "#Metric\tValue\n",
        "#Avg.\t81.22\n",
        "#AI2 Reasoning Challenge (25-Shot)\t79.78\n",
        "#HellaSwag (10-Shot)\t91.15\n",
        "#MMLU (5-Shot)\t77.95\n",
        "#TruthfulQA (0-shot)\t74.50\n",
        "#Winogrande (5-shot)\t87.85\n",
        "#GSM8k (5-shot)\t76.12\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, trust_remote_code=True, do_sample=True)\n",
        "\n",
        "# load into pipeline\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xlg7rlwzTpZL"
      },
      "outputs": [],
      "source": [
        "#prompt=\"What was the first album Beyoncé released as a solo artist?\"\n",
        "prompt=\"What is the capital of canada?\"\n",
        "outputs = pipe(prompt, max_new_tokens=512, do_sample=True, temperature=0.8,top_k=50, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8S7gBybTbGn"
      },
      "outputs": [],
      "source": [
        "print('Question: %s'%prompt)\n",
        "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOzKc5Z4ygMSezdfIRf9eYy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}