{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPM0BjggeX1lxvg4zH9J8Yf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/AGENT_ORCHESTRARION_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 OR L4 IN GOOGLE COLAB\n",
        "#!pip install -U transformers\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install datasets trl ninja packaging --quiet\n",
        "\n",
        "# Uncomment only if you're using A100 GPU\n",
        "#!pip install flash-attn --no-build-isolation\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install crewai --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install faiss-gpu --quiet\n",
        "!pip install sentence-transformers --quiet\n",
        "\n",
        "!pip install langchain_community -q\n",
        "\n",
        "!pip install faiss-cpu -q\n",
        "\n",
        "!pip install crewai-tools -q\n",
        "\n",
        "!pip install transformers -U -q"
      ],
      "metadata": {
        "id": "JpmClhdWqOSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bksvpyYquYhX",
        "outputId": "460dd890-5cf8-4276-f044-d988e74955b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar 22 15:50:09 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   43C    P8             17W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from langchain.tools import Tool\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import colab_env"
      ],
      "metadata": {
        "id": "i_E3c6n8qhDF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee95585-eacc-4b5f-bd73-fe92d02b24d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## new code"
      ],
      "metadata": {
        "id": "ePQO33VjFGm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade crewai langchain -q\n",
        "!pip install colab-env --quiet\n",
        "!pip install langchain_community -q\n",
        "!pip install faiss-cpu -q\n",
        "!pip install crewai-tools -q\n",
        "!pip install transformers -U -q"
      ],
      "metadata": {
        "id": "D3CqzlePup3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"You seem to be using the pipelines sequentially on GPU\")\n",
        "\n",
        "import colab_env\n",
        "import os\n",
        "\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(\n",
        "  token=access_token_write,\n",
        "  add_to_git_credential=True\n",
        ")"
      ],
      "metadata": {
        "id": "Bf3DDnXS7M7u"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade crewai langchain -q\n",
        "!pip install colab-env --quiet\n",
        "!pip install langchain_community -q\n",
        "!pip install faiss-cpu -q\n",
        "!pip install crewai-tools -q\n",
        "!pip install transformers -U -q"
      ],
      "metadata": {
        "id": "ezrlCK_rBMvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JZDGJq7ZIfZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install crewai==0.28.8 -q\n",
        "!pip crewai_tools==0.1.6 -q\n",
        "!pip langchain_community==0.0.29 -q\n",
        "!pip install transformers -U -q"
      ],
      "metadata": {
        "id": "soRg2wmjIZ5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew, Process, LLM\n",
        "from langchain_community.llms import HuggingFaceHub"
      ],
      "metadata": {
        "id": "rWc9jXcnNWSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-env --quiet\n",
        "import colab_env\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE_0MNkdNsCq",
        "outputId": "750a4f9d-c705-4933-f5b0-441ce88477e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "crewai 0.28.8 requires python-dotenv==1.0.0, but you have python-dotenv 0.21.1 which is incompatible.\n",
            "embedchain 0.1.113 requires python-dotenv<2.0.0,>=1.0.0, but you have python-dotenv 0.21.1 which is incompatible.\n",
            "crewai-tools 0.38.1 requires click>=8.1.8, but you have click 8.1.7 which is incompatible.\n",
            "crewai-tools 0.38.1 requires crewai>=0.105.0, but you have crewai 0.28.8 which is incompatible.\n",
            "crewai-tools 0.38.1 requires embedchain>=0.1.114, but you have embedchain 0.1.113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mDrive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")"
      ],
      "metadata": {
        "id": "D4ID0oUrNnTB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade crewai crewai_tools -q\n",
        "!pip install transformers -U -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xOrTkkSYcr8",
        "outputId": "433ceb6f-a22a-431f-9f2f-e380046d1979"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew, Process\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "import torch\n",
        "from langchain.llms import HuggingFaceHub\n",
        "import os\n",
        "import colab_env\n",
        "\n",
        "# Retrieve the token from the environment variable, if available\n",
        "access_token_write = os.getenv(\"HUGGINGFACE_ACCESS_TOKEN_WRITE\")\n",
        "\n",
        "# If access_token_write is not set, fallback to using the HUGGINGFACEHUB_API_TOKEN env variable\n",
        "if access_token_write is None:\n",
        "    access_token_write = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "else:\n",
        "   os.environ[\"MODEL_PROVIDER_HUGGINGFACE\"] = \"true\"\n",
        "   # Set the MODEL_PROVIDER environment variable to tell litellm to use Hugging Face Hub\n",
        "   os.environ[\"MODEL_PROVIDER\"] = \"huggingface\"\n",
        "\n",
        "   print(\"THE HUGGINGFACE_ACCESS_TOKEN_WRITE IS WORKING ..... \")\n",
        "   print('\\n')\n",
        "\n",
        "# If neither is set, prompt the user to enter the token manually\n",
        "if access_token_write is None:\n",
        "    access_token_write = input(\"Enter your Hugging Face API token: \")\n",
        "\n",
        "\n",
        "# Initialize the HuggingFaceHub LLM using the token\n",
        "model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"  # Or your preferred model\n",
        "hf_llm = HuggingFaceHub(\n",
        "    repo_id=model_name,\n",
        "    model_kwargs={\"temperature\": 0.1, \"max_length\": 200},\n",
        "    huggingfacehub_api_token=access_token_write  # Pass the token here\n",
        ")\n",
        "\n",
        "\n",
        "# 2. Define Agents\n",
        "researcher = Agent(\n",
        "    role='Senior Research Analyst',\n",
        "    goal='Conduct thorough research on a given topic using only the provided LLM.',\n",
        "    backstory=\"You are a seasoned research analyst with a knack for finding key information.\",\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    llm=hf_llm,  # Assign the Hugging Face Hub LLM\n",
        ")\n",
        "\n",
        "\n",
        "writer = Agent(\n",
        "    role='Technical Writer',\n",
        "    goal='Write a clear and concise tutorial based on research using only the provided LLM.',\n",
        "    backstory=\"You are an expert technical writer, skilled in simplifying complex topics.\",\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    llm=hf_llm,  # Assign the Hugging Face Hub LLM\n",
        ")\n",
        "\n",
        "# 3. Define Tasks\n",
        "research_task = Task(\n",
        "    description=\"Research the basics of crewAI, focusing on agents, tasks, and crews.\",\n",
        "    agent=researcher,\n",
        "    expected_output=\"A concise summary of crewAI, agents, tasks, and crews.\",\n",
        ")\n",
        "\n",
        "write_tutorial_task = Task(\n",
        "    description=\"Write a short tutorial explaining crewAI using the research provided.\",\n",
        "    agent=writer,\n",
        "    expected_output=\"A short tutorial explaining crewAI concepts.\",\n",
        ")\n",
        "\n",
        "# 4. Form the Crew\n",
        "crew = Crew(\n",
        "    agents=[researcher, writer],\n",
        "    tasks=[research_task, write_tutorial_task],\n",
        "    process=Process.sequential,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# 5. Kickoff the Crew\n",
        "result = crew.kickoff()\n",
        "\n",
        "# 6. Print the Result\n",
        "print(\"\\n\\nTutorial using Hugging Face Hub LLM:\\n\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "BJYbLvvrNT9C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}