{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMe2H/8DSHliKt7B9QB1Y2W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/AGENT_ORCHESTRARION_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 OR L4 IN GOOGLE COLAB\n",
        "#!pip install -U transformers\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install datasets trl ninja packaging --quiet\n",
        "\n",
        "# Uncomment only if you're using A100 GPU\n",
        "#!pip install flash-attn --no-build-isolation\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install crewai --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install faiss-gpu --quiet\n",
        "!pip install sentence-transformers --quiet\n",
        "\n",
        "!pip install langchain_community -q\n",
        "\n",
        "!pip install faiss-cpu -q\n",
        "\n",
        "!pip install crewai-tools -q\n",
        "\n",
        "!pip install transformers -U -q"
      ],
      "metadata": {
        "id": "JpmClhdWqOSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bksvpyYquYhX",
        "outputId": "56b22429-d7a3-434e-bc06-7a8de77a4813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar 22 11:20:06 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0             42W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from langchain.tools import Tool\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import colab_env\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "id": "i_E3c6n8qhDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## new code"
      ],
      "metadata": {
        "id": "ePQO33VjFGm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 OR L4 IN GOOGLE COLAB\n",
        "#!pip install -U transformers\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install datasets trl ninja packaging --quiet\n",
        "\n",
        "# Uncomment only if you're using A100 GPU\n",
        "#!pip install flash-attn --no-build-isolation\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install crewai --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install faiss-gpu --quiet\n",
        "!pip install sentence-transformers --quiet\n",
        "\n",
        "!pip install langchain_community -q\n",
        "\n",
        "!pip install faiss-cpu -q\n",
        "\n",
        "!pip install crewai-tools -q\n",
        "\n",
        "!pip install transformers -U -q"
      ],
      "metadata": {
        "id": "2mMVir4LFCjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from langchain.tools import Tool\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import colab_env\n",
        "# from openai import OpenAI  # Removed OpenAI import\n",
        "\n",
        "# client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))  # Removed OpenAI client initialization\n",
        "\n",
        "# BitsAndBytesConfig int-4 config\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Setup LangChain components for RAG (using Mistral via Transformers)\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "vectorstore = FAISS.from_texts([\"Example product information\", \"More example product info\"], embeddings) # Replace with your actual data\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"  # 01 march 2024 AND 10/03/\n",
        "\n",
        "# Mistral pipeline setup with 4-bit quantization\n",
        "mistral_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "mistral_llm = HuggingFacePipeline(pipeline=mistral_pipeline)\n",
        "\n",
        "# Modified RAG logic using LLMChain\n",
        "template = \"\"\"Use the following pieces of context to answer the question\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt_template = PromptTemplate(\n",
        "    template=template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Set the environment variable for LiteLLM to use Hugging Face\n",
        "os.environ[\"LITELLM_PROVIDER\"] = \"huggingface\"\n",
        "\n",
        "from crewai.tools import BaseTool  # Import BaseTool from crewai.tools\n",
        "\n",
        "# Define a CrewAI Tool\n",
        "from crewai.tools import BaseTool\n",
        "from crewai.agents.crew_agent_executor import CrewAgentExecutor  # Replace\n",
        "\n",
        "# Then, you may need to access AgentExecutor like this:\n",
        "# agent_executor = CrewAgentExecutor.AgentExecutor(llm=model_id, provider=\n",
        "\n",
        "from crewai.tools import BaseTool\n",
        "import litellm\n",
        "\n",
        "litellm.provider = \"huggingface\"\n",
        "\n",
        "class CrewTool(BaseTool):\n",
        "    name: str = \"Product Information Retriever\"\n",
        "    description: str = \"Useful for retrieving product information from internal databases.\"\n",
        "\n",
        "    def run(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Retrieves product information based on the query using Mistral pipeline.\n",
        "        Includes enhanced error handling.\n",
        "        \"\"\"\n",
        "        if not query:\n",
        "            print(\"Error: Query cannot be empty.\")\n",
        "            return \"No query provided.\"  # Return a specific error message\n",
        "\n",
        "        try:\n",
        "            docs = retriever.get_relevant_documents(query)\n",
        "            if not docs:\n",
        "                print(\"Warning: No relevant documents found.\")\n",
        "                return \"No product information found for the given query.\"\n",
        "\n",
        "            context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "            # Call mistral_pipeline with query\n",
        "            response = mistral_pipeline(query, max_new_tokens=256)\n",
        "\n",
        "            # Extract generated text from the response\n",
        "            content = response[0]['generated_text']\n",
        "            return content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during product information retrieval: {e}\")\n",
        "            return \"An error occurred during product information retrieval.\"  # Return a specific error message\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Retrieves product information based on the query using Mistral pipeline.\n",
        "        This is a placeholder for the actual implementation.\n",
        "        \"\"\"\n",
        "        if not query:\n",
        "            print(\"Error: Query cannot be empty.\")\n",
        "            return \"No query provided.\"\n",
        "\n",
        "        # RAG implementation using LLMChain\n",
        "        rag_chain = LLMChain(llm=mistral_llm, prompt=prompt_template)\n",
        "        docs = retriever.get_relevant_documents(query)\n",
        "        context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        try:\n",
        "            response = rag_chain.run(context=context, question=query)\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f\"Error during product information retrieval: {e}\")\n",
        "            return \"An error occurred during product information retrieval.\"\n",
        "\n",
        "# Define Agents using CrewAI\n",
        "retrieval_agent = Agent(\n",
        "    role='Product Information Retriever',\n",
        "    goal='Retrieve product details from internal databases.',\n",
        "    backstory='You are an expert in accessing product specifications.',\n",
        "    tools=[CrewTool()],  # Use the CrewTool class\n",
        "    llm=mistral_llm,  # Use the Mistral LLM directly\n",
        ")\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "llama2_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Use from_pretrained to download the model\n",
        "model = AutoModelForCausalLM.from_pretrained(llama2_model_id,\n",
        "                                             trust_remote_code=True,\n",
        "                                             quantization_config=quantization_config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(llama2_model_id,\n",
        "                                           trust_remote_code=True)\n",
        "\n",
        "# Now, update the pipeline to use the downloaded model\n",
        "llama2_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    quantization_config=quantization_config\n",
        ")\n",
        "\n",
        "llama2_llm = HuggingFacePipeline(pipeline=llama2_pipeline)\n",
        "\n",
        "search_agent = Agent(\n",
        "    role='Web Searcher',\n",
        "    goal='Gather relevant information from external web sources.',\n",
        "    backstory='You are an expert at searching the web for information',\n",
        "    llm=llama2_llm,\n",
        "    llm_config={\"model\": llama2_model_id, \"provider\": \"huggingface\"}\n",
        ")\n",
        "\n",
        "sentiment_agent = Agent(\n",
        "    role='Sentiment Analyzer',\n",
        "    goal='Determine the customer sentiment.',\n",
        "    backstory='You are an expert at analyzing customer emotions.',\n",
        "    llm=llama2_llm,\n",
        "    llm_config={\"model\": llama2_model_id, \"provider\": \"huggingface\"}\n",
        ")\n",
        "\n",
        "rag_agent = Agent(\n",
        "    role='Response Generator',\n",
        "    goal='Formulate a comprehensive and helpful response, integrating product details, web search results, and sentiment analysis.',\n",
        "    backstory='You are an expert at combining retrieved information into a cohesive and informative response for customers.',\n",
        "    llm=llama2_llm,\n",
        "    llm_config={\"model\": llama2_model_id, \"provider\": \"huggingface\"}\n",
        ")\n",
        "\n",
        "# Define Tasks\n",
        "retrieval_task = Task(\n",
        "    description=\"Retrieve product information based on the user's query.\",\n",
        "    agent=retrieval_agent,\n",
        "    expected_output=\"str\"  # Change expected_output to \"str\"\n",
        ")\n",
        "search_task = Task(\n",
        "    description=\"Search the web for any related information.\",\n",
        "    agent=search_agent,\n",
        "    expected_output=\"str\"  # Change expected_output to \"str\"\n",
        ")\n",
        "sentiment_task = Task(\n",
        "    description=\"Determine the sentiment of the customer's query.\",\n",
        "    agent=sentiment_agent,\n",
        "    expected_output=\"str\"  # Change expected_output to \"str\"\n",
        ")\n",
        "rag_task = Task(\n",
        "    description=\"\"\"\n",
        "    Combine the product information, web search results, and sentiment analysis to generate a final response.\n",
        "    The response should be structured as follows:\n",
        "    - Product Information: (Summarize the product details)\n",
        "    - Web Search Summary: (Summarize relevant information found on the web)\n",
        "    - Sentiment Analysis: (Report the overall customer sentiment)\n",
        "    - Final Answer: (A comprehensive and helpful answer to the user's query, incorporating all the above information)\n",
        "    \"\"\",\n",
        "    agent=rag_agent,\n",
        "    expected_output=\"str\"  # Change expected_output to \"str\"\n",
        ")\n",
        "\n",
        "# Create Crew\n",
        "crew = Crew(\n",
        "    agents=[retrieval_agent, search_agent, sentiment_agent, rag_agent],\n",
        "    tasks=[retrieval_task, search_task, sentiment_task, rag_task],\n",
        "    process=Process.sequential\n",
        ")\n",
        "\n",
        "# Run Crew\n",
        "user_query = \"Where can I find information about my product?\"\n",
        "result = crew.kickoff(inputs={\"user_query\": user_query})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "NZPj9-3jEqu7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}