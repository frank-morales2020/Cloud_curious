{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPzznFYbt1BMF8/C9mohAkI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/AGENT_ORCHESTRARION_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pytorch & other libraries\n",
        "!pip install torch tensorboard --quiet\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
        "\n",
        "\n",
        "\n",
        "#FlashAttention only supports Ampere GPUs or newer. #NEED A100 OR L4 IN GOOGLE COLAB\n",
        "#!pip install -U transformers\n",
        "!pip install -U flash-attn --no-build-isolation --quiet\n",
        "\n",
        "\n",
        "! pip install peft --quiet\n",
        "! pip install datasets trl ninja packaging --quiet\n",
        "\n",
        "# Uncomment only if you're using A100 GPU\n",
        "#!pip install flash-attn --no-build-isolation\n",
        "!pip install diffusers safetensors  --quiet\n",
        "!pip install colab-env --quiet\n",
        "\n",
        "!pip install crewai --quiet\n",
        "!pip install langchain --quiet\n",
        "!pip install faiss-gpu --quiet\n",
        "!pip install sentence-transformers --quiet\n",
        "\n",
        "!pip install langchain_community -q\n",
        "\n",
        "!pip install faiss-cpu -q\n",
        "\n",
        "!pip install crewai-tools -q\n",
        "\n",
        "!pip install transformers -U -q"
      ],
      "metadata": {
        "id": "JpmClhdWqOSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bksvpyYquYhX",
        "outputId": "56b22429-d7a3-434e-bc06-7a8de77a4813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar 22 11:20:06 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0             42W /  400W |       0MiB /  40960MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import IPython\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        ")\n",
        "\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from langchain.tools import Tool\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import colab_env\n",
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = os.getenv(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "id": "i_E3c6n8qhDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## new code"
      ],
      "metadata": {
        "id": "ePQO33VjFGm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import torch\n",
        "from typing import List, Optional\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai.tools import BaseTool\n",
        "# from langchain_huggingface import HuggingFaceEmbeddings as HFEmbeddings # corrected import\n",
        "\n",
        "# Initialize logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Utility Functions\n",
        "def concatenate_documents(docs: List) -> str:\n",
        "    \"\"\"\n",
        "    Concatenates the page content of documents with newline characters.\n",
        "\n",
        "    Args:\n",
        "        docs: A list of documents.\n",
        "\n",
        "    Returns:\n",
        "        A string containing the concatenated page content.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not docs:\n",
        "            logging.warning(\"No documents provided for concatenation.\")\n",
        "            return \"\"\n",
        "        return \"\\n\".join([doc.page_content for doc in docs])\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Error concatenating documents:\")\n",
        "        return \"\"  # Or raise an exception, depending on your error handling policy\n",
        "\n",
        "# LLM Setup\n",
        "def setup_mistral_pipeline() -> Optional[HuggingFacePipeline]:\n",
        "    \"\"\"\n",
        "    Sets up the Mistral pipeline with 4-bit quantization.\n",
        "\n",
        "    Returns:\n",
        "        A HuggingFacePipeline using the Mistral model, or None if setup fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        model_id = \"mistralai/Mistral-7B-Instruct-v0.1\" # 01 march 2024 AND\n",
        "        mistral_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model_id,\n",
        "            quantization_config=quantization_config\n",
        "        )\n",
        "        return HuggingFacePipeline(pipeline=mistral_pipeline)\n",
        "    except AttributeError as e:\n",
        "        logging.error(f\"AttributeError in setup_mistral_pipeline: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Error setting up Mistral pipeline:\")\n",
        "        return None\n",
        "\n",
        "def setup_llama2_pipeline() -> Optional[HuggingFacePipeline]:\n",
        "    \"\"\"\n",
        "    Sets up the Llama 2 pipeline.\n",
        "\n",
        "    Returns:\n",
        "        A HuggingFacePipeline using the Llama 2 model, or None if setup fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        llama2_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            llama2_model_id,\n",
        "            trust_remote_code=True,\n",
        "            quantization_config=BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(llama2_model_id, trust_remote_code=True)\n",
        "        llama2_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            # quantization_config=quantization_config # NameError: name 'quantization_config' is not defined\n",
        "        )\n",
        "        return HuggingFacePipeline(pipeline=llama2_pipeline)\n",
        "    except AttributeError as e:\n",
        "        logging.error(f\"AttributeError in setup_llama2_pipeline: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Error setting up Llama 2 pipeline:\")\n",
        "        return None\n",
        "\n",
        "# Tools\n",
        "class ProductInfoRetrieverTool(BaseTool):\n",
        "    \"\"\"\n",
        "    A tool for retrieving product information from a retriever.\n",
        "    \"\"\"\n",
        "\n",
        "    name: str = \"Product Information Retriever\"\n",
        "    description: str = \"Useful for retrieving product information from internal databases.\"\n",
        "\n",
        "    def __init__(self, retriever: RetrievalQA, mistral_pipeline: Optional[callable] = None):\n",
        "        super().__init__()\n",
        "        self.retriever = retriever\n",
        "        self.mistral_pipeline = mistral_pipeline\n",
        "\n",
        "    def _run(self, query: str) -> str:\n",
        "        \"\"\"\n",
        "        Retrieves product information based on the query.\n",
        "\n",
        "        Args:\n",
        "            query: The query to retrieve product information for.\n",
        "\n",
        "        Returns:\n",
        "            The retrieved product information.\n",
        "        \"\"\"\n",
        "        if not query:\n",
        "            logging.error(\"Query cannot be empty.\")\n",
        "            return \"No query provided.\"\n",
        "\n",
        "        try:\n",
        "            docs = self.retriever.get_relevant_documents(query)\n",
        "            if not docs:\n",
        "                logging.warning(\"No relevant documents found.\")\n",
        "                return \"No product information found for the given query.\"\n",
        "\n",
        "            context = concatenate_documents(docs)  # Use the utility function\n",
        "\n",
        "            if self.mistral_pipeline:\n",
        "                try:\n",
        "                    response = self.mistral_pipeline(context, max_new_tokens=256)\n",
        "                    content = response[0]['generated_text']\n",
        "                    return content\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Error during mistral pipeline execution: {e}\")\n",
        "                    return \"Error processing retrieved information.\"\n",
        "            else:\n",
        "                logging.warning(\"Mistral pipeline not provided. Returning raw context.\")\n",
        "                return context  # Return the raw context if no pipeline\n",
        "        except Exception as e:\n",
        "            logging.exception(\"Error during product information retrieval:\")\n",
        "            return \"An error occurred during product information retrieval.\"\n",
        "\n",
        "# Agents\n",
        "class RetrievalAgent(Agent):\n",
        "    \"\"\"\n",
        "    An agent responsible for retrieving product information.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, llm: HuggingFacePipeline, retriever_tool: BaseTool): # changed type hint\n",
        "        super().__init__(\n",
        "            role='Product Information Retriever',\n",
        "            goal='Retrieve product details from internal databases.',\n",
        "            backstory='You are an expert in accessing product specifications.',\n",
        "            tools=[retriever_tool],\n",
        "            llm=llm\n",
        "        )\n",
        "\n",
        "class SearchAgent(Agent):\n",
        "    \"\"\"\n",
        "    An agent responsible for searching the web for information.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm: HuggingFacePipeline, llm_config: Optional[dict] = None):\n",
        "        super().__init__(\n",
        "            role='Web Searcher',\n",
        "            goal='Gather relevant information from external web sources.',\n",
        "            backstory='You are an expert at searching the web for information.',\n",
        "            llm=llm,\n",
        "            llm_config=llm_config or {\"model\": \"meta-llama/Llama-2-7b-chat-hf\", \"provider\": \"huggingface\"}\n",
        "        )\n",
        "\n",
        "class SentimentAnalyzerAgent(Agent):\n",
        "    \"\"\"\n",
        "    An agent responsible for analyzing sentiment.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm: HuggingFacePipeline, llm_config: Optional[dict] = None):\n",
        "        super().__init__(\n",
        "            role='Sentiment Analyzer',\n",
        "            goal='Determine the customer sentiment.',\n",
        "            backstory='You are an expert at analyzing customer emotions.',\n",
        "            llm=llm,\n",
        "            llm_config=llm_config or {\"model\": \"meta-llama/Llama-2-7b-chat-hf\", \"provider\": \"huggingface\"}\n",
        "        )\n",
        "\n",
        "class ResponseGeneratorAgent(Agent):\n",
        "    \"\"\"\n",
        "    An agent responsible for generating a comprehensive response.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm: HuggingFacePipeline, llm_config: Optional[dict] = None):\n",
        "        super().__init__(\n",
        "            role='Response Generator',\n",
        "            goal='Formulate a comprehensive and helpful response, integrating product information, web search results, and sentiment analysis.',\n",
        "            backstory='You are an expert at combining retrieved information into a coherent and helpful answer.',\n",
        "            llm=llm,\n",
        "            llm_config=llm_config or {\"model\": \"meta-llama/Llama-2-7b-chat-hf\", \"provider\": \"huggingface\"}\n",
        "        )\n",
        "\n",
        "# Tasks\n",
        "def create_tasks(retrieval_agent: RetrievalAgent, search_agent: SearchAgent, sentiment_agent: SentimentAnalyzerAgent, rag_agent: ResponseGeneratorAgent):\n",
        "    \"\"\"\n",
        "    Creates the tasks for the crew.\n",
        "\n",
        "    Args:\n",
        "        retrieval_agent: The retrieval agent.\n",
        "        search_agent: The search agent.\n",
        "        sentiment_agent: The sentiment analyzer agent.\n",
        "        rag_agent: The response generator agent.\n",
        "\n",
        "    Returns:\n",
        "        A list of tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    retrieval_task = Task(\n",
        "        description=\"Retrieve product information based on the user's query.\",\n",
        "        agent=retrieval_agent,\n",
        "        expected_output=\"str\"\n",
        "    )\n",
        "\n",
        "    search_task = Task(\n",
        "        description=\"Search the web for any related information.\",\n",
        "        agent=search_agent,\n",
        "        expected_output=\"str\"\n",
        "    )\n",
        "\n",
        "    sentiment_task = Task(\n",
        "        description=\"Determine the sentiment of the customer's query.\",\n",
        "        agent=sentiment_agent,\n",
        "        expected_output=\"str\"\n",
        "    )\n",
        "\n",
        "    rag_task = Task(\n",
        "        description=\"\"\"\n",
        "            Combine the product information, web search results, and sentiment analysis to generate a comprehensive response.\n",
        "            The response should be structured as follows:\n",
        "\n",
        "            Product Information: (Summarize the product details)\n",
        "\n",
        "            Web Search Summary: (Summarize relevant information found on the web)\n",
        "\n",
        "            Sentiment Analysis: (Report the overall customer sentiment)\n",
        "\n",
        "            Final Answer: (A comprehensive and helpful answer to the user's query)\n",
        "            \"\"\",\n",
        "        agent=rag_agent,\n",
        "        expected_output=\"str\"\n",
        "    )\n",
        "\n",
        "    return [retrieval_task, search_task, sentiment_task, rag_task]\n",
        "\n",
        "# Main Function\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Orchestrates the crew of agents to handle product-related queries.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Initialize LLMs and Embeddings\n",
        "        mistral_llm = setup_mistral_pipeline()\n",
        "        if not mistral_llm:\n",
        "            logging.error(\"Failed to setup Mistral pipeline. Exiting.\")\n",
        "            return\n",
        "\n",
        "        llama2_llm = setup_llama2_pipeline()\n",
        "        if not llama2_llm:\n",
        "            logging.error(\"Failed to setup Llama2 pipeline. Exiting.\")\n",
        "            return\n",
        "\n",
        "        embeddings = HuggingFaceEmbeddings() # corrected instantiation\n",
        "\n",
        "        # 2. Setup Vectorstore and Retriever (Example)\n",
        "        vectorstore = FAISS.from_texts([\"Example product information\", \"More example data\"], embeddings)\n",
        "        retriever = vectorstore.as_retriever()\n",
        "\n",
        "        # 3. Initialize Tools\n",
        "        retriever_tool = ProductInfoRetrieverTool(retriever=retriever, mistral_pipeline=mistral_llm)\n",
        "\n",
        "        # 4. Initialize Agents\n",
        "        retrieval_agent = RetrievalAgent(llm=mistral_llm, retriever_tool=retriever_tool)\n",
        "        search_agent = SearchAgent(llm=llama2_llm)\n",
        "        sentiment_agent = SentimentAnalyzerAgent(llm=llama2_llm)\n",
        "        rag_agent = ResponseGeneratorAgent(llm=llama2_llm)\n",
        "\n",
        "        # 5. Create Tasks\n",
        "        tasks = create_tasks(retrieval_agent, search_agent, sentiment_agent, rag_agent)\n",
        "\n",
        "        # 6. Create and Run Crew\n",
        "        crew = Crew(\n",
        "            agents=[retrieval_agent, search_agent, sentiment_agent, rag_agent],\n",
        "            tasks=tasks,\n",
        "            process=Process.sequential  # Or Process.hierarchical\n",
        "        )\n",
        "\n",
        "        result = crew.kickoff()\n",
        "        print(\"Final result:\", result)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.exception(\"An error occurred during the crew execution:\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2mMVir4LFCjl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}