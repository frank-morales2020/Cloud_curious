{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPgJxRD1OQBth9uzHEngjF3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/frank-morales2020/Cloud_curious/blob/master/Qwen3_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvYlB--DHtLY"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen3-8B\" # Or any other Qwen3 model like \"Qwen/Qwen3-30B-A3B\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\", # or torch.bfloat16 for bfloat16 models\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "prompt = \"Explain the concept of quantum entanglement.\"\n",
        "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "output = model.generate(input_ids, max_new_tokens=512)\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRvv2TtGKrv4",
        "outputId": "15d19e5c-7ed1-4814-e301-f8b16459ba59"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user\n",
            "Explain the concept of quantum entanglement.\n",
            "assistant\n",
            "<think>\n",
            "Okay, I need to explain quantum entanglement. Let me start by recalling what I know. Entanglement is a phenomenon in quantum mechanics where particles become interconnected, right? But I should make sure I get the details right.\n",
            "\n",
            "First, maybe I should mention that entangled particles are in a superposition of states. So, when you measure one, the other's state is instantly determined, no matter the distance. Wait, but how does that work? Einstein called it \"spooky action at a distance,\" which he didn't like. But I think that's been proven through experiments, like Bell's theorem. \n",
            "\n",
            "I should explain the basics. Let's say you have two particles, like electrons, that are entangled. Their quantum states are linked. If you measure the spin of one, the other's spin is instantly known, even if they're light-years apart. But it's not that information is transmitted faster than light, right? Because the outcome is random, so no actual information is sent. That's important to clarify to avoid misunderstandings.\n",
            "\n",
            "Wait, how do you create entangled particles? Maybe through processes like spontaneous parametric down-conversion, where a photon splits into two entangled photons. Or maybe through other interactions. But maybe that's too detailed. The user might just want a basic explanation.\n",
            "\n",
            "Also, I should mention that entanglement is a key resource for quantum computing and quantum communication. But maybe that's for later. Let me structure this: start with the definition, then how it works, the EPR paradox and Bell's theorem, and then the implications or applications. \n",
            "\n",
            "I need to make sure not to mix up quantum superposition with entanglement. Superposition is when a particle exists in multiple states at once. Entanglement is when two or more particles are correlated in such a way that the state of one affects the other. \n",
            "\n",
            "Wait, maybe use an example. Like, two entangled particles with opposite spins. If one is measured as up, the other is down, regardless of distance. But the measurement outcome is probabilistic. So the correlation is there, but the individual results are random. That's the key point. \n",
            "\n",
            "Also, clarify that it's not about communication, but about the correlation between measurements. So even though the particles are separated, their states are linked. But no information is sent faster than light. That's important because it avoids violating relativity. \n",
            "\n",
            "I should check if there are common misconceptions to address. For example, people might\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AGENT"
      ],
      "metadata": {
        "id": "Y5suduW7S4xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import re # Import regex module\n",
        "\n",
        "class Qwen3ExplanationAgent:\n",
        "    \"\"\"\n",
        "    An AI agent designed to explain concepts using the Qwen3 model,\n",
        "    emulating a structured thought process (analyze, plan, generate, refine).\n",
        "    This agent directly integrates the Qwen3 model for content generation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"Qwen/Qwen3-8B\", name=\"Qwen3Agent\"): # Added 'name' parameter with default\n",
        "        \"\"\"\n",
        "        Initializes the Qwen3 model and tokenizer.\n",
        "        Args:\n",
        "            model_name (str): The name of the Qwen3 model to load.\n",
        "            name (str): The name of the agent.\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.name = name # Initialize the 'name' attribute\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self._load_model()\n",
        "        # Updated knowledge_base_keywords for flight planning domain\n",
        "        self.knowledge_base_keywords = {\n",
        "            \"flight planning\": [\"route optimization\", \"fuel calculation\", \"weather considerations\", \"air traffic control\", \"regulations\"],\n",
        "            \"route optimization\": [\"great circle route\", \"wind impact\", \"restricted airspace\", \"waypoints\"],\n",
        "            \"fuel calculation\": [\"payload\", \"distance\", \"altitude\", \"reserve fuel\", \"fuel consumption rate\"],\n",
        "            \"weather considerations\": [\"wind\", \"turbulence\", \"icing\", \"thunderstorms\", \"visibility\"],\n",
        "            \"air traffic control\": [\"airspace classes\", \"flight rules\", \"communication procedures\"],\n",
        "            \"aviation regulations\": [\"flight rules\", \"licensing\", \"aircraft maintenance\"]\n",
        "        }\n",
        "        print(f\"[{self.name} - Init] Agent initialized with model: {self.model_name}\")\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Loads the Qwen3 model and tokenizer. This can be resource-intensive.\n",
        "        \"\"\"\n",
        "        print(f\"[{self.name} - Model Loading] Loading Qwen3 model '{self.model_name}'...\") # Used self.name\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                self.model_name,\n",
        "                torch_dtype=\"auto\", # Automatically selects best dtype (e.g., bfloat16, float16, float32)\n",
        "                device_map=\"auto\"   # Automatically maps model layers to available devices (GPU/CPU)\n",
        "            )\n",
        "            print(f\"[{self.name} - Model Loading] Model loaded successfully on device: {self.model.device}\") # Used self.name\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.name} - Model Loading Error] Failed to load model: {e}\") # Used self.name\n",
        "            print(\"Please ensure 'transformers' and 'torch' are installed and you have sufficient resources (e.g., GPU memory).\")\n",
        "            self.tokenizer = None # Set to None to indicate failure\n",
        "            self.model = None    # Set to None to indicate failure\n",
        "\n",
        "    def _analyze_query(self, query):\n",
        "        \"\"\"\n",
        "        Simulates analyzing the user's query to identify the core concept.\n",
        "        Args:\n",
        "            query (str): The user's input query.\n",
        "        Returns:\n",
        "            str: The identified concept key (e.g., \"flight planning\") or None.\n",
        "        \"\"\"\n",
        "        # print(f\"[{self.name} - Step 1: Analyzing Query] User asked: '{query}'\") # Removed log\n",
        "        query_lower = query.lower()\n",
        "        for concept in self.knowledge_base_keywords:\n",
        "            if concept in query_lower:\n",
        "                # print(f\"[{self.name} - Analysis] Identified core concept: '{concept}'\") # Removed log\n",
        "                return concept\n",
        "        # print(f\"[{self.name} - Analysis] Could not identify a specific concept. Will attempt a general explanation.\") # Removed log\n",
        "        return None\n",
        "\n",
        "    def _plan_explanation(self, concept_key):\n",
        "        \"\"\"\n",
        "        Simulates planning the structure of the explanation based on the concept.\n",
        "        Args:\n",
        "            concept_key (str): The identified concept.\n",
        "        Returns:\n",
        "            list: A list of sub-prompts for each section of the explanation.\n",
        "        \"\"\"\n",
        "        # print(f\"[{self.name} - Step 2: Planning Explanation] Structuring explanation for '{concept_key}'...\") # Removed log\n",
        "        if concept_key and concept_key in self.knowledge_base_keywords:\n",
        "            sections = self.knowledge_base_keywords[concept_key]\n",
        "            plan = [f\"Explain the {section.replace('_', ' ')} of {concept_key}.\" for section in sections]\n",
        "            # print(f\"[{self.name} - Plan] Generated plan: {json.dumps(plan, indent=2)}\") # Removed log\n",
        "            return plan\n",
        "        else:\n",
        "            # print(f\"[{self.name} - Plan] No specific plan for '{concept_key}'. Will provide a general response.\") # Removed log\n",
        "            return [f\"Explain '{concept_key}' in detail.\"] if concept_key else [\"Provide a general explanation for the query.\"]\n",
        "\n",
        "    def _generate_section_content(self, section_prompt):\n",
        "        \"\"\"\n",
        "        Uses the Qwen3 model to generate content for a specific section.\n",
        "        Args:\n",
        "            section_prompt (str): The specific prompt for generating a section (e.g., \"Define quantum entanglement.\").\n",
        "        Returns:\n",
        "            str: The generated text for that section.\n",
        "        \"\"\"\n",
        "        if not self.model or not self.tokenizer:\n",
        "            return \"Error: Qwen3 model not loaded. Cannot generate content.\"\n",
        "\n",
        "        # print(f\"[{self.name} - Step 3: Generating Content] Generating for: '{section_prompt}'\") # Removed log\n",
        "        messages = [{\"role\": \"user\", \"content\": section_prompt}]\n",
        "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        # Get both input_ids and attention_mask\n",
        "        encoded_input = self.tokenizer(text, return_tensors=\"pt\")\n",
        "        input_ids = encoded_input.input_ids.to(self.model.device)\n",
        "        attention_mask = encoded_input.attention_mask.to(self.model.device) # Explicitly get attention_mask\n",
        "\n",
        "        try:\n",
        "            # Pass attention_mask to the generate method and increase max_new_tokens\n",
        "            output = self.model.generate(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask, # Pass attention_mask\n",
        "                max_new_tokens=4096,           # Increased max_new_tokens\n",
        "                temperature=0.7,\n",
        "                do_sample=True\n",
        "            )\n",
        "            raw_generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            # --- DEBUG PRINT: Raw Generated Text ---\n",
        "            print(f\"\\n--- DEBUG: Section Prompt ---\\n{section_prompt}\\n------------------------------\")\n",
        "            print(f\"\\n--- DEBUG: Raw Generated Text ---\\n{raw_generated_text}\\n-----------------------------------\\n\")\n",
        "\n",
        "            # --- CRITICAL FIX: Robustly extract assistant's response and then clean <think> tags ---\n",
        "            assistant_response = \"\"\n",
        "            # Pattern to find the last 'assistant' turn and capture everything after it.\n",
        "            # This accounts for 'assistant\\n' or 'ASSISTANT:' or just 'assistant' at the end of a turn.\n",
        "            # Using re.DOTALL to match across newlines, re.IGNORECASE for flexibility.\n",
        "            match = re.search(r'(?:assistant|ASSISTANT):?\\s*(.*)', raw_generated_text, re.DOTALL | re.IGNORECASE)\n",
        "            if match:\n",
        "                assistant_response = match.group(1).strip()\n",
        "            else:\n",
        "                # Fallback: if no clear assistant marker is found after generation.\n",
        "                # This might happen if the model's output deviates significantly.\n",
        "                # In this case, we'll try to remove the initial user prompt part.\n",
        "                user_prompt_part = text # 'text' contains the user prompt from apply_chat_template\n",
        "                # This regex ensures we only remove the *beginning* of the string if it matches the prompt\n",
        "                user_prompt_pattern = r\"^\" + re.escape(user_prompt_part.strip()) + r\"\\s*\"\n",
        "                assistant_response = re.sub(user_prompt_pattern, \"\", raw_generated_text, flags=re.DOTALL).strip()\n",
        "                # If after removing the prompt, it's still empty, just take the raw text and hope for the best\n",
        "                if not assistant_response:\n",
        "                    assistant_response = raw_generated_text.strip()\n",
        "\n",
        "            # --- DEBUG PRINT: Assistant Response (before cleaning) ---\n",
        "            #print(f\"\\n--- DEBUG: Assistant Response (before cleaning) ---\\n{assistant_response}\\n-----------------------------------\\n\")\n",
        "\n",
        "            # Now, apply cleaning only to the extracted assistant's response\n",
        "            # Step 1: Remove complete <think>...</think> blocks\n",
        "            cleaned_text = re.sub(r'<think>.*?</think>', '', assistant_response, flags=re.DOTALL)\n",
        "            # Step 2: Remove any remaining <think> tags and everything after them (for unclosed tags)\n",
        "            # This regex will remove '<think>' and everything that follows it until the end of the string.\n",
        "            cleaned_text = re.sub(r'<think>.*', '', cleaned_text, flags=re.DOTALL)\n",
        "\n",
        "            final_text = cleaned_text.strip() # Ensure leading/trailing whitespace is removed after cleaning\n",
        "\n",
        "            # --- DEBUG PRINT: Final Cleaned Text ---\n",
        "            print(f\"\\n--- DEBUG: Final Cleaned Text ---\\n{final_text}\\n-----------------------------------\\n\")\n",
        "\n",
        "            # print(f\"[{self.name} - Generation] Generated {len(final_text.split())} words for section.\") # Removed log\n",
        "            return final_text\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.name} - Generation Error] Failed to generate content: {e}\") # Used self.name\n",
        "            return f\"Failed to generate content for '{section_prompt}' due to an error.\"\n",
        "\n",
        "    def _assemble_and_refine_response(self, concept_key, generated_sections):\n",
        "        \"\"\"\n",
        "        Simulates assembling and refining the generated content into a final response.\n",
        "        Args:\n",
        "            concept_key (str): The identified concept.\n",
        "            generated_sections (dict): Dictionary of generated content for each section.\n",
        "        Returns:\n",
        "            str: The final, polished explanation.\n",
        "        \"\"\"\n",
        "        # print(f\"[{self.name} - Step 4: Assembling & Refining] Combining sections and polishing...\") # Removed log\n",
        "        final_response_parts = []\n",
        "\n",
        "        if concept_key:\n",
        "            final_response_parts.append(f\"Here's an explanation of **{concept_key.replace('_', ' ').title()}**:\")\n",
        "\n",
        "            # Order the sections based on the predefined plan or a logical flow\n",
        "            ordered_sections = self.knowledge_base_keywords.get(concept_key, [])\n",
        "            for section_type in ordered_sections:\n",
        "                if section_type in generated_sections and generated_sections[section_type]:\n",
        "                    title = section_type.replace('_', ' ').title()\n",
        "                    final_response_parts.append(f\"\\n### {title}\\n{generated_sections[section_type]}\")\n",
        "        else:\n",
        "            # If no specific concept, just present the general explanation\n",
        "            if \"general_explanation\" in generated_sections:\n",
        "                final_response_parts.append(generated_sections[\"general_explanation\"])\n",
        "            else:\n",
        "                final_response_parts.append(\"I couldn't generate a specific explanation for your query.\")\n",
        "\n",
        "\n",
        "        # Add a concluding remark\n",
        "        final_response_parts.append(\"\\n\\nI hope this detailed explanation is helpful!\")\n",
        "\n",
        "        final_response = \"\\n\".join(final_response_parts)\n",
        "        # print(f\"[{self.name} - Refinement] Final response assembled.\") # Removed log\n",
        "        return final_response\n",
        "\n",
        "    def explain_concept(self, query):\n",
        "        \"\"\"\n",
        "        Orchestrates the agent's full thought process to explain a concept.\n",
        "        Args:\n",
        "            query (str): The user's query.\n",
        "        Returns:\n",
        "            str: The agent's final explanation.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Agentic Process Start ---\") # Keep this to show start\n",
        "        if not self.model or not self.tokenizer:\n",
        "            print(\"[Agent - Error] Model not ready. Cannot proceed with explanation.\")\n",
        "            return \"I am unable to process your request as the underlying model could not be loaded. Please check your environment.\"\n",
        "\n",
        "        concept_key = self._analyze_query(query)\n",
        "        plan_sections = self._plan_explanation(concept_key)\n",
        "\n",
        "        generated_content = {}\n",
        "        if concept_key:\n",
        "            # Generate content for each planned section using the Qwen3 model\n",
        "            for section_prompt_template in plan_sections:\n",
        "                # Extract the section type from the prompt for dict key (e.g., \"definition\")\n",
        "                # This is a bit heuristic, assuming prompt like \"Explain the definition of X.\"\n",
        "                section_type = section_prompt_template.split(\"Explain the \")[1].split(\" of\")[0].replace(' ', '_')\n",
        "                generated_content[section_type] = self._generate_section_content(section_prompt_template)\n",
        "        else:\n",
        "            # If no specific concept, generate a general explanation\n",
        "            general_prompt = f\"Explain '{query}' in detail.\"\n",
        "            generated_content[\"general_explanation\"] = self._generate_section_content(general_prompt)\n",
        "\n",
        "\n",
        "        final_explanation = self._assemble_and_refine_response(concept_key, generated_content)\n",
        "        print(\"--- Agentic Process End ---\\n\") # Keep this to show end\n",
        "        return final_explanation\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the interactive Qwen3 explanation agent.\n",
        "    \"\"\"\n",
        "    # Initialize the agent. This will load the Qwen3 model.\n",
        "    # Ensure you have 'transformers' and 'torch' installed.\n",
        "    # This step can take a while and consume significant memory.\n",
        "    agent = Qwen3ExplanationAgent()\n",
        "\n",
        "    if not agent.model:\n",
        "        print(\"Agent could not be initialized. Exiting.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nStarting Qwen3 Agentic Explanation System in fully automatic mode.\")\n",
        "    # Define the query to be processed automatically for flight planning\n",
        "    automatic_query = \"Explain flight planning.\" # Changed to flight planning\n",
        "\n",
        "    response = agent.explain_concept(automatic_query)\n",
        "    print(f\"\\nAgent's Automatic Explanation for '{automatic_query}':\\n{response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "U2pKfbI7RYk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import re # Import regex module\n",
        "\n",
        "class Qwen3ExplanationAgent:\n",
        "    \"\"\"\n",
        "    An AI agent designed to explain concepts using the Qwen3 model,\n",
        "    emulating a structured thought process (analyze, plan, generate, refine).\n",
        "    This agent directly integrates the Qwen3 model for content generation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, model, name=\"Qwen3Agent\"):\n",
        "        \"\"\"\n",
        "        Initializes the Qwen3ExplanationAgent with pre-loaded tokenizer and model.\n",
        "        Args:\n",
        "            tokenizer: The pre-loaded Qwen3 tokenizer.\n",
        "            model: The pre-loaded Qwen3 model.\n",
        "            name (str): The name of the agent.\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.name = name\n",
        "        # Updated knowledge_base_keywords for flight planning domain\n",
        "        self.knowledge_base_keywords = {\n",
        "            \"flight planning\": [\"route optimization\", \"fuel calculation\", \"weather considerations\", \"air traffic control\", \"regulations\"],\n",
        "            \"route optimization\": [\"great circle route\", \"wind impact\", \"restricted airspace\", \"waypoints\"],\n",
        "            \"fuel calculation\": [\"payload\", \"distance\", \"altitude\", \"reserve fuel\", \"fuel consumption rate\"],\n",
        "            \"weather considerations\": [\"wind\", \"turbulence\", \"icing\", \"thunderstorms\", \"visibility\"],\n",
        "            \"air traffic control\": [\"airspace classes\", \"flight rules\", \"communication procedures\"],\n",
        "            \"aviation regulations\": [\"flight rules\", \"licensing\", \"aircraft maintenance\"]\n",
        "        }\n",
        "        print(f\"[{self.name} - Init] Agent initialized.\")\n",
        "        if self.model and self.model.device:\n",
        "            print(f\"[{self.name} - Init] Model available on device: {self.model.device}\")\n",
        "        else:\n",
        "            print(f\"[{self.name} - Init] Model device not determined or model not loaded properly.\")\n",
        "\n",
        "\n",
        "    def _analyze_query(self, query):\n",
        "        \"\"\"\n",
        "        Simulates analyzing the user's query to identify the core concept.\n",
        "        Args:\n",
        "            query (str): The user's input query.\n",
        "        Returns:\n",
        "            str: The identified concept key (e.g., \"flight planning\") or None.\n",
        "        \"\"\"\n",
        "        query_lower = query.lower()\n",
        "        for concept in self.knowledge_base_keywords:\n",
        "            if concept in query_lower:\n",
        "                return concept\n",
        "        return None\n",
        "\n",
        "    def _plan_explanation(self, concept_key):\n",
        "        \"\"\"\n",
        "        Simulates planning the structure of the explanation based on the concept.\n",
        "        Args:\n",
        "            concept_key (str): The identified concept.\n",
        "        Returns:\n",
        "            list: A list of sub-prompts for each section of the explanation.\n",
        "        \"\"\"\n",
        "        if concept_key and concept_key in self.knowledge_base_keywords:\n",
        "            sections = self.knowledge_base_keywords[concept_key]\n",
        "            plan = [f\"Explain the {section.replace('_', ' ')} of {concept_key}.\" for section in sections]\n",
        "            return plan\n",
        "        else:\n",
        "            return [f\"Explain '{concept_key}' in detail.\"] if concept_key else [\"Provide a general explanation for the query.\"]\n",
        "\n",
        "    def _generate_section_content(self, section_prompt):\n",
        "        \"\"\"\n",
        "        Uses the Qwen3 model to generate content for a specific section.\n",
        "        Args:\n",
        "            section_prompt (str): The specific prompt for generating a section (e.g., \"Define quantum entanglement.\").\n",
        "        Returns:\n",
        "            str: The generated text for that section.\n",
        "        \"\"\"\n",
        "        if not self.model or not self.tokenizer:\n",
        "            return \"Error: Qwen3 model or tokenizer not provided. Cannot generate content.\"\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": section_prompt}]\n",
        "        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        encoded_input = self.tokenizer(text, return_tensors=\"pt\")\n",
        "        input_ids = encoded_input.input_ids.to(self.model.device)\n",
        "        attention_mask = encoded_input.attention_mask.to(self.model.device)\n",
        "\n",
        "        try:\n",
        "            output = self.model.generate(\n",
        "                input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=4096,\n",
        "                temperature=0.7,\n",
        "                do_sample=True\n",
        "            )\n",
        "            raw_generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "            assistant_response = \"\"\n",
        "            match = re.search(r'(?:assistant|ASSISTANT):?\\s*(.*)', raw_generated_text, re.DOTALL | re.IGNORECASE)\n",
        "            if match:\n",
        "                assistant_response = match.group(1).strip()\n",
        "            else:\n",
        "                user_prompt_part = text\n",
        "                user_prompt_pattern = r\"^\" + re.escape(user_prompt_part.strip()) + r\"\\s*\"\n",
        "                assistant_response = re.sub(user_prompt_pattern, \"\", raw_generated_text, flags=re.DOTALL).strip()\n",
        "                if not assistant_response:\n",
        "                    assistant_response = raw_generated_text.strip()\n",
        "\n",
        "            cleaned_text = re.sub(r'<think>.*?</think>', '', assistant_response, flags=re.DOTALL)\n",
        "            cleaned_text = re.sub(r'<think>.*', '', cleaned_text, flags=re.DOTALL)\n",
        "\n",
        "            final_text = cleaned_text.strip()\n",
        "            return final_text\n",
        "        except Exception as e:\n",
        "            print(f\"[{self.name} - Generation Error] Failed to generate content: {e}\")\n",
        "            return f\"Failed to generate content for '{section_prompt}' due to an error.\"\n",
        "\n",
        "    def _assemble_and_refine_response(self, concept_key, generated_sections):\n",
        "        \"\"\"\n",
        "        Simulates assembling and refining the generated content into a final response.\n",
        "        Args:\n",
        "            concept_key (str): The identified concept.\n",
        "            generated_sections (dict): Dictionary of generated content for each section.\n",
        "        Returns:\n",
        "            str: The final, polished explanation.\n",
        "        \"\"\"\n",
        "        final_response_parts = []\n",
        "\n",
        "        if concept_key:\n",
        "            final_response_parts.append(f\"Here's an explanation of **{concept_key.replace('_', ' ').title()}**:\")\n",
        "\n",
        "            ordered_sections = self.knowledge_base_keywords.get(concept_key, [])\n",
        "            for section_type in ordered_sections:\n",
        "                if section_type in generated_sections and generated_sections[section_type]:\n",
        "                    title = section_type.replace('_', ' ').title()\n",
        "                    final_response_parts.append(f\"\\n### {title}\\n{generated_sections[section_type]}\")\n",
        "        else:\n",
        "            if \"general_explanation\" in generated_sections:\n",
        "                final_response_parts.append(generated_sections[\"general_explanation\"])\n",
        "            else:\n",
        "                final_response_parts.append(\"I couldn't generate a specific explanation for your query.\")\n",
        "\n",
        "        final_response_parts.append(\"\\n\\nI hope this detailed explanation is helpful!\")\n",
        "\n",
        "        final_response = \"\\n\".join(final_response_parts)\n",
        "        return final_response\n",
        "\n",
        "    def explain_concept(self, query):\n",
        "        \"\"\"\n",
        "        Orchestrates the agent's full thought process to explain a concept.\n",
        "        Args:\n",
        "            query (str): The user's query.\n",
        "        Returns:\n",
        "            str: The agent's final explanation.\n",
        "        \"\"\"\n",
        "        print(\"\\n--- Agentic Process Start ---\")\n",
        "        if not self.model or not self.tokenizer:\n",
        "            print(\"[Agent - Error] Model not ready. Cannot proceed with explanation.\")\n",
        "            return \"I am unable to process your request as the underlying model could not be loaded. Please check your environment.\"\n",
        "\n",
        "        concept_key = self._analyze_query(query)\n",
        "        plan_sections = self._plan_explanation(concept_key)\n",
        "\n",
        "        generated_content = {}\n",
        "        if concept_key:\n",
        "            for section_prompt_template in plan_sections:\n",
        "                section_type = section_prompt_template.split(\"Explain the \")[1].split(\" of\")[0].replace(' ', '_')\n",
        "                generated_content[section_type] = self._generate_section_content(section_prompt_template)\n",
        "        else:\n",
        "            general_prompt = f\"Explain '{query}' in detail.\"\n",
        "            generated_content[\"general_explanation\"] = self._generate_section_content(general_prompt)\n",
        "\n",
        "        final_explanation = self._assemble_and_refine_response(concept_key, generated_content)\n",
        "        print(\"--- Agentic Process End ---\\n\")\n",
        "        return final_explanation\n",
        "\n",
        "# --- Model loading moved outside the class ---\n",
        "def load_qwen3_model(model_name=\"Qwen/Qwen3-8B\"):\n",
        "    \"\"\"\n",
        "    Loads the Qwen3 model and tokenizer. This can be resource-intensive.\n",
        "    Returns:\n",
        "        tuple: (tokenizer, model) or (None, None) if loading fails.\n",
        "    \"\"\"\n",
        "    print(f\"[Global Model Loading] Loading Qwen3 model '{model_name}'...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=\"auto\",\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        print(f\"[Global Model Loading] Model loaded successfully on device: {model.device}\")\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        print(f\"[Global Model Loading Error] Failed to load model: {e}\")\n",
        "        print(\"Please ensure 'transformers' and 'torch' are installed and you have sufficient resources (e.g., GPU memory).\")\n",
        "        return None, None\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the interactive Qwen3 explanation agent.\n",
        "    \"\"\"\n",
        "    # Load the Qwen3 model and tokenizer globally\n",
        "    tokenizer, model = load_qwen3_model()\n",
        "\n",
        "    if not model or not tokenizer:\n",
        "        print(\"Global model loading failed. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Initialize the agent with the pre-loaded tokenizer and model\n",
        "    agent = Qwen3ExplanationAgent(tokenizer=tokenizer, model=model)\n",
        "\n",
        "    print(\"\\nStarting Qwen3 Agentic Explanation System in fully automatic mode.\")\n",
        "    # Define the query to be processed automatically for flight planning\n",
        "    automatic_query = \"Explain flight planning.\"\n",
        "\n",
        "    response = agent.explain_concept(automatic_query)\n",
        "    print(f\"\\nAgent's Automatic Explanation for '{automatic_query}':\\n{response}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "-PpRMpruhKY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Qwen3 model and tokenizer globally\n",
        "tokenizer, model = load_qwen3_model()\n",
        "\n",
        "if not model or not tokenizer:\n",
        "    print(\"Global model loading failed. Exiting.\")\n",
        "    #return"
      ],
      "metadata": {
        "id": "-2yExGE-i0_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the agent with the pre-loaded tokenizer and model\n",
        "agent = Qwen3ExplanationAgent(tokenizer=tokenizer, model=model)\n",
        "\n",
        "print(\"\\nStarting Qwen3 Agentic Explanation System in fully automatic mode.\")\n",
        "# Define the query to be processed automatically for flight planning\n",
        "automatic_query = \"Explain flight planning.\"\n",
        "\n",
        "response = agent.explain_concept(automatic_query)\n",
        "print(f\"\\nAgent's Automatic Explanation for '{automatic_query}':\\n{response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y01UvB9jGZp",
        "outputId": "87a000ba-fe7e-4e90-8014-6a973d166336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Qwen3Agent - Init] Agent initialized.\n",
            "[Qwen3Agent - Init] Model available on device: cpu\n",
            "\n",
            "Starting Qwen3 Agentic Explanation System in fully automatic mode.\n",
            "\n",
            "--- Agentic Process Start ---\n"
          ]
        }
      ]
    }
  ]
}